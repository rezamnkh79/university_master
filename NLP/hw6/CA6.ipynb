{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-02T12:10:52.406402Z",
     "start_time": "2025-02-02T12:10:52.397852Z"
    }
   },
   "source": [
    "print(\"\"\"\n",
    "۱. چرا داده‌ها را به chunk تبدیل می‌کنیم؟ تأثیر chunk_size و chunk_overlap چیست؟\n",
    "تقسیم داده‌ها به chunk باعث پردازش بهتر متن و استخراج دقیق‌تر اطلاعات توسط مدل می‌شود. chunk_size اندازه هر بخش را تعیین می‌کند و chunk_overlap با ایجاد همپوشانی، از گسستگی اطلاعات جلوگیری می‌کند.\n",
    "\n",
    "۲. چرا بهتر است که بازیابی ترکیبی باشد؟ بازیابی ترکیبی به چه معنی است؟\n",
    "بازیابی ترکیبی از دو روش جستجوی کلمات کلیدی و جستجوی معنایی استفاده می‌کند تا اطلاعات دقیق‌تری ارائه دهد. این روش باعث افزایش دقت چت‌بات در یافتن پاسخ‌های مرتبط می‌شود.\n",
    "\n",
    "۳. چرا قبل از تولید پاسخ، context‌های بازیابی یا جستجو شده را فیلتر می‌کنیم؟\n",
    "فیلترگذاری context باعث حذف اطلاعات نامرتبط و افزایش دقت پاسخ می‌شود. این کار از ایجاد پاسخ‌های نادرست توسط مدل جلوگیری می‌کند.\n",
    "\n",
    "۴. مدل قابلیت پاسخ به سوالات را دارد. چرا کلاً این وظیفه را به مدل نمی‌سپاریم و RAG پیاده‌سازی کردیم؟\n",
    "مدل‌های زبانی ممکن است اطلاعات نادرست تولید کنند، اما RAG با ترکیب جستجو و تولید متن، پاسخ‌های دقیق‌تری ارائه می‌دهد. این روش از داده‌های به‌روز استفاده کرده و اعتبار پاسخ‌ها را افزایش می‌دهد.\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "۱. چرا داده‌ها را به chunk تبدیل می‌کنیم؟ تأثیر chunk_size و chunk_overlap چیست؟\n",
      "تقسیم داده‌ها به chunk باعث پردازش بهتر متن و استخراج دقیق‌تر اطلاعات توسط مدل می‌شود. chunk_size اندازه هر بخش را تعیین می‌کند و chunk_overlap با ایجاد همپوشانی، از گسستگی اطلاعات جلوگیری می‌کند.\n",
      "\n",
      "۲. چرا بهتر است که بازیابی ترکیبی باشد؟ بازیابی ترکیبی به چه معنی است؟\n",
      "بازیابی ترکیبی از دو روش جستجوی کلمات کلیدی و جستجوی معنایی استفاده می‌کند تا اطلاعات دقیق‌تری ارائه دهد. این روش باعث افزایش دقت چت‌بات در یافتن پاسخ‌های مرتبط می‌شود.\n",
      "\n",
      "۳. چرا قبل از تولید پاسخ، context‌های بازیابی یا جستجو شده را فیلتر می‌کنیم؟\n",
      "فیلترگذاری context باعث حذف اطلاعات نامرتبط و افزایش دقت پاسخ می‌شود. این کار از ایجاد پاسخ‌های نادرست توسط مدل جلوگیری می‌کند.\n",
      "\n",
      "۴. مدل قابلیت پاسخ به سوالات را دارد. چرا کلاً این وظیفه را به مدل نمی‌سپاریم و RAG پیاده‌سازی کردیم؟\n",
      "مدل‌های زبانی ممکن است اطلاعات نادرست تولید کنند، اما RAG با ترکیب جستجو و تولید متن، پاسخ‌های دقیق‌تری ارائه می‌دهد. این روش از داده‌های به‌روز استفاده کرده و اعتبار پاسخ‌ها را افزایش می‌دهد.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T14:51:32.200057Z",
     "start_time": "2025-02-02T14:51:32.045448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import requests\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import LanceDB\n",
    "import lancedb\n",
    "from langchain.chains import RetrievalQA\n",
    "from llama_parse import LlamaParse\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# تنظیمات API\n",
    "AVALAI_BASE_URL = \"https://api.avalai.ir/v1\"\n",
    "GPT_MODEL_NAME = \"gpt-4o-mini\"\n",
    "GEMINI_MODEL_NAME = \"gemini-1.5-flash\"\n",
    "TAVILY_API_KEY = \"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\"\n",
    "\n",
    "# مقداردهی اولیه مدل‌ها\n",
    "gpt4o_chat = ChatOpenAI(\n",
    "    model=GPT_MODEL_NAME,\n",
    "    base_url=AVALAI_BASE_URL,\n",
    "    api_key=\"aa-IvpAOjBtYPok34GinJJjP88e7sfDmFni7ZFIbhxMrpa8P0c9\",\n",
    ")\n",
    "LLAMA_CLOUD_API_KEY = \"llx-tDiq3pq1fwKcrJX6adgMOS2SiTOvxDDFuD6HQ2jLTJ4fEMFc\"\n",
    "# gemini_chat = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME)\n",
    "\n",
    "# تنظیمات مسیرهای ذخیره‌سازی\n",
    "PDF_FILE_PATH = \"The_New_Complete_Book_of_Food.pdf\"\n",
    "LANCE_DB_PATH = \"food_knowledge.lance\"\n",
    "DB_PATH = \"food_orders.db\"\n",
    "\n",
    "# پردازش فایل PDF با استفاده از LlamaParse\n",
    "# پردازش فایل PDF با استفاده از LlamaParse\n",
    "def parse_pdf(file_path):\n",
    "    parser = LlamaParse(api_key=LLAMA_CLOUD_API_KEY)\n",
    "    text_data = parser.load_data(file_path)\n",
    "    text = \"\\n\".join([doc.text for doc in text_data])\n",
    "    return text\n",
    "\n",
    "\n",
    "# تقسیم‌بندی متن به چانک‌ها\n",
    "def split_text(text):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ذخیره چانک‌ها در SQLite\n",
    "def store_embeddings(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    db_connection = lancedb.connect(LANCE_DB_PATH)\n",
    "    table = db_connection.create_table(\"food_data\", schema={\"id\": int, \"text\": str, \"vector\": list}, mode=\"overwrite\")\n",
    "\n",
    "    records = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embedding_vector = embeddings.embed_query(chunk)\n",
    "        records.append({\"id\": i, \"text\": chunk, \"vector\": embedding_vector})\n",
    "\n",
    "    table.add(records)\n",
    "    return db_connection\n",
    "\n",
    "\n",
    "# تابع برای جستجو در SQLite\n",
    "def search_in_db(query):\n",
    "    connection = sqlite3.connect(DB_PATH)\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT content FROM food_data\")\n",
    "    results = cursor.fetchall()\n",
    "    connection.close()\n",
    "    matches = [row[0] for row in results if query.lower() in row[0].lower()]\n",
    "    return matches\n",
    "\n",
    "\n",
    "# تابع برای جستجو در اینترنت با استفاده از Tavily API\n",
    "def search_online(query):\n",
    "    url = \"https://api.tavily.com/search\"\n",
    "    headers = {\"Authorization\": f\"Bearer {TAVILY_API_KEY}\"}\n",
    "    params = {\"query\": query, \"num_results\": 3}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"results\"]\n",
    "    return []\n",
    "\n",
    "\n",
    "# تابع فیلتر کردن context نامرتبط\n",
    "def filter_context(context):\n",
    "    return context\n",
    "\n",
    "\n",
    "# تابع اصلی برای پردازش سوالات کاربران\n",
    "def process_user_query(question, model=\"gpt4o\"):\n",
    "    retrieved_docs = search_in_db(question)\n",
    "\n",
    "    if retrieved_docs:\n",
    "        context = \"\\n\".join(retrieved_docs)\n",
    "    else:\n",
    "        search_results = search_online(question)\n",
    "        context = \"\\n\".join([result[\"snippet\"] for result in search_results])\n",
    "\n",
    "    context = filter_context(context)\n",
    "\n",
    "    if not context:\n",
    "        return \"متأسفم، اطلاعاتی درباره این موضوع ندارم.\"\n",
    "\n",
    "    llm = gpt4o_chat if model == \"gpt4o\" else gemini_chat\n",
    "    response = llm(context)\n",
    "    return response\n",
    "\n",
    "\n",
    "# تعیین حوزه سوالات مجاز\n",
    "def validate_question_scope(question):\n",
    "    allowed_keywords = [\n",
    "        \"food\", \"meal\", \"nutrition\", \"diet\", \"restaurant\", \"cuisine\", \"dish\", \"recipe\", \"ingredients\", \"healthy eating\",\n",
    "        \"calories\", \"cooking\", \"menu\", \"snack\", \"beverage\", \"gourmet\", \"organic food\", \"fast food\", \"fine dining\", \"vegan\",\n",
    "        \"vegetarian\", \"protein\", \"carbohydrates\", \"fats\", \"vitamins\", \"minerals\", \"superfoods\", \"ethnic food\", \"street food\"\n",
    "    ]\n",
    "    if not any(keyword in question.lower() for keyword in allowed_keywords):\n",
    "        return \"This question is outside the chatbot's domain.\"\n",
    "    return None\n",
    "\n",
    "\n",
    "# تابع پاسخگویی به سوالات کاربران\n",
    "def answer_food_question(question, model=\"gpt4o\"):\n",
    "    validation_msg = validate_question_scope(question)\n",
    "    if validation_msg:\n",
    "        return validation_msg\n",
    "\n",
    "    return process_user_query(question, model)\n",
    "\n"
   ],
   "id": "ddedc0e81ea4da15",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T14:51:36.526717Z",
     "start_time": "2025-02-02T14:51:36.416998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# پردازش و ذخیره‌سازی داده‌ها برای اولین بار\n",
    "if __name__ == \"__main__\":\n",
    "    PDF_FILE_PATH = \"The_New_Complete_Book_of_Food.pdf\"\n",
    "    text = parse_pdf(PDF_FILE_PATH)\n",
    "\n",
    "\n",
    "    print(\"پردازش و ذخیره‌سازی اطلاعات غذاها تکمیل شد!\")\n",
    "\n",
    "\n"
   ],
   "id": "42d592bdeb27ad08",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Detected nested async. Please use nest_asyncio.apply() to allow nested event loops.Or, use async entry methods like `aquery()`, `aretriever`, `achat`, etc.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/Desktop/university/master/venvs/NLP/CA2_env/lib/python3.12/site-packages/llama_index/core/async_utils.py:33\u001B[0m, in \u001B[0;36masyncio_run\u001B[0;34m(coro)\u001B[0m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;66;03m# If we're here, there's an existing loop but it's not running\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_until_complete\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcoro\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;66;03m# If we can't get the event loop, we're likely in a different thread, or its already running\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.12/asyncio/base_events.py:663\u001B[0m, in \u001B[0;36mBaseEventLoop.run_until_complete\u001B[0;34m(self, future)\u001B[0m\n\u001B[1;32m    662\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_closed()\n\u001B[0;32m--> 663\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_running\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    665\u001B[0m new_task \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m futures\u001B[38;5;241m.\u001B[39misfuture(future)\n",
      "File \u001B[0;32m/usr/lib/python3.12/asyncio/base_events.py:622\u001B[0m, in \u001B[0;36mBaseEventLoop._check_running\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    621\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_running():\n\u001B[0;32m--> 622\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThis event loop is already running\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    623\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m events\u001B[38;5;241m.\u001B[39m_get_running_loop() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: This event loop is already running",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/Desktop/university/master/venvs/NLP/CA2_env/lib/python3.12/site-packages/llama_index/core/async_utils.py:38\u001B[0m, in \u001B[0;36masyncio_run\u001B[0;34m(coro)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43masyncio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcoro\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/usr/lib/python3.12/asyncio/runners.py:190\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(main, debug, loop_factory)\u001B[0m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m events\u001B[38;5;241m.\u001B[39m_get_running_loop() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;66;03m# fail fast with short traceback\u001B[39;00m\n\u001B[0;32m--> 190\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    191\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124masyncio.run() cannot be called from a running event loop\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Runner(debug\u001B[38;5;241m=\u001B[39mdebug, loop_factory\u001B[38;5;241m=\u001B[39mloop_factory) \u001B[38;5;28;01mas\u001B[39;00m runner:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: asyncio.run() cannot be called from a running event loop",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m      3\u001B[0m     PDF_FILE_PATH \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe_New_Complete_Book_of_Food.pdf\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 4\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[43mparse_pdf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPDF_FILE_PATH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mپردازش و ذخیره‌سازی اطلاعات غذاها تکمیل شد!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[9], line 37\u001B[0m, in \u001B[0;36mparse_pdf\u001B[0;34m(file_path)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparse_pdf\u001B[39m(file_path):\n\u001B[1;32m     36\u001B[0m     parser \u001B[38;5;241m=\u001B[39m LlamaParse(api_key\u001B[38;5;241m=\u001B[39mLLAMA_CLOUD_API_KEY)\n\u001B[0;32m---> 37\u001B[0m     text_data \u001B[38;5;241m=\u001B[39m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([doc\u001B[38;5;241m.\u001B[39mtext \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m text_data])\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m text\n",
      "File \u001B[0;32m~/Desktop/university/master/venvs/NLP/CA2_env/lib/python3.12/site-packages/llama_parse/base.py:858\u001B[0m, in \u001B[0;36mLlamaParse.load_data\u001B[0;34m(self, file_path, extra_info, fs)\u001B[0m\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(nest_asyncio_msg)\n\u001B[1;32m    857\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 858\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[0;32m~/Desktop/university/master/venvs/NLP/CA2_env/lib/python3.12/site-packages/llama_parse/base.py:853\u001B[0m, in \u001B[0;36mLlamaParse.load_data\u001B[0;34m(self, file_path, extra_info, fs)\u001B[0m\n\u001B[1;32m    851\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Load data from the input path.\"\"\"\u001B[39;00m\n\u001B[1;32m    852\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 853\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43masyncio_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    855\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m nest_asyncio_err \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e):\n",
      "File \u001B[0;32m~/Desktop/university/master/venvs/NLP/CA2_env/lib/python3.12/site-packages/llama_index/core/async_utils.py:40\u001B[0m, in \u001B[0;36masyncio_run\u001B[0;34m(coro)\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mrun(coro)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m---> 40\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m     41\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDetected nested async. Please use nest_asyncio.apply() to allow nested event loops.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     42\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOr, use async entry methods like `aquery()`, `aretriever`, `achat`, etc.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     43\u001B[0m     )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Detected nested async. Please use nest_asyncio.apply() to allow nested event loops.Or, use async entry methods like `aquery()`, `aretriever`, `achat`, etc."
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import requests\n",
    "import Levenshtein\n",
    "import pyarrow as pa\n",
    "import lancedb\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from llama_parse import LlamaParse\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# تنظیمات API\n",
    "AVALAI_BASE_URL = \"https://api.avalai.ir/v1\"\n",
    "GPT_MODEL_NAME = \"gpt-4o-mini\"\n",
    "GEMINI_MODEL_NAME = \"gemini-1.5-flash\"\n",
    "TAVILY_API_KEY = \"your_tavily_api_key\"\n",
    "LLAMA_CLOUD_API_KEY = os.environ.get(\"LLAMA_CLOUD_API_KEY\", \"your_api_key_here\")\n",
    "\n",
    "# مقداردهی اولیه مدل‌ها\n",
    "gpt4o_chat = ChatOpenAI(\n",
    "    model=GPT_MODEL_NAME,\n",
    "    base_url=AVALAI_BASE_URL,\n",
    "    api_key=os.environ[\"AVALAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "gemini_chat = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME)\n",
    "\n",
    "# تنظیمات مسیرهای ذخیره‌سازی\n",
    "PDF_FILE_PATH = \"The_New_Complete_Book_of_Food.pdf\"\n",
    "LANCE_DB_PATH = \"./food_knowledge.lance\"\n",
    "\n",
    "# پردازش فایل PDF با استفاده از LlamaParse\n",
    "def parse_pdf(file_path):\n",
    "    parser = LlamaParse(api_key=LLAMA_CLOUD_API_KEY)\n",
    "    text_data = parser.load_data(file_path)\n",
    "    text = \"\\n\".join([doc.text for doc in text_data])\n",
    "    return text\n",
    "\n",
    "# تقسیم‌بندی متن به چانک‌ها\n",
    "def split_text(text):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# ذخیره چانک‌ها در LanceDB\n",
    "def store_embeddings(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    db_connection = lancedb.connect(LANCE_DB_PATH)\n",
    "    schema = pa.schema([\n",
    "        (\"id\", pa.int32()),\n",
    "        (\"text\", pa.string()),\n",
    "        (\"vector\", pa.list_(pa.float32()))\n",
    "    ])\n",
    "    table = db_connection.create_table(\"food_data\", schema=schema, mode=\"overwrite\")\n",
    "    records = [{\"id\": i, \"text\": chunk, \"vector\": embeddings.embed_query(chunk)} for i, chunk in enumerate(chunks)]\n",
    "    table.add(records)\n",
    "    return db_connection\n",
    "\n",
    "# جستجو در LanceDB\n",
    "def search_in_db(query):\n",
    "    db_connection = lancedb.connect(LANCE_DB_PATH)\n",
    "    table = db_connection.open_table(\"food_data\")\n",
    "    results = table.search(query).limit(10).to_pandas()\n",
    "    matches = results[\"text\"].tolist()\n",
    "    return matches\n",
    "\n",
    "# تابع پاسخگویی به سوالات کاربران\n",
    "def answer_food_question(question, model=\"gpt4o\"):\n",
    "    validation_msg = validate_question_scope(question)\n",
    "    if validation_msg:\n",
    "        return validation_msg\n",
    "    return process_user_query(question, model)\n",
    "\n",
    "# پردازش و ذخیره‌سازی داده‌ها برای اولین بار\n",
    "if __name__ == \"__main__\":\n",
    "    text = parse_pdf(PDF_FILE_PATH)\n",
    "    chunks = split_text(text)\n",
    "    store_embeddings(chunks)\n",
    "    print(\"Processing and storing food information is complete!\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"Ask your question about food (type 'exit' to quit): \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        response = answer_food_question(user_input)\n",
    "        print(\"Response:\", response)\n"
   ],
   "id": "ef446084ffd1906f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlrrsi07MDhO"
   },
   "source": [
    "<h1 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">تمرین شماره ۱: درس پردازش زبان طبیعی - دانشگاه تهران، پائیز ۱۴۰۳</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T42xpmfrMDhQ"
   },
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 18px;\">\n",
    "نام:رضا منصوری خواه\n",
    "<br/>\n",
    "شماره دانشجویی:۸۱۰۱۰۳۲۴۶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58azWVGxMDhQ"
   },
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "سوالات خودتان را می‌توانید از طریق ایمیل\n",
    "<code>ghanizadeh.amin@ut.ac.ir</code>\n",
    " از طراح تمرین اول بپرسید."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foW-gR1HMDhR"
   },
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 18px; color: red; font-weight: bold;\">\n",
    "قوانین و توضیحاتی آخر فایل تمرین حتما به دقت مطالعه شود.\n",
    "</div>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "run this command\n",
    "# **pip3 install -r requirement.txt**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "https://chatgpt.com/share/67115ca5-2e40-8006-94e9-3e5895bd9205"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52tAk_6dMWnF"
   },
   "source": [
    "# **سوال اول:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkAAw0haMdyS"
   },
   "source": [
    "*بخش اول:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:45:21.192543Z",
     "start_time": "2024-10-17T15:45:21.183416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    \"قسمت اول                                                                                                                              \")\n",
    "print(\"\"\"\n",
    "تشخیص موجودیت‌های نام‌گذاری شده (NER) یکی از وظایف پردازش زبان طبیعی (NLP) است که به شناسایی و استخراج موجودیت‌های نام‌گذاری شده از متن می‌پردازد. این موجودیت‌ها می‌توانند اشخاص مکان‌ها سازمان‌ها تاریخ‌ها و سایر اشیاء دنیای واقعی باشند. هدف اصلی NER شناسایی و طبقه‌بندی این موجودیت‌ها در داده‌های متنی غیرساختاریافته است مانند مقالات خبری پست‌های شبکه‌های اجتماعی یا اسناد داخلی شرکت‌ها                                       .\n",
    "هدف NER                                                                                                                                \n",
    "هدف اصلی NER شناسایی و دسته‌بندی این موجودیت‌ها به طور خودکار و با دقت بالا است                                                        \n",
    "تحلیل متن: کمک به تحلیلگران برای درک بهتر محتوای متن و شناسایی مفاهیم کلیدی                                                        .\n",
    "جستجوی اطلاعات: بهبود کیفیت نتایج جستجو با شناسایی موجودیت‌ها و ارتباط آن‌ها با یکدیگر                                                .\n",
    "خلاصه‌سازی متن: کمک به خلاصه‌سازی اطلاعات با شناسایی و استخراج موجودیت‌های مهم                                                           .\n",
    "پاسخ‌گویی به سوالات: بهبود سیستم‌های پاسخ‌گویی به سوالات با شناسایی موجودیت‌ها و ارتباطات آن‌ها                                                                                                                                                                                          .\n",
    "نظارت بر رسانه‌ها: نظارت و تحلیل رسانه‌ها با شناسایی نام‌های افراد سازمان‌ها و رویدادها                                                .\n",
    "https://www.ibm.com/topics/named-entity-recognition\n",
    "\n",
    "\"\"\")\n",
    "print(\"=\" * 150)\n",
    "print(\n",
    "    \"قسمت دوم                                                                                                                              \")\n",
    "import re\n",
    "\n",
    "file_path = 'data/ner.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "email_pattern = r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b'\n",
    "emails = re.findall(email_pattern, text)\n",
    "print(\"Emails:\", emails)\n",
    "\n",
    "phone_pattern_simple = r'\\b\\d{1,3}[-.\\s()]?\\d{1,4}[-.\\s()]?\\d{1,4}[-.\\s()]?\\d{0,4}\\b'\n",
    "phones = re.findall(phone_pattern_simple, text)\n",
    "print(\"Phone Numbers:\", phones)\n",
    "\n",
    "url_pattern_simple = r'\\bhttps?://\\S+\\b'\n",
    "urls_simple = re.findall(url_pattern_simple, text)\n",
    "print(\"URLs:\", urls_simple)\n",
    "print(\"=\" * 150)\n",
    "print(\n",
    "    \"قسمت سوم                                                                                                                              \")\n",
    "print(\"\"\"مزایای regex شامل سفارشی‌سازی آسان برای الگوهای خاص، سرعت و سادگی در شناسایی الگوهای ساده و قابلیت کار با متون غیرساختاری است. معایب آن شامل پیچیدگی در پیدا کردن الگوهای مناسب برای مسائل پیچیده، عدم دقت کافی در شناسایی موارد و عدم توانایی در بررسی معنایی متن است.                                               \n",
    "\"\"\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قسمت اول                                                                                                                              \n",
      "\n",
      "تشخیص موجودیت‌های نام‌گذاری شده (NER) یکی از وظایف پردازش زبان طبیعی (NLP) است که به شناسایی و استخراج موجودیت‌های نام‌گذاری شده از متن می‌پردازد. این موجودیت‌ها می‌توانند اشخاص مکان‌ها سازمان‌ها تاریخ‌ها و سایر اشیاء دنیای واقعی باشند. هدف اصلی NER شناسایی و طبقه‌بندی این موجودیت‌ها در داده‌های متنی غیرساختاریافته است مانند مقالات خبری پست‌های شبکه‌های اجتماعی یا اسناد داخلی شرکت‌ها                                       .\n",
      "هدف NER                                                                                                                                \n",
      "هدف اصلی NER شناسایی و دسته‌بندی این موجودیت‌ها به طور خودکار و با دقت بالا است                                                        \n",
      "تحلیل متن: کمک به تحلیلگران برای درک بهتر محتوای متن و شناسایی مفاهیم کلیدی                                                        .\n",
      "جستجوی اطلاعات: بهبود کیفیت نتایج جستجو با شناسایی موجودیت‌ها و ارتباط آن‌ها با یکدیگر                                                .\n",
      "خلاصه‌سازی متن: کمک به خلاصه‌سازی اطلاعات با شناسایی و استخراج موجودیت‌های مهم                                                           .\n",
      "پاسخ‌گویی به سوالات: بهبود سیستم‌های پاسخ‌گویی به سوالات با شناسایی موجودیت‌ها و ارتباطات آن‌ها                                                                                                                                                                                          .\n",
      "نظارت بر رسانه‌ها: نظارت و تحلیل رسانه‌ها با شناسایی نام‌های افراد سازمان‌ها و رویدادها                                                .\n",
      "https://www.ibm.com/topics/named-entity-recognition\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "قسمت دوم                                                                                                                              \n",
      "Emails: ['info@vrinnovations.com', 'john.martinez@techadvisors.net', 'support@innovateads.co', 'alice.p@mailmasters.com', 'sales@mailmasters.com', 'director@socialbuzz.org', 'support@deepad.ai', 'info@legaleagle.org', 'partners@influenceme.io', 'contact@brandvision.com', 'hello@mobilemarketersinc.com', 'paul.jenkins@cxinnovators.com', 'info@digitalfuture2025.com']\n",
      "Phone Numbers: ['1-800-555-1234', '2025', '44 20 7946 0958', '1-310-555-7890', '1-212-555-6677', '1-718-555-9988', '49 30 1234 5678', '33 1 2345 6789', '61 3 9876 5432', '1-202-555-2244', '1-415-555-3344', '1-646-555-9988', '1-888-555-1234']\n",
      "URLs: ['https://www.digitalfuture2025.com', 'http://www.techadvisors.net', 'https://www.innovateads.co', 'https://www.innovateads.co/blog', 'http://www.mailmasters.com', 'https://www.socialbuzz.org', 'http://www.deepad.ai', 'http://www.deepad.ai/research', 'https://www.legaleagle.org/marketing-laws', 'http://www.influenceme.io', 'https://www.brandvision.com', 'http://www.mobilemarketersinc.com', 'https://www.cxinnovators.com', 'https://www.cxinnovators.com/blog', 'https://www.digitalfuture2025.com/articles']\n",
      "======================================================================================================================================================\n",
      "قسمت سوم                                                                                                                              \n",
      "مزایای regex شامل سفارشی‌سازی آسان برای الگوهای خاص، سرعت و سادگی در شناسایی الگوهای ساده و قابلیت کار با متون غیرساختاری است. معایب آن شامل پیچیدگی در پیدا کردن الگوهای مناسب برای مسائل پیچیده، عدم دقت کافی در شناسایی موارد و عدم توانایی در بررسی معنایی متن است.                                               \n",
      "\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5vi1eJVMkCH"
   },
   "source": [
    "*بخش دوم:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tDGqlfCjMDhR",
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-10-17T15:47:29.491398Z",
     "start_time": "2024-10-17T15:47:29.480170Z"
    }
   },
   "source": [
    "print(\n",
    "    \"قسمت اول                                                                                                                              \")\n",
    "print(\"\"\"\n",
    "توکنایزرهای Rule-Based:                                                                                                                                      \n",
    "مزایا:                                                                                                                                      \n",
    " معمولاً سریع و کارآمد هستند و به سادگی می‌توانند متن را به واحدهای معنی‌دار تقسیم کنند و  دقت بالایی در تقسیم‌بندی متن دارند به خصوص در مواردی که قوانین واضحی وجود دارد                                                                                                                                \n",
    "                                                    \n",
    "                                                                                                                       معایب:              \n",
    "همه ی الگوهارا نمیتوان پیاده سازی کرد و در جاهایی ک ابهامی وجود دارد نقص هایی دارد و جواب نمیدهد و معمولا باید دانش و دیتای زیادی در ان زمنیه داشته باشد                                                                                                                                                                      کاربرد:                                                                                                                                   \n",
    "                                   در مواردی ک دقت بالا نیاز است کاربرد دارد مثل پزشکی                                                 \n",
    "                                   \n",
    "                                   \n",
    "                                                                                                      توکنایزرهای مبتنی بر یادگیری ماشین:\n",
    "                                                                                                              \n",
    "                        مزایا: می‌توانند الگوهای پیچیده و ارتباطات بین کلمات را از داده‌های آموزشی یاد بگیرند. این توکنایزرها می‌توانند با افزایش داده‌های آموزشی عملکرد خود را بهبود میبخشد                                                                       \n",
    "                        معایب:        نیاز به مجموعه داده‌های بزرگ و با کیفیت دارند و پیچیدگی زیاد دارند و قالبا تفسیرشان سخت است               \n",
    "        کاربرد:در مواردی ک نیاز به تحلیل های پیچیده مانند کلاس بندی روی شبکه ها یا پیدا کردن موارد یکسان و طبیق انها کاربرد دارند          \n",
    "\"\"\")\n",
    "print(\"=\" * 150)\n",
    "print(\n",
    "    \"قسمت دوم                                                                                                                              \")\n",
    "\n",
    "file_path = 'data/ner.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "tokens = text.split(\" \")\n",
    "print(tokens)\n",
    "print(\"\"\"\n",
    "                                                                                                                                        معایب\n",
    "معایب توکنایزر مبتنی بر فضای خالی شامل عدم دقت در تفکیک کلمات و عدم مدیریت نشانه‌گذاری است که ممکن است باعث شناسایی نادرست کلمات ترکیبی و نگه‌داشتن نشانه‌ها به عنوان بخشی از کلمات شود. همچنین این توکنایزر قادر به شناسایی زبان‌های خاص و موارد ویژه مانند ایمیل‌ها و شماره تلفن‌ها نیست و در متون بزرگ            کارایی کمتری دارد.                                                                                                                              \n",
    "\"\"\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قسمت اول                                                                                                                              \n",
      "\n",
      "توکنایزرهای Rule-Based:                                                                                                                                      \n",
      "مزایا:                                                                                                                                      \n",
      " معمولاً سریع و کارآمد هستند و به سادگی می‌توانند متن را به واحدهای معنی‌دار تقسیم کنند و  دقت بالایی در تقسیم‌بندی متن دارند به خصوص در مواردی که قوانین واضحی وجود دارد                                                                                                                                \n",
      "                                                    \n",
      "                                                                                                                       معایب:              \n",
      "همه ی الگوهارا نمیتوان پیاده سازی کرد و در جاهایی ک ابهامی وجود دارد نقص هایی دارد و جواب نمیدهد و معمولا باید دانش و دیتای زیادی در ان زمنیه داشته باشد                                                                                                                                                                      کاربرد:                                                                                                                                   \n",
      "                                   در مواردی ک دقت بالا نیاز است کاربرد دارد مثل پزشکی                                                 \n",
      "                                   \n",
      "                                   \n",
      "                                                                                                      توکنایزرهای مبتنی بر یادگیری ماشین:\n",
      "                                                                                                              \n",
      "                        مزایا: می‌توانند الگوهای پیچیده و ارتباطات بین کلمات را از داده‌های آموزشی یاد بگیرند. این توکنایزرها می‌توانند با افزایش داده‌های آموزشی عملکرد خود را بهبود میبخشد                                                                       \n",
      "                        معایب:        نیاز به مجموعه داده‌های بزرگ و با کیفیت دارند و پیچیدگی زیاد دارند و قالبا تفسیرشان سخت است               \n",
      "        کاربرد:در مواردی ک نیاز به تحلیل های پیچیده مانند کلاس بندی روی شبکه ها یا پیدا کردن موارد یکسان و طبیق انها کاربرد دارند          \n",
      "\n",
      "======================================================================================================================================================\n",
      "قسمت دوم                                                                                                                              \n",
      "['As', 'we', 'move', 'further', 'into', 'the', 'digital', 'age,', 'the', 'landscape', 'of', 'marketing', 'is', 'evolving', 'at', 'a', 'pace', 'faster', 'than', 'ever', 'before.', 'Businesses', 'are', 'harnessing', 'the', 'power', 'of', 'artificial', 'intelligence,', 'data', 'analytics,', 'and', 'personalized', 'advertising', 'to', 'reach', 'their', 'target', 'audiences.', 'If', 'you', 'wish', 'to', 'learn', 'more', 'about', 'the', 'latest', 'trends,', 'visit', 'https://www.digitalfuture2025.com.\\n\\nOne', 'significant', 'change', 'is', 'the', 'integration', 'of', 'augmented', 'reality', '(AR)', 'in', 'advertising', 'campaigns.', 'Brands', 'like', 'VRInnovations', '(contact:', 'info@vrinnovations.com)', 'are', 'leading', 'the', 'charge,', 'offering', 'immersive', 'experiences', 'that', 'let', 'users', '\"try\"', 'products', 'virtually', 'before', 'making', 'a', 'purchase.', 'This', 'level', 'of', 'engagement', 'is', 'a', 'game-changer.', 'For', 'collaborations,', 'reach', 'out', 'to', 'their', 'team', 'at', '+1-800-555-1234.\\n\\nIn', 'a', 'report', 'published', 'by', 'TechAdvisors', '(visit', 'them', 'at', 'http://www.techadvisors.net),', 'experts', 'highlighted', 'the', 'rise', 'of', 'programmatic', 'advertising', 'as', 'the', 'next', 'big', 'thing.', '\"By', '2025,', 'we', 'expect', '90%', 'of', 'online', 'ads', 'to', 'be', 'placed', 'using', 'automated', 'algorithms,\"', 'says', 'John', 'Martinez,', 'chief', 'data', 'scientist', 'at', 'TechAdvisors.', 'You', 'can', 'contact', 'him', 'directly', 'at', 'john.martinez@techadvisors.net', 'or', 'at', 'his', 'office', 'number', '+44', '20', '7946', '0958', 'for', 'more', 'information', 'on', 'their', 'latest', 'research.\\n\\nInnovate', 'Ads', '(website:', 'https://www.innovateads.co)', 'is', 'another', 'key', 'player', 'in', 'the', 'digital', 'marketing', 'world.', 'Their', 'data-driven', 'approach', 'has', 'proven', 'effective', 'for', 'businesses', 'ranging', 'from', 'small', 'startups', 'to', 'global', 'enterprises.', 'For', 'queries,', 'shoot', 'an', 'email', 'to', 'support@innovateads.co', 'or', 'dial', 'their', 'customer', 'support', 'line', 'at', '+1-310-555-7890.', 'Their', 'client', 'success', 'stories', 'are', 'featured', 'regularly', 'on', 'their', 'blog', 'at', 'https://www.innovateads.co/blog.\\n\\nMeanwhile,', 'email', 'marketing', 'continues', 'to', 'be', 'an', 'effective', 'tool', 'for', 'brands', 'to', 'communicate', 'with', 'customers', 'directly.', 'MailMasters', '(website:', 'http://www.mailmasters.com)', 'has', 'been', 'at', 'the', 'forefront', 'of', 'this', 'domain.', 'They', 'recently', 'introduced', 'a', 'new', 'tool', 'for', 'analyzing', 'open', 'rates', 'and', 'customer', 'engagement.', 'You', 'can', 'contact', 'their', 'CEO,', 'Alice', 'Peters,', 'at', 'alice.p@mailmasters.com', 'or', 'give', 'her', 'a', 'call', 'at', '+1-212-555-6677.', 'For', 'sales', 'inquiries,', 'you', 'can', 'also', 'reach', 'out', 'to', 'their', 'team', 'via', 'sales@mailmasters.com.\\n\\nSocial', 'media', 'platforms', 'are', 'another', 'major', 'force', 'in', 'the', 'evolving', 'marketing', 'scene.', 'According', 'to', 'a', 'recent', 'survey', 'by', 'SocialBuzz', '(website:', 'https://www.socialbuzz.org),', 'brands', 'that', 'engage', 'customers', 'through', 'personalized', 'messages', 'on', 'platforms', 'like', 'Instagram,', 'Twitter,', 'and', 'LinkedIn', 'saw', 'a', '45%', 'increase', 'in', 'user', 'engagement.', 'For', 'more', 'details,', 'email', 'their', 'marketing', 'director', 'at', 'director@socialbuzz.org', 'or', 'call', 'them', 'at', '+1-718-555-9988.\\n\\nWith', 'the', 'increasing', 'use', 'of', 'AI', 'in', 'digital', 'marketing,', 'companies', 'like', 'DeepAd', '(website:', 'http://www.deepad.ai)', 'are', 'developing', 'tools', 'that', 'help', 'marketers', 'predict', 'customer', 'behavior', 'with', 'astounding', 'accuracy.', 'You', 'can', 'reach', 'their', 'support', 'team', 'at', 'support@deepad.ai', 'or', 'call', '+49', '30', '1234', '5678', 'to', 'learn', 'more', 'about', 'their', 'product', 'offerings.', 'Their', 'latest', 'research', 'papers', 'are', 'available', 'at', 'http://www.deepad.ai/research.\\n\\nTo', 'stay', 'ahead', 'of', 'the', 'curve,', 'businesses', 'must', 'stay', 'informed', 'about', 'the', 'legal', 'regulations', 'surrounding', 'digital', 'marketing.', 'In', 'an', 'article', 'by', 'LegalEagle', '(visit:', 'https://www.legaleagle.org/marketing-laws),', 'experts', 'warned', 'of', 'the', 'stringent', 'data', 'privacy', 'laws', 'being', 'implemented', 'in', 'regions', 'like', 'the', 'EU.', 'For', 'more', 'legal', 'insights,', 'contact', 'their', 'team', 'at', 'info@legaleagle.org', 'or', 'call', '+33', '1', '2345', '6789.\\n\\nAnother', 'trend', 'worth', 'noting', 'is', 'the', 'increasing', 'influence', 'of', 'influencer', 'marketing.', 'Influencers', 'are', 'now', 'a', 'critical', 'part', 'of', 'most', 'brand', 'campaigns.', 'Platforms', 'like', 'InfluenceMe', '(website:', 'http://www.influenceme.io)', 'connect', 'brands', 'with', 'influencers', 'across', 'the', 'globe.', 'To', 'learn', 'more', 'about', 'their', 'services,', 'email', 'partners@influenceme.io', 'or', 'reach', 'their', 'international', 'office', 'at', '+61', '3', '9876', '5432.\\n\\nA', 'representative', 'from', 'BrandVision', '(email:', 'contact@brandvision.com,', 'website:', 'https://www.brandvision.com)', 'explained', 'that', 'the', 'shift', 'toward', 'more', 'authentic,', 'relatable', 'content', 'is', 'driving', 'higher', 'conversion', 'rates.', 'For', 'more', 'details', 'on', 'how', 'BrandVision', 'operates,', 'call', 'their', 'office', 'at', '+1-202-555-2244.\\n\\nIn', 'addition,', 'mobile', 'advertising', 'is', 'expected', 'to', 'dominate', 'digital', 'marketing.', 'According', 'to', 'MobileMarketers', 'Inc.', '(website:', 'http://www.mobilemarketersinc.com),', 'brands', 'that', 'optimize', 'their', 'campaigns', 'for', 'mobile', 'platforms', 'are', 'seeing', 'better', 'engagement', 'rates', 'than', 'ever', 'before.', 'For', 'more', 'information,', 'contact', 'them', 'at', 'hello@mobilemarketersinc.com', 'or', 'call', '+1-415-555-3344.\\n\\nLastly,', 'customer', 'experience', 'will', 'continue', 'to', 'shape', 'the', 'future', 'of', 'digital', 'marketing.', 'Companies', 'like', 'CXInnovators', '(website:', 'https://www.cxinnovators.com)', 'are', 'pioneering', 'new', 'ways', 'to', 'create', 'seamless,', 'personalized', 'customer', 'journeys.', 'Their', 'CEO,', 'Paul', 'Jenkins,', 'can', 'be', 'reached', 'at', 'paul.jenkins@cxinnovators.com', 'or', 'at', '+1-646-555-9988', 'for', 'more', 'insights.', 'The', 'CXInnovators', 'blog,', 'updated', 'weekly,', 'provides', 'valuable', 'tips', 'on', 'enhancing', 'customer', 'experience', 'at', 'https://www.cxinnovators.com/blog.\\n\\nTo', 'sum', 'it', 'up,', 'the', 'future', 'of', 'digital', 'marketing', 'lies', 'in', 'harnessing', 'data,', 'embracing', 'AI,', 'and', 'prioritizing', 'customer', 'experience.', 'Whether', 'you’re', 'just', 'starting', 'out', 'or', 'looking', 'to', 'enhance', 'your', 'existing', 'strategies,', 'there’s', 'no', 'shortage', 'of', 'tools', 'and', 'resources', 'available', 'to', 'help', 'you', 'succeed.', 'If', 'you', 'have', 'questions,', 'feel', 'free', 'to', 'drop', 'an', 'email', 'to', 'info@digitalfuture2025.com', 'or', 'give', 'us', 'a', 'call', 'at', '+1-888-555-1234.', 'You', 'can', 'also', 'find', 'more', 'articles', 'like', 'this', 'one', 'at', 'https://www.digitalfuture2025.com/articles.']\n",
      "\n",
      "                                                                                                                                        معایب\n",
      "معایب توکنایزر مبتنی بر فضای خالی شامل عدم دقت در تفکیک کلمات و عدم مدیریت نشانه‌گذاری است که ممکن است باعث شناسایی نادرست کلمات ترکیبی و نگه‌داشتن نشانه‌ها به عنوان بخشی از کلمات شود. همچنین این توکنایزر قادر به شناسایی زبان‌های خاص و موارد ویژه مانند ایمیل‌ها و شماره تلفن‌ها نیست و در متون بزرگ            کارایی کمتری دارد.                                                                                                                              \n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش سوم:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2024-10-17T14:29:55.634164Z",
     "start_time": "2024-10-17T14:29:55.621391Z"
    }
   },
   "source": [
    "#write your code here\n",
    "\n",
    "print(\n",
    "    \"قسمت اول                                                                                                                              \")\n",
    "print(\n",
    "    \" الگوریتم Levenshtein Distance                                                                                                                \")\n",
    "print(\n",
    "    \"فاصله Levenshtein تعداد حداقل عملیات‌های ویرایشی (اضافه‌کردن حذف‌کردن یا جایگزینی یک کاراکتر) است که برای تبدیل یک رشته به رشته دیگر لازم است\"\n",
    "    \"که هزینه جایگزینی در این روش برابر با ۲ هست (طق اسلاید) ولی هرچی سزچ زدم هزینه رو همون ۱ گرفته بودن برای همشون من هم ۱ گرفتم              \")\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "for i in range(m + 1):\n",
    "    distance[i][0] = i\n",
    "for j in range(n + 1):\n",
    "    distance[0][j] = j\n",
    "formula\n",
    "for j in range(1, n + 1):\n",
    "    cost = 0 if str_1[i - 1] == B[j - 1] else 1\n",
    "    distance[i][j] = min(\n",
    "                distance[i - 1][j] + 1,      # delete\n",
    "                distance[i][j - 1] + 1,      # insert\n",
    "                distance[i - 1][j - 1] + cost  # substitution\n",
    "            )\n",
    "\"\"\")\n",
    "\n",
    "print(\n",
    "    \" الگوریتم Damerau-Levenshtein Distance                                                                                                     \")\n",
    "print(\n",
    "    \" نسخه‌ای توسعه‌یافته از فاصله Levenshtein است که علاوه بر عملیات‌های ویرایشی استاندارد (اضافه‌کردن، حذف‌کردن، جایگزینی) جابجایی دو کاراکتر مجاور را نیز در نظر می‌گیرد.                                                                                                        \")\n",
    "\n",
    "print(\"\"\"\n",
    "formula\n",
    "for j in range(1, n + 1):\n",
    "    cost = 0 if str_1[i - 1] == B[j - 1] else 1\n",
    "    distance[i][j] = min(\n",
    "        distance[i - 1][j] + 1,      # delete\n",
    "        distance[i][j - 1] + 1,      # insert\n",
    "        distance[i - 1][j - 1] + cost  # substitution\n",
    "    )\n",
    "    displacement\n",
    "    if i > 1 and j > 1 and str_1[i - 1] == B[j - 2] and str_1[i - 2] == B[j - 1]:\n",
    "        distance[i][j] = min(distance[i][j], distance[i - 2][j - 2] + 1)\n",
    "\"\"\")\n",
    "print(\"=\" * 150)\n",
    "print(\n",
    "    \"قسمت دوم                                                                                                                              \")\n",
    "print(\"\\n\")\n",
    "training_data = [\n",
    "    (\"kitten\", \"sitting\"),\n",
    "    (\"saturday\", \"sunday\"),\n",
    "    (\"book\", \"back\"),\n",
    "    (\"algorithm\", \"logarithm\"),\n",
    "    (\"\", \"test\"),\n",
    "    (\"abc\", \"acb\")\n",
    "]\n",
    "\n",
    "\n",
    "def get_initial_variable(str_1: str, str_2: str):\n",
    "    m = len(str_1)\n",
    "    n = len(str_2)\n",
    "    distance = [[0] * (n + 1) for i in range(m + 1)]\n",
    "    for i in range(m + 1):\n",
    "        distance[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        distance[0][j] = j\n",
    "    return distance, m, n\n",
    "\n",
    "\n",
    "def find_minimum_distance(str_1: str, str_2: str, distance, i, j):\n",
    "    cost = 0 if str_1[i - 1] == str_2[j - 1] else 1\n",
    "    distance[i][j] = min(\n",
    "        distance[i - 1][j] + 1,  # delete\n",
    "        distance[i][j - 1] + 1,  # insert\n",
    "        distance[i - 1][j - 1] + cost  # substitution\n",
    "    )\n",
    "\n",
    "\n",
    "def levenshtein_distance(str_1: str, str_2: str):\n",
    "    distance, m, n = get_initial_variable(str_1, str_2)\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            find_minimum_distance(str_1, str_2, distance, i, j)\n",
    "    return distance[m][n]\n",
    "\n",
    "\n",
    "print(\"levenshtein_distance algorithm\")\n",
    "for (str_1, str_2) in training_data:\n",
    "    levenshtein_result = levenshtein_distance(str_1, str_2)\n",
    "    print(f\"for strings: ({str_1}, {str_2}) the result is : {levenshtein_result}\")\n",
    "\n",
    "\n",
    "def damerau_levenshtein_distance(str_1: str, str_2: str):\n",
    "    distance, m, n = get_initial_variable(str_1, str_2)\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            find_minimum_distance(str_1, str_2, distance, i, j)\n",
    "            # displacement adjacent char\n",
    "            if i > 1 and j > 1 and str_1[i - 1] == str_2[j - 2] and str_1[i - 2] == str_2[j - 1]:\n",
    "                distance[i][j] = min(distance[i][j], distance[i - 2][j - 2] + 1)\n",
    "    return distance[m][n]\n",
    "\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"damerau_levenshtein algorithm\")\n",
    "for (str_1, str_2) in training_data:\n",
    "    levenshtein_result = damerau_levenshtein_distance(str_1, str_2)\n",
    "    print(f\"for strings: ({str_1}, {str_2}) the result is : {levenshtein_result}\")\n",
    "\n",
    "print(\"\\nhttps://www.geeksforgeeks.org/introduction-to-levenshtein-distance/\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قسمت اول                                                                                                                              \n",
      " الگوریتم Levenshtein Distance                                                                                                                \n",
      "فاصله Levenshtein تعداد حداقل عملیات‌های ویرایشی (اضافه‌کردن حذف‌کردن یا جایگزینی یک کاراکتر) است که برای تبدیل یک رشته به رشته دیگر لازم استکه هزینه جایگزینی در این روش برابر با ۲ هست (طق اسلاید) ولی هرچی سزچ زدم هزینه رو همون ۱ گرفته بودن برای همشون من هم ۱ گرفتم              \n",
      "\n",
      "\n",
      "for i in range(m + 1):\n",
      "    distance[i][0] = i\n",
      "for j in range(n + 1):\n",
      "    distance[0][j] = j\n",
      "formula\n",
      "for j in range(1, n + 1):\n",
      "    cost = 0 if str_1[i - 1] == B[j - 1] else 1\n",
      "    distance[i][j] = min(\n",
      "                distance[i - 1][j] + 1,      # delete\n",
      "                distance[i][j - 1] + 1,      # insert\n",
      "                distance[i - 1][j - 1] + cost  # substitution\n",
      "            )\n",
      "\n",
      " الگوریتم Damerau-Levenshtein Distance                                                                                                     \n",
      " نسخه‌ای توسعه‌یافته از فاصله Levenshtein است که علاوه بر عملیات‌های ویرایشی استاندارد (اضافه‌کردن، حذف‌کردن، جایگزینی) جابجایی دو کاراکتر مجاور را نیز در نظر می‌گیرد.                                                                                                        \n",
      "\n",
      "formula\n",
      "for j in range(1, n + 1):\n",
      "    cost = 0 if str_1[i - 1] == B[j - 1] else 1\n",
      "    distance[i][j] = min(\n",
      "        distance[i - 1][j] + 1,      # delete\n",
      "        distance[i][j - 1] + 1,      # insert\n",
      "        distance[i - 1][j - 1] + cost  # substitution\n",
      "    )\n",
      "    displacement\n",
      "    if i > 1 and j > 1 and str_1[i - 1] == B[j - 2] and str_1[i - 2] == B[j - 1]:\n",
      "        distance[i][j] = min(distance[i][j], distance[i - 2][j - 2] + 1)\n",
      "\n",
      "======================================================================================================================================================\n",
      "قسمت دوم                                                                                                                              \n",
      "\n",
      "\n",
      "levenshtein_distance algorithm\n",
      "for strings: (kitten, sitting) the result is : 3\n",
      "for strings: (saturday, sunday) the result is : 3\n",
      "for strings: (book, back) the result is : 2\n",
      "for strings: (algorithm, logarithm) the result is : 3\n",
      "for strings: (, test) the result is : 4\n",
      "for strings: (abc, acb) the result is : 2\n",
      "==================================================\n",
      "damerau_levenshtein algorithm\n",
      "for strings: (kitten, sitting) the result is : 3\n",
      "for strings: (saturday, sunday) the result is : 3\n",
      "for strings: (book, back) the result is : 2\n",
      "for strings: (algorithm, logarithm) the result is : 3\n",
      "for strings: (, test) the result is : 4\n",
      "for strings: (abc, acb) the result is : 1\n",
      "\n",
      "https://www.geeksforgeeks.org/introduction-to-levenshtein-distance/\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3u2F7h_YMmvp"
   },
   "source": [
    "# **سوال دوم:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vAFTulEMpUx"
   },
   "source": [
    "*بخش اول:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mqfcKeLQMrpU",
    "ExecuteTime": {
     "end_time": "2024-10-17T15:58:49.908373Z",
     "start_time": "2024-10-17T15:58:49.903010Z"
    }
   },
   "source": [
    "print(\n",
    "    \" قسمت اول                                                                                                                                    \")\n",
    "print(\"\"\"\n",
    "فرآیند آموزش توکنایزر WordPiece با  واژگان ابتدایی که شامل تمام کاراکترهای تک‌واژه‌ای موجود در زبان است آغاز می‌شود. در این فرآیند تمامی جفت‌های متوالی کاراکترها در داده‌های آموزشی شناسایی و فراوانی هر جفت کاراکتر محاسبه می‌شود. سپس جفت کاراکتری که بیشترین فراوانی را دارد به عنوان یک توکن جدید به واژگان اضافه شده و داده‌ها به‌روزرسانی می‌شوند. این مراحل به صورت تکراری انجام می‌شوند تا واژگان به اندازه مطلوب برسد.                                    \n",
    "\"\"\")\n",
    "print(\"https://towardsdatascience.com/wordpiece-subword-based-tokenization-algorithm-1fbd14394ed7\")\n",
    "print(\"=\" * 150)\n",
    "print(\n",
    "    \" قسمت دوم                                                                                                                                    \")\n",
    "print(\n",
    "    \"توکنایزر WordPiece در مدل‌های زبانی معروفی مانند BERT، ALBERT و DistilBERT به کار رفته است.                                               \")\n",
    "print(\"\\nhttps://huggingface.co/docs/transformers/en/model_doc/bert\")\n",
    "print(\"=\" * 150)\n",
    "print(\n",
    "    \" قسمت سوم                                                                                                                                    \")\n",
    "print(\"\"\"\n",
    "به طور شهودی، WordPiece کمی با BPE متفاوت است زیرا هنگام ادغام دو نماد، ارزیابی می‌کند که چه چیزی را از دست می‌دهد تا اطمینان حاصل کند که این ادغام ارزشمند است. بنابراین، WordPiece برای داده‌های آموزشی خاص بهینه‌سازی شده است. WordPiece دارای اندازه دایره‌المعارف کمتری خواهد بود و در نتیجه پارامترهای کمتری برای آموزش نیاز دارد. هم‌چنین، همگرایی سریع‌تر خواهد بود.                                                                                              \n",
    "\"\"\")\n",
    "print(\"\\nhttps://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " قسمت اول                                                                                                                                    \n",
      "\n",
      "فرآیند آموزش توکنایزر WordPiece با  واژگان ابتدایی که شامل تمام کاراکترهای تک‌واژه‌ای موجود در زبان است آغاز می‌شود. در این فرآیند تمامی جفت‌های متوالی کاراکترها در داده‌های آموزشی شناسایی و فراوانی هر جفت کاراکتر محاسبه می‌شود. سپس جفت کاراکتری که بیشترین فراوانی را دارد به عنوان یک توکن جدید به واژگان اضافه شده و داده‌ها به‌روزرسانی می‌شوند. این مراحل به صورت تکراری انجام می‌شوند تا واژگان به اندازه مطلوب برسد.                                    \n",
      "\n",
      "https://towardsdatascience.com/wordpiece-subword-based-tokenization-algorithm-1fbd14394ed7\n",
      "======================================================================================================================================================\n",
      " قسمت دوم                                                                                                                                    \n",
      "توکنایزر WordPiece در مدل‌های زبانی معروفی مانند BERT، ALBERT و DistilBERT به کار رفته است.                                               \n",
      "\n",
      "https://huggingface.co/docs/transformers/en/model_doc/bert\n",
      "======================================================================================================================================================\n",
      " قسمت سوم                                                                                                                                    \n",
      "\n",
      "به طور شهودی، WordPiece کمی با BPE متفاوت است زیرا هنگام ادغام دو نماد، ارزیابی می‌کند که چه چیزی را از دست می‌دهد تا اطمینان حاصل کند که این ادغام ارزشمند است. بنابراین، WordPiece برای داده‌های آموزشی خاص بهینه‌سازی شده است. WordPiece دارای اندازه دایره‌المعارف کمتری خواهد بود و در نتیجه پارامترهای کمتری برای آموزش نیاز دارد. هم‌چنین، همگرایی سریع‌تر خواهد بود.                                                                                              \n",
      "\n",
      "\n",
      "https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*بخش دوم:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:40:14.897315Z",
     "start_time": "2024-10-17T15:40:14.052646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "file_path = 'data/ferdowsi.txt'\n",
    "\n",
    "# read first 90000 line as training data and use after this line as test data\n",
    "num_lines_to_read = 90000\n",
    "lines = []\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for current_line, line in enumerate(file, start=1):\n",
    "        lines.append(line.strip())\n",
    "        if current_line == num_lines_to_read:\n",
    "            break\n",
    "\n",
    "corpus = '\\n'.join(lines)\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece())\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=5000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[]\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(corpus.splitlines(), trainer)\n",
    "\n",
    "input_text = \"\"\"\n",
    "|بیامد که سازد همی رزمگاه\n",
    "|چو ماهوی سوری سپه را بدید\n",
    "|تو گفتی که جانش ز تن برپرید\n",
    "|ز بس جوشن و خود و زرین سپر\n",
    "|ز بس نیزه و گر ز وچاچی تبر\n",
    "|غمی شد برابر صفی برکشید\n",
    "|هوا نیلگون شد زمین ناپدید\n",
    "|چو بیژن سپه را همه راست کرد\n",
    "|به ایرانیان برکمین خواست کرد\n",
    "|بدانست ماهوی و از قلبگاه\n",
    "|خروشان برفت ازمیان سپاه\n",
    "|نگه کرد بیژن درفشش بدید\n",
    "|بدانست کو جست خواهد گزید\n",
    "|به برسام فرمود کز قلبگاه\n",
    "|به یکسو گذار آنک داری سپاه\n",
    "|نباید که ماهوی سوری ز جنگ\n",
    "|بترسد به جیحون کشد بی‌درنگ\n",
    "|به تیزی ازو چشم خود برمدار\n",
    "|که با او دگرگونه سازیم کار\n",
    "|چو برسام چینی درفشش بدید\n",
    "|سپه را ز لشکر به یکسو کشید\n",
    "|همی‌تاخت تاپیش ریگ فرب\n",
    "|پر آژنگ رخ پر ز دشنام لب\n",
    "|مر او را بریگ فرب دربیافت\n",
    "|رکابش گران کرد و اندر شتافت\n",
    "|چو نزدیک ماهو برابر به بود\n",
    "|نزد خنجر او را دلیری نمود\n",
    "|کمربند بگرفت و او را ز زین\n",
    "|برآورد و آسان بزد بر زمین\n",
    "|فرود آمد و دست او را ببست\n",
    "\"\"\"\n",
    "encoded = tokenizer.encode(input_text)\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "print(\"\\n\")\n",
    "print(\n",
    "    \"برای حل این سوال حدود 90000 خط اول رو به عنوان داده train درنظر گرفتم و 9219 خط بعدی رو به عنوان داده تست درنظر گرفتم.                                     \")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokens: ['|', 'بیامد', 'که', 'سازد', 'همی', 'رزمگاه', '|', 'چو', 'ماه', '##وی', 'سور', '##ی', 'سپه', 'را', 'بدید', '|', 'تو', 'گفتی', 'که', 'جانش', 'ز', 'تن', 'بر', '##پر', '##ید', '|', 'ز', 'بس', 'جوشن', 'و', 'خود', 'و', 'زرین', 'سپر', '|', 'ز', 'بس', 'نیزه', 'و', 'گر', 'ز', 'و', '##چ', '##اچ', '##ی', 'تبر', '|', 'غمی', 'شد', 'برابر', 'صف', '##ی', 'برکشید', '|', 'هوا', 'نیلگون', 'شد', 'زمین', 'ناپدید', '|', 'چو', 'بیژن', 'سپه', 'را', 'همه', 'راست', 'کرد', '|', 'به', 'ایرانیان', 'برک', '##می', '##ن', 'خواست', 'کرد', '|', 'بدانست', 'ماه', '##وی', 'و', 'از', 'قلبگاه', '|', 'خروشان', 'برفت', 'از', '##می', '##ان', 'سپاه', '|', 'نگه', 'کرد', 'بیژن', 'درفش', '##ش', 'بدید', '|', 'بدانست', 'کو', 'جست', 'خواهد', 'گزید', '|', 'به', 'برس', '##ام', 'فرمود', 'کز', 'قلبگاه', '|', 'به', 'یکسو', 'گذار', 'آنک', 'داری', 'سپاه', '|', 'نباید', 'که', 'ماه', '##وی', 'سور', '##ی', 'ز', 'جنگ', '|', 'بترسد', 'به', 'جیحون', 'کشد', 'بی\\u200cدرنگ', '|', 'به', 'تیزی', 'ازو', 'چشم', 'خود', 'برم', '##دار', '|', 'که', 'با', 'او', 'دگرگونه', 'سازیم', 'کار', '|', 'چو', 'برس', '##ام', 'چینی', 'درفش', '##ش', 'بدید', '|', 'سپه', 'را', 'ز', 'لشکر', 'به', 'یکسو', 'کشید', '|', 'همی\\u200cتاخت', 'تا', '##پیش', 'ریگ', 'فر', '##ب', '|', 'پر', 'آژنگ', 'رخ', 'پر', 'ز', 'دشنام', 'لب', '|', 'مر', 'او', 'را', 'بری', '##گ', 'فر', '##ب', 'در', '##بی', '##افت', '|', 'رک', '##اب', '##ش', 'گران', 'کرد', 'و', 'اندر', 'شتافت', '|', 'چو', 'نزدیک', 'ماه', '##و', 'برابر', 'به', 'بود', '|', 'نزد', 'خنجر', 'او', 'را', 'دلیری', 'نمود', '|', 'کمربند', 'بگرفت', 'و', 'او', 'را', 'ز', 'زین', '|', 'برآورد', 'و', 'آسان', 'بزد', 'بر', 'زمین', '|', 'فرود', 'آمد', 'و', 'دست', 'او', 'را', 'ببست']\n",
      "\n",
      "\n",
      "برای حل این سوال حدود 90000 خط اول رو به عنوان داده train درنظر گرفتم و 9219 خط بعدی رو به عنوان داده تست درنظر گرفتم.                                     \n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش سوم:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T17:57:55.270473Z",
     "start_time": "2024-10-17T17:57:52.250639Z"
    }
   },
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, n_gram: int):\n",
    "        self.n_gram = n_gram\n",
    "        self.n_gram_count_result = dict()\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text: str) -> List[str]:\n",
    "        return text.split()\n",
    "\n",
    "    def generate_ngrams(self, tokens: List[str]):\n",
    "        return list(zip(*[tokens[i:] for i in range(self.n_gram)]))\n",
    "\n",
    "    def train(self, corpus: str):\n",
    "        tokens = self.tokenize(corpus)\n",
    "        ngrams = self.generate_ngrams(tokens)\n",
    "        for ngram in ngrams:\n",
    "            prefix = ngram[:-1]\n",
    "            suffix = ngram[-1]\n",
    "            if prefix not in self.n_gram_count_result:\n",
    "                self.n_gram_count_result[prefix] = {}\n",
    "            if suffix not in self.n_gram_count_result[prefix]:\n",
    "                self.n_gram_count_result[prefix][suffix] = 0\n",
    "            self.n_gram_count_result[prefix][suffix] += 1\n",
    "\n",
    "    def generate_random_sentence(self, input_text: str, max_length: int):\n",
    "        tokens = self.tokenize(input_text)\n",
    "        sentence = tokens[:]\n",
    "        current = tuple(tokens[-(self.n_gram - 1):])\n",
    "        for _ in range(max_length):\n",
    "            if current in self.n_gram_count_result:\n",
    "                possible_next = list(self.n_gram_count_result[current])\n",
    "                next_word = random.choice(possible_next)\n",
    "                sentence.append(next_word)\n",
    "                current = tuple(sentence[-(self.n_gram - 1):])\n",
    "            else:\n",
    "                break\n",
    "        return ' '.join(sentence)\n",
    "\n",
    "\n",
    "with open('data/ferdowsi.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "bigram_model = NGramModel(2)\n",
    "four_gram_model = NGramModel(4)\n",
    "eight_gram_model = NGramModel(8)\n",
    "\n",
    "bigram_model.train(corpus)\n",
    "four_gram_model.train(corpus)\n",
    "eight_gram_model.train(corpus)\n",
    "\n",
    "input_text = \"\"\"\n",
    "|خداونت پیل و خداوند مور\n",
    "|بزرگی و نیک اختری زو شناس\n",
    "|وزو دار تا زنده باشی سپاس\n",
    "\"\"\"\n",
    "\n",
    "generated_bigram_sentence = bigram_model.generate_random_sentence(input_text, max_length=200)\n",
    "generated_four_gram_sentence = four_gram_model.generate_random_sentence(input_text, max_length=200)\n",
    "generated_eight_gram_sentence = eight_gram_model.generate_random_sentence(input_text, max_length=200)\n",
    "\n",
    "print(\"Bigram Sentence:\")\n",
    "print(generated_bigram_sentence)\n",
    "print(\"\\n4-gram Sentence:\")\n",
    "print(generated_four_gram_sentence)\n",
    "print(\"\\n8-gram Sentence:\")\n",
    "print(generated_eight_gram_sentence)\n",
    "\n",
    "print(\"=\"*150)\n",
    "print(\n",
    "    \" قسمت چهارم                                                                                                                                    \")\n",
    "print(\"\"\"\n",
    "با مشکلاتی چون افزایش ابعاد و پراکنده شدن داده‌ها مواجه‌اند که منجر به کاهش دقت و توانایی تعمیم مدل می‌شود. همچنین، نیاز به منابع محاسباتی بیشتری دارند و نمی‌توانند روابط معنایی را به خوبی شناسایی کنند.                                                                                                     \n",
    "\"\"\")\n",
    "print(\" مقابسه                                                                                                                            \")\n",
    "print(\"\"\"\n",
    "تحلیل بیگرام بیشتر بر تکرار عبارات و ساختارهای کوتاه تمرکز دارد، زیرا تنها به پیش‌بینی واژه بعدی بر اساس کلمه قبلی می‌پردازد. چها ر گرام جملات پیچیده‌تری ارائه می‌دهد که به ایجاد جملات معنادار کمک می‌کند. هشت‌ گرام به تنوع واژگان و ساختارهای پیچیده‌تری می‌انجامد که به با معنا بودن بیشتر متن کمک    می‌کند. با افزایش تعداد کلمات در هر سطح عمق معنایی و محتوای بیشتری شکل می‌گیرد. درنتیجه هرچه تعداد کلمات بیشتر شود، تنوع و پیچیدگی ساختاری نیز افزایش می‌یابد.                                                                                                                                      \n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Sentence:\n",
      "|خداونت پیل و خداوند مور |بزرگی و نیک اختری زو شناس |وزو دار تا زنده باشی سپاس |چنین چاک اندرست |سپهدار شیراوژنا مهترا |که گشتند لشکر آراستست |فریدونیان نیز دختر پرفسون |چنان بگسلد پاک پروردگار |بگفت وفرود آمد بپرسید کز شب روشنایی دریغ |همی پاسبان |ازان شاد |تهمتن بیک تن پشت کسی دادکن |ز ناهید پیدا درش |چو نازش کنند |برفتند پیران گرفتش یکایک بگشت |به پرده‌سرای |فراوان بیابی جزا ز دام |برآری یکی روشن وپادشا دادگر |بران جامه کرده کبود |نوشتی سرافراز جان آفرین |ابر سام |بزرگست وبخشنده و توش |چو پای کوب |بهر انجمن |مگر از رای |بزد ترک فرمود کسری غمی |بنیروی این آتش بجنبد که گوینده‌ام |بدین عهد او |بمانده بدان چیز جفتست با دلاور شکیب |بفرمود کاهنگران آورند |ور اومید کو چنان زار و یکباره چشم شادیت بر زیردستان شوند |نماند که مرد |خروشان برو سوگواری کنید |خروشی چوشیر ژیان آوریم |چه گویی سخن‌های بی‌کام جویندهٔ کین برادر مخوان |سخنها نباید سپردن گرفت |کمانش تو |نبشته سوی آتش چو مرگ آورد روز گفت الانان فرستادشان تیره گرددش بخت وز کینه دلها بمهرت همی تاج مهان |نوشتند عهدی درست |شهنشاه بهرام یل زابلی |که اینست فرهنگ ماست |جوانمردی وداد |چنو باز جوید سوی نیکخواه |یکایک بگفتند با مغز آهرمنست |جهان پیر |مراگر نمودی چو بنشید فرخ گرفتم بخم کمند |میان سپه برنهادند زرین\n",
      "\n",
      "4-gram Sentence:\n",
      "|خداونت پیل و خداوند مور |بزرگی و نیک اختری زو شناس |وزو دار تا زنده باشی سپاس |که زنده برفتی توازپیش جنگ |نه هنگام رایست و روز درنگ |چوبشنید طلخند آواز اوی |شد از جادویی تنش یک لخت کوه |نیامد همی از کشیدن ستوه |چهارم چنان دیدم ای نامدار |که مردی شدی تشنه بر جویبار |همی آب ماهی برو ریختی |سر تشنه از آب بگریختی |جهان مرد و آب از پس او رویم |به داد جهان‌آفرین بگرویم |به درویش داد آنچ بودش نهان |ز بالا و از چهر من |بیمیخت با جان تو کارزار |چنین گفت با شاه‌زاده تخوار |که گر جست خواهی همی |بکوشی که این کینه کاهی همی |نگه کن که فرجام من چون بود |بدو گفت جویان که ایمن مشو |ز جویان و از خنجر سرد رو |که اکنون به رزمی بزرگ اندرست |دریده به چنگال گرگ |شده پاره پاره کنان و کشان |ز رستم به دشمن رسیده نشان |روانش چو پردخته شد |شهنشاه زان رنجها رخته شد |ازان مهتران نام لهراسب ماند |که از دفتر شاه کس برنخواند |ببیژن بفرمود تا با سپاه |بمکران بباشد یکی چندگاه |نجوید جز از خوبی و خرمی |به پیش تو گر برگشایم زبان |که گیو دلاور به گردان چه کرد |دلت سیر گردد به دشت |چو این شهر بینی نشاید گذشت |تذروان و طاووس و کبک دری\n",
      "\n",
      "8-gram Sentence:\n",
      "|خداونت پیل و خداوند مور |بزرگی و نیک اختری زو شناس |وزو دار تا زنده باشی سپاس |نمودار گفتار من من بسم |بدین در نکوهیدهٔ هرکسم |که چندان بزرگی و شاهی و گنج |نبد در زمانه کس از من به رنج |همان نیز چندان سلیح و سپاه |گرانمایه اسپان و تخت و کلاه |ز پیش نیاکان ما یافتی |چو در بندگی تیز بشتافتی |چه مایه جهان داشت لهراسپ شاه |نکردی گذر سوی آن بارگاه |چو او شهر ایران به گشتاسپ داد |نیامد ترا هیچ زان تخت یاد |سوی او یکی نامه ننوشته‌ای |از آرایش بندگی گشته‌ای |نرفتی به درگاه او بنده‌وار |نخواهی به گیتی کسی شهریار |ز هوشنگ و جم و فریدون گرد |که از تخم ضحاک شاهی ببرد |همی رو چنین تا سر کیقباد |که تاج فریدون به سر بر نهاد |چو گشتاسپ شه نیست یک نامدار |به رزم و به بزم و به دانش گرای |سرافراز محمود فرخنده‌رای |کزویست نام بزرگی به جای |جهاندار ابوالقاسم پر خرد |که رایش همی از خرد برخورد |همی باد تا جاودان شاد دل |ز رنج و ز غم گشته آزاد دل |شهنشاه ایران و زابلستان |ز قنوج تا مرز کابلستان |برو آفرین باد و بر لشکرش |چه بر خویش و بر دوده و کشورش |جهاندار سالار او میر نصر |کزو شادمانست گردنده عصر |دریغش نیاید ز بخشیدن ایچ |نه\n",
      "======================================================================================================================================================\n",
      " قسمت چهارم                                                                                                                                    \n",
      "\n",
      "با مشکلاتی چون افزایش ابعاد و پراکنده شدن داده‌ها مواجه‌اند که منجر به کاهش دقت و توانایی تعمیم مدل می‌شود. همچنین، نیاز به منابع محاسباتی بیشتری دارند و نمی‌توانند روابط معنایی را به خوبی شناسایی کنند.                                                                                                     \n",
      "\n",
      " مقابسه                                                                                                                            \n",
      "\n",
      "تحلیل بیگرام بیشتر بر تکرار عبارات و ساختارهای کوتاه تمرکز دارد، زیرا تنها به پیش‌بینی واژه بعدی بر اساس کلمه قبلی می‌پردازد. چها ر گرام جملات پیچیده‌تری ارائه می‌دهد که به ایجاد جملات معنادار کمک می‌کند. هشت‌ گرام به تنوع واژگان و ساختارهای پیچیده‌تری می‌انجامد که به با معنا بودن بیشتر متن کمک    می‌کند. با افزایش تعداد کلمات در هر سطح عمق معنایی و محتوای بیشتری شکل می‌گیرد. درنتیجه هرچه تعداد کلمات بیشتر شود، تنوع و پیچیدگی ساختاری نیز افزایش می‌یابد.                                                                                                                                      \n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش چهارم:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:37:29.024461Z",
     "start_time": "2024-10-17T15:37:26.976751Z"
    }
   },
   "source": [
    "print(\n",
    "    \" قسمت اول                                                                                                                                    \")\n",
    "print(\"\"\"\n",
    "معیاری است که در دسته ارزیابی ذاتی قرار دارد و از احتمال استفاده میکند و  برای ارزیابی کیفیت یک مدل زبان استفاده می‌شود. به طور خاص نشان‌دهنده توانایی مدل در پیش‌بینی یک توالی کلمات است                                                                                                                          .\n",
    "فرمول برابر است با میانگین لگاریتم معکوس احتمال وقوع یک کلمه با توجه به مدل است.                                                                           \n",
    " اگر مدل دارای مقدار کمتری از این معیار  باشد، نشان‌دهنده این است که مدل   توانایی خوبی در پیش‌بینی توالی کلمات دارد.                                                                                                              \n",
    "\"\"\")\n",
    "print(\"=\" * 150)\n",
    "print(\n",
    "    \" قسمت دوم                                                                                                                                    \")\n",
    "import math\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, n_gram):\n",
    "        self.n_gram = n_gram\n",
    "        self.n_gram_count_result = dict()\n",
    "        self.total_count = 0\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        return text.split()\n",
    "\n",
    "    def generate_ngrams(self, tokens: List[str]):\n",
    "        return list(zip(*[tokens[i:] for i in range(self.n_gram)]))\n",
    "\n",
    "    def train(self, text: str):\n",
    "        tokens = self.tokenize(text)\n",
    "        ngrams = self.generate_ngrams(tokens)\n",
    "        for ngram in ngrams:\n",
    "            prefix = ngram[:-1]\n",
    "            suffix = ngram[-1]\n",
    "            if prefix not in self.n_gram_count_result:\n",
    "                self.n_gram_count_result[prefix] = {}\n",
    "            if suffix not in self.n_gram_count_result[prefix]:\n",
    "                self.n_gram_count_result[prefix][suffix] = 0\n",
    "            self.n_gram_count_result[prefix][suffix] += 1\n",
    "            self.total_count += 1\n",
    "\n",
    "    def get_probability(self, prefix: Tuple[str], suffix: str) -> float:\n",
    "        if prefix in self.n_gram_count_result and suffix in self.n_gram_count_result[prefix]:\n",
    "            return self.n_gram_count_result[prefix][suffix] / sum(self.n_gram_count_result[prefix].values())\n",
    "        else:\n",
    "            return 1 / self.total_count\n",
    "\n",
    "    def calculate_perplexity(self, text: str) -> float:\n",
    "        tokens = self.tokenize(text)\n",
    "        ngrams = self.generate_ngrams(tokens)\n",
    "        if len(ngrams) == 0:\n",
    "            return 1e10  # ten billion\n",
    "        log_probability_sum = 0\n",
    "        for ngram in ngrams:\n",
    "            prefix = ngram[:-1]\n",
    "            suffix = ngram[-1]\n",
    "            probability = self.get_probability(prefix, suffix)\n",
    "            log_probability_sum += (-math.log(probability))\n",
    "        return math.exp(log_probability_sum / len(ngrams))\n",
    "\n",
    "\n",
    "with open('data/ferdowsi.txt', 'r', encoding='utf-8') as file:\n",
    "    ferdowsi_corpus = file.read()\n",
    "\n",
    "with open('data/modern_poet.txt', 'r', encoding='utf-8') as file:\n",
    "    modern_poet_corpus = file.readlines()\n",
    "\n",
    "with open('data/hafez.txt', 'r', encoding='utf-8') as file:\n",
    "    hafez_corpus = file.readlines()\n",
    "\n",
    "four_gram = NGramModel(n_gram=8)\n",
    "four_gram.train(ferdowsi_corpus)\n",
    "\n",
    "hafez_perplexities = [four_gram.calculate_perplexity(text.strip()) for text in hafez_corpus]\n",
    "average_hafez_perplexity = sum(hafez_perplexities) / len(hafez_perplexities)\n",
    "\n",
    "modern_poet_perplexities = [four_gram.calculate_perplexity(text.strip()) for text in modern_poet_corpus]\n",
    "modern_poet_perplexity = sum(modern_poet_perplexities) / len(modern_poet_perplexities)\n",
    "\n",
    "print(f\"Average Perplexity Hafez: {average_hafez_perplexity}\")\n",
    "print(f\"Average Perplexity Modern Poet: {modern_poet_perplexity}\")\n",
    "\n",
    "print(\"=\" * 150)\n",
    "print(\n",
    "    \" قسمت سوم                                                                                                                                    \")\n",
    "print(\"\"\"\n",
    "برای شعر های حافظ برخی از کلمات یا ترکیبشون  در داده‌های حافظ وجود ندارد و این باعث می‌شود که احتمال آن‌ها صفر شود، که در نتیجه، سرگشتگی به بی نهایت        می‌رسید. برای جلوگیری از این مشکل به جای بی نهایت از یک عدد بزرگ ۱۰ میلیارد استفاده کردم و عددی بزرگ بدست امد ولی دیگر بینهایت نمیشد تا بتوان مقایسه ای نسبی کرد.نتیجه بدست امده نشان میدهد ک برای شعر حافظ این داده ها مناسب نبودن و شعر سعدی تفاوت دارن با شعر حافظ و کلمات استفاده شده و ترکیب انها فرق دارد.\n",
    "\n",
    "برای شعر های نو عددی ک بدست اومد نسبت به حافظکمتر شد ولی بازم عدد نسبتا بزرگی هست و  این یعنی  که مدل در پیش‌بینی اشعار نو نیز بسیار ضعیف است و کلا سبک و ترکیب           کلمات شعر نو با شعر فردوسی فرق داره ولی باز بهتر از شعر حافظ شد                                                                                  \n",
    "             \n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " قسمت اول                                                                                                                                    \n",
      "\n",
      "معیاری است که در دسته ارزیابی ذاتی قرار دارد و از احتمال استفاده میکند و  برای ارزیابی کیفیت یک مدل زبان استفاده می‌شود. به طور خاص نشان‌دهنده توانایی مدل در پیش‌بینی یک توالی کلمات است                                                                                                                          .\n",
      "فرمول برابر است با میانگین لگاریتم معکوس احتمال وقوع یک کلمه با توجه به مدل است.                                                                           \n",
      " اگر مدل دارای مقدار کمتری از این معیار  باشد، نشان‌دهنده این است که مدل   توانایی خوبی در پیش‌بینی توالی کلمات دارد.                                                                                                              \n",
      "\n",
      "======================================================================================================================================================\n",
      " قسمت دوم                                                                                                                                    \n",
      "Average Perplexity Hafez: 7030672481.863614\n",
      "Average Perplexity Modern Poet: 562047.9999999923\n",
      "======================================================================================================================================================\n",
      " قسمت سوم                                                                                                                                    \n",
      "\n",
      "برای شعر های حافظ برخی از کلمات یا ترکیبشون  در داده‌های حافظ وجود ندارد و این باعث می‌شود که احتمال آن‌ها صفر شود، که در نتیجه، سرگشتگی به بی نهایت        می‌رسید. برای جلوگیری از این مشکل به جای بی نهایت از یک عدد بزرگ ۱۰ میلیارد استفاده کردم و عددی بزرگ بدست امد ولی دیگر بینهایت نمیشد تا بتوان مقایسه ای نسبی کرد.نتیجه بدست امده نشان میدهد ک برای شعر حافظ این داده ها مناسب نبودن و شعر سعدی تفاوت دارن با شعر حافظ و کلمات استفاده شده و ترکیب انها فرق دارد.\n",
      "\n",
      "برای شعر های نو عددی ک بدست اومد نسبت به حافظکمتر شد ولی بازم عدد نسبتا بزرگی هست و  این یعنی  که مدل در پیش‌بینی اشعار نو نیز بسیار ضعیف است و کلا سبک و ترکیب           کلمات شعر نو با شعر فردوسی فرق داره ولی باز بهتر از شعر حافظ شد                                                                                  \n",
      "             \n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **سوال سوم:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش اول:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T16:05:33.202275Z",
     "start_time": "2024-10-17T16:05:33.196353Z"
    }
   },
   "source": [
    "print(\n",
    "    \" قسمت اول                                                                                                                                    \")\n",
    "print(\"\"\"\n",
    "استفاده از n-gram به عنوان روشی برای طبقه‌بندی متن در پردازش زبان طبیعی شامل استخراج توالی‌های n-کلمه‌ای از متون می‌شود. این توالی‌ها به عنوان ویژگی‌های مدل شناخته می‌شوند و می‌توانند اطلاعات مهمی درباره ساختار و محتوای متن ارائه دهند. پس از استخراج n-gram‌ها، آن‌ها به یک ماتریس ویژگی تبدیل می‌شوند که نشان‌دهنده حضور یا عدم حضور هر n-gram در هر متن است.این ماتریس می‌تواند به عنوان ورودی برای الگوریتم‌های یادگیری ماشین مانند رگرسیون لجستیک، بیز، یا درخت تصمیم استفاده شود. با آموزش مدل بر روی مجموعه داده‌های آموزشی، می‌توانیم آن را برای طبقه‌بندی متن‌های جدید به کار ببریم.  این روش به ما کمک می‌کند تا متن‌ها را بر اساس ویژگی‌های زبانی و ساختاری که از n-gram‌ها به دست آمده‌اند، طبقه‌بندی کنیم.                       \n",
    "\"\"\")\n",
    "print(\"=\" * 150)\n",
    "print(\n",
    "    \" قسمت دوم                                                                                                                                    \")\n",
    "print(\"\"\"\n",
    "افزایش مقدار n در n-gram در صورت کمبود داده‌ها می‌تواند به مشکلاتی چون افزایش ابعاد و عدم وجود n-gram‌های کافی منجر شود که در نتیجه احتمال بیشتر کلمات صفر میشه. همچنین، این کار ممکن است حساسیت به نویز را افزایش داده و زمان و هزینه محاسباتی را بالا ببرد. در نهایت، مدل ممکن است به داده‌های آموزشی وابسته‌تر شده و وقتی داده تست بدهیم نتیجه خوبی برنگرداند و حتی اصلا نتواند جوابی بدهد                                                                  \n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " قسمت اول                                                                                                                                    \n",
      "\n",
      "استفاده از n-gram به عنوان روشی برای طبقه‌بندی متن در پردازش زبان طبیعی شامل استخراج توالی‌های n-کلمه‌ای از متون می‌شود. این توالی‌ها به عنوان ویژگی‌های مدل شناخته می‌شوند و می‌توانند اطلاعات مهمی درباره ساختار و محتوای متن ارائه دهند. پس از استخراج n-gram‌ها، آن‌ها به یک ماتریس ویژگی تبدیل می‌شوند که نشان‌دهنده حضور یا عدم حضور هر n-gram در هر متن است.این ماتریس می‌تواند به عنوان ورودی برای الگوریتم‌های یادگیری ماشین مانند رگرسیون لجستیک، بیز، یا درخت تصمیم استفاده شود. با آموزش مدل بر روی مجموعه داده‌های آموزشی، می‌توانیم آن را برای طبقه‌بندی متن‌های جدید به کار ببریم.  این روش به ما کمک می‌کند تا متن‌ها را بر اساس ویژگی‌های زبانی و ساختاری که از n-gram‌ها به دست آمده‌اند، طبقه‌بندی کنیم.                       \n",
      "\n",
      "======================================================================================================================================================\n",
      " قسمت دوم                                                                                                                                    \n",
      "\n",
      "افزایش مقدار n در n-gram در صورت کمبود داده‌ها می‌تواند به مشکلاتی چون افزایش ابعاد ویژگی و عدم وجود n-gram‌های کافی منجر شود که در نتیجه احتمال بعضی کلمات صفر میشه. همچنین، این کار ممکن است حساسیت به نویز را افزایش داده و زمان و هزینه محاسباتی را بالا ببرد. در نهایت، مدل ممکن است به داده‌های آموزشی وابسته‌تر شده و وقتی داده تست بدهیم نتیجه خوبی برنگرداند و حتی اصلا نتواند جوابی بدهد                                                                  \n",
      "\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*بخش دوم:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:37:09.556377Z",
     "start_time": "2024-10-17T15:37:06.197840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\n",
    "    \" قسمت اول و دوم                                                                                                                                    \")\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "\n",
    "data_file = \"data/digikala.csv\"\n",
    "corpus = pd.read_csv(data_file)\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=5000,\n",
    "    min_frequency=2\n",
    ")\n",
    "# Use first 2700 line for training\n",
    "tokenizer.train_from_iterator(list(corpus.Text)[:2700], trainer=trainer)\n",
    "\n",
    "tokenized_dataset = []\n",
    "for comment in list(corpus.Text):\n",
    "    encoded = tokenizer.encode(comment.strip())\n",
    "    tokenized_dataset.append(encoded.tokens)\n",
    "\n",
    "print(\"5 lines of tokenized dataset\")\n",
    "for token in tokenized_dataset[2800:2805]:\n",
    "    print(token)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " قسمت اول و دوم                                                                                                                                    \n",
      "\n",
      "\n",
      "\n",
      "5 lines of tokenized dataset\n",
      "['4', ' ماهه ', 'خریدم ', 'ب', 'شدت ', 'راضیم ', 'از ', 'عملکرد ', '  ', 'طراحی ', 'عالی ', 'جنس ', 'کفی ', 'خوب ', 'عملکرد ', 'و ', 'اتو', 'کشی ', 'محش', 'ر ', 'من ', 'ب', 'شدت ', 'رو ', 'اتو ', 'شدن ', 'لباس', 'ام ', 'وس', 'واس ', 'دارم  ', 'گ', 'ا', 'ها ', 'حتی ', 'کت ', 'و ', 'شلو', 'ار ', 'و ', 'میدن ', 'اتو', 'شویی ', 'پ', 'سش ', 'ک ', 'میگیرم ', 'باز ', 'خودم ', 'ی ', 'دستی ', 'بهش ', 'میک', 'شم ', ' ', 'این محصول ', 'منو ', 'راضی ', 'کرد']\n",
      "['من ', 'قبلا ', 'کتری ', 'برقی ', 'بوش ', 'مدل ', 'T', 'W', 'K', '8', '6', '13 ', 'را ', 'خریده ', 'بودم که ', 'همون', 'جا ', 'در ', 'مور', 'دش ', 'نوشتم ', ' این ', 'یکی ', 'مدل ', 'را برای ', 'مادرم ', 'خریدم که ', 'متاسفانه ', 'متوجه شدم ', 'کیفیت ساختش ', 'نسبت به ', 'اون ', 'مد', 'لی که ', 'خودم ', 'پارسال ', 'گرفتم ', 'خیلی ', 'پایین ', 'اومده ', ' ', 'دسته ', 'کتری ', 'کمی ', 'لق ', 'هست و ', ' ', 'قی', 'ژ', '  ', 'قی', 'ژ', ' ', 'میکنه ', 'انگار ', 'میخواد ', 'از ', 'جا ', 'کنده ', 'بشه ', 'مح', 'ف', 'ظه ', 'پلاستیکی ', 'مخصوص ', 'آب ', 'جوش ', 'داخلش ', 'هم خیلی ', 'ب', 'د و ', 'بی کیفیت ', ' ', 'ت', 'ز', 'ریق ', 'پلاستیک ', 'شده و ', 'این ', 'کیفیت ', 'برای ', 'برند ', 'بوش ', 'ف', 'اج', 'عه ', 'هست  ', 'مدل ', 'T', 'W', 'K', '8', '6', '13 ', 'هم نسبت به ', 'هزینه ', 'ای که ', 'براش ', 'پرداخت ', 'کردم ', 'آن', 'چن', 'ان ', 'تو', 'ج', 'یه ', 'خرید ', 'نداشت ', 'چه ', 'برسه ', 'به ', 'این مدل ', '8', '6', '11 ', 'که واقعا ', 'از همه ', 'چی', 'ش ', 'زدن ', 'و کیفیت ', 'به ', 'مع', 'ن', 'ای ', 'واقعی ', 'کل', 'مه ', 'چینی ', 'شده  ', 'مشخص ', 'هست ', 'جن', 'سی ', 'تولید ', 'کردن ', 'مناسب ', 'بازار ', 'ایرا', 'ن و ', 'نه ', 'بازار ', 'ارو', 'پا ', ' م', 'تاسف', 'م که ', 'فقط ', 'نام ', 'بوش ', 'را ', 'ید', 'ک ', 'میکشه ', ' از ', 'رنگ ', 'بندی ', 'هم ', 'بهتره ', 'حرفی ', 'نز', 'نم ', 'هر ', 'چه ', 'قدر ', 'مدل ', '8', '6', '13 ', 'زیبا و ', 'شکی', 'ل ', 'بود ', 'این مدل ', 'در نهایت ', 'بی ', 'سلی', 'ق', 'گی ', 'با ', 'ترکی', 'ب ', 'رنگ ', 'سفید ', 'و ', 'مشکی ', 'طراحی ', 'ش', 'ده']\n",
      "['من ', 'اتوی ', 'مخزن ', 'دار ', 'تفال ', 'رو ', 'برا', 'ساس ', 'تجربه ', 'ای که ', 'خواهرم ', 'داشت ', 'گرفتم ', ' البته ', 'ای', 'شون ', 'اتو ', 'را ', 'تقریبا ', '10 ', 'سال پیش ', 'گرفته ', 'بودن  ', 'اتو ', 'کیفیت ', 'اتو', 'دهی ', ' ', 'خوبی داره  ', 'اما ', 'میزان ', 'بخار', 'دهی', 'ش ', 'اصلا ', 'اون ', 'انتظار', 'ی که ', 'من ', 'داشتم ', 'نبود ', 'ضمن ', 'اینکه ', 'در ', 'توضیحات ', 'اتو', 'نوشته ', 'شده ', 'طول ', 'سیم ', '3', ' متر ', ' ', 'در', 'صورتی که ', '1 ', '5', ' م', 'تره ', '  واقعا ', 'از این ', 'مسئ', 'له ', 'م', 'تح', 'یر ', 'شدم ', ' متاسفانه ', 'من ', 'بعد از ', '11 ', 'روز ', 'از خرید ', 'اتو ', 'متوجه ', 'این ', 'مسئ', 'له ', 'شدم ', 'وگرنه ', 'حتما ', 'عو', 'د', 'ت ', 'می', 'دادم ', 'چون ', 'طول ', 'سیم ', 'اتو ', 'برای من ', 'جهت ', 'مص', 'ار', 'فی ', 'مثل ', 'اتو ', 'کردن ', 'پر', 'ده ', 'خیلی ', 'مهم ', 'هست']\n",
      "['اگر ', 'بین ', 'خرید ', 'اط', 'و ', 'مخ', 'ز', 'نی ', 'و ', 'پر', 'سی ', 'مر', 'دد ', 'هستید ', 'من این ', 'دست', 'ک', 'اه ', 'را ', 'پیشنهاد میکنم ', '  در ', 'عرض ', '2 ', 'دقیقه ', 'یک ', 'پی', 'راه', 'ن ', 'یا ', 'شلو', 'ار ', 'کامل ', 'اط', 'و ', 'کنید و ', 'خسته ', 'ن', 'شو', 'ید ', '  ', 'بدلیل ', 'حذف ', 'مخزن ', 'آب ', 'از ', 'دسته ', 'اط', 'و ', 'وزن ', 'دسته ', 'کم ', 'شده ', 'و م', 'چ ', 'دست', 'تان ', 'خسته ', 'نمی ', 'شود  ', 'با ', 'فشار ', 'دادن ', 'دکمه ', 'بخار ', '500 ', 'گرم ', 'بخار ', 'خارج ', 'میشود ', 'که ', 'سخت ', 'ترین ', 'چرو', 'ک', 'ها ', 'ب', 'سرعت ', 'باز ', 'می', 'شو', 'د']\n",
      "['من این ', 'جارو ', 'رو چند ', 'روزی ', 'هست که ', 'خریدم و ', 'واقعا ', 'ازش راضی هستم ', 'و ', 'صدای ', 'کمی ', 'داره و ', 'از هر نظر ', 'عال', 'ی', 'ه']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش سوم:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:37:14.236173Z",
     "start_time": "2024-10-17T15:37:13.848265Z"
    }
   },
   "source": [
    "print(\n",
    "    \" قسمت اول دوم سوم                                                                                                                                    \")\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class NGramClassifier:\n",
    "    def __init__(self, n_gram):\n",
    "        self.n_gram = n_gram\n",
    "        self.n_gram_count_result = {\n",
    "            \"positive\": dict(),\n",
    "            \"negative\": dict(),\n",
    "        }\n",
    "        self.class_freq = {\n",
    "            \"positive\": 0,\n",
    "            \"negative\": 0,\n",
    "        }\n",
    "\n",
    "    def create_ngrams(self, text):\n",
    "        tokens = text.split()\n",
    "        ngrams = [tuple(tokens[i:i + self.n_gram]) for i in range(len(tokens) - self.n_gram + 1)]\n",
    "        return ngrams\n",
    "\n",
    "    def find_positive_and_negative_word_for_train_data(self, texts, labels):\n",
    "        for text, label in zip(texts, labels):\n",
    "            ngrams = self.create_ngrams(text)\n",
    "            self.class_freq[label] += 1\n",
    "            for ngram in ngrams:\n",
    "                if ngram not in self.n_gram_count_result[label]:\n",
    "                    self.n_gram_count_result[label][ngram] = 0\n",
    "                self.n_gram_count_result[label][ngram] += 1\n",
    "\n",
    "    def predict_labels(self, texts):\n",
    "        predictions = []\n",
    "        for text in texts:\n",
    "            ngrams = self.create_ngrams(text)\n",
    "            label_scores = {\n",
    "                \"positive\": 0,\n",
    "                \"negative\": 0,\n",
    "            }\n",
    "            for ngram in ngrams:\n",
    "                for label in self.n_gram_count_result.keys():\n",
    "                    if ngram in self.n_gram_count_result[label]:\n",
    "                        label_scores[label] += self.n_gram_count_result[label][ngram]\n",
    "            predictions.append(max(label_scores, key=label_scores.get))\n",
    "        return predictions\n",
    "\n",
    "    def evaluate(self, texts, labels):\n",
    "        prediction_labels = self.predict_labels(texts)\n",
    "        correct = sum(prediction_label == test_label for prediction_label, test_label in zip(prediction_labels, labels))\n",
    "        num_of_true_positive = 0\n",
    "        num_of_false_positive = 0\n",
    "        num_of_false_negative = 0\n",
    "        for predict_label, test_label in zip(prediction_labels, labels):\n",
    "            if predict_label == 'positive' and test_label == 'positive':\n",
    "                num_of_true_positive += 1\n",
    "            elif predict_label == 'positive' and test_label == 'negative':\n",
    "                num_of_false_positive += 1\n",
    "            elif predict_label == 'negative' and test_label == 'positive':\n",
    "                num_of_false_negative += 1\n",
    "        accuracy = correct / len(texts)\n",
    "        precision = num_of_true_positive / (num_of_true_positive + num_of_false_positive) if (\n",
    "                                                                                                     num_of_true_positive + num_of_false_positive) > 0 else 0\n",
    "        recall = num_of_true_positive / (num_of_true_positive + num_of_false_negative) if (\n",
    "                                                                                                  num_of_true_positive + num_of_false_negative) > 0 else 0\n",
    "\n",
    "        return accuracy, precision, recall\n",
    "\n",
    "\n",
    "data_file = \"data/digikala.csv\"\n",
    "corpus = pd.read_csv(data_file)\n",
    "\n",
    "labels = []\n",
    "for score in corpus['Score']:\n",
    "    if score > 50:\n",
    "        labels.append('positive')\n",
    "    else:\n",
    "        labels.append('negative')\n",
    "corpus['Label'] = labels\n",
    "\n",
    "# use the first 2700 data for the train data and use the other data for the test data\n",
    "train_data, test_data = corpus[:2700], corpus[2700:]\n",
    "text_train, label_train = train_data['Text'], train_data['Label']\n",
    "text_test, label_test = test_data['Text'], test_data['Label']\n",
    "\n",
    "print(\"Bigram\")\n",
    "bigram_classifier = NGramClassifier(n_gram=2)\n",
    "bigram_classifier.find_positive_and_negative_word_for_train_data(text_train, label_train)\n",
    "bigram_accuracy, bigram_precision, bigram_recall = bigram_classifier.evaluate(text_test, label_test)\n",
    "print('accuracy: ' + str(bigram_accuracy))\n",
    "print('precision: ' + str(bigram_precision))\n",
    "print('recall: ' + str(bigram_recall))\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Trigram\")\n",
    "trigram_classifier = NGramClassifier(n_gram=3)\n",
    "trigram_classifier.find_positive_and_negative_word_for_train_data(text_train, label_train)\n",
    "trigram_accuracy, trigram_precision, trigram_recall = trigram_classifier.evaluate(text_test, label_test)\n",
    "print('accuracy: ' + str(trigram_accuracy))\n",
    "print('precision: ' + str(trigram_precision))\n",
    "print('recall: ' + str(trigram_recall))\n",
    "print(\"\\nhttps://chistio.ir/precision-recall-f/\")\n",
    "\n",
    "print(\"=\" * 150)\n",
    "\n",
    "print(\n",
    "    \" قسمت چهارم                                                                                                                                    \")\n",
    "print(\"\"\"\n",
    "                                                                                                                                                    recall\n",
    "بازخوانی هرچه بیشتر باشد یعنی اینکه که مدل توانسته است تمام نمونه‌هایی که  واقعا مثبت هستند را به درستی شناسایی کند که توی مدل بی گرام این عدد ۱ شده یعنی ۱۰۰ درصد و توی تری گرام نزدیک یه ۱ شده و یعنی مدل بی گرام تونسته به خوبی اینکارو انجام بده ولی مدل تری گرام تعدادی از نمونه‌های مثبت واقعی را از دست داده است اما این تعداد بسیار کم است.                                                                                                                                         \n",
    "\n",
    "                                                                                                                                                Precision\n",
    "  این معیار هرچه بالاتر باشد به این معناست که تعداد زیادی از پیش‌بینی‌های مثبت درست بوده‌ است که در مدل تری گرام این مقدار ۵ درصد بیشتر از بی گرام شده  است که نشان می‌دهد این مدل در پیش‌بینی دسته مثبت کمی بهتر عمل کرده است و تعداد بیشتری از پیش‌بینی‌های مثبت درست بوده‌اند.                                                       \n",
    "    \n",
    "\"\"\")\n",
    "print(\"=\" * 150)\n",
    "print(\n",
    "    \" قسمت پنجم                                                                                                                                    \")\n",
    "print(\"\"\"\n",
    "استفاده از این طبقه بندی می‌تواند در شرایطی مفید باشد که میخواهیم سریع پیاده سازی و اموزش را شروع کنیم و  منابع محاسباتی محدودی داریم و یا داده‌های آموزشی کافی نداریم. این مدل‌ها به دلیل سادگی و تفسیرپذیری خوبی که دارند میتوان برای پروژه‌هایی مانند تحلیل خیلی ساده متن، و پردازش زبان‌هایکه دارای منابع کمی هستند  استفاده کرد. در کاربردهای پیچیده و نیاز به دقت بالا به این صورت نبوده که اصلا جواب ندهند و حتی ممکن است  در بعضی مواقع در مدل‌های پیچیده‌تر عملکرد بهتری داشته باشند.\n",
    "\"\"\")\n",
    "print(\"\\nhttps://library.fiveable.me/key-terms/introduction-to-advanced-programming-in-r/n-gram-models\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " قسمت اول دوم سوم                                                                                                                                    \n",
      "Bigram\n",
      "accuracy: 0.9055258467023173\n",
      "precision: 0.9055258467023173\n",
      "recall: 1.0\n",
      "==================================================\n",
      "Trigram\n",
      "accuracy: 0.9144385026737968\n",
      "precision: 0.9181818181818182\n",
      "recall: 0.9940944881889764\n",
      "\n",
      "https://chistio.ir/precision-recall-f/\n",
      "======================================================================================================================================================\n",
      " قسمت چهارم                                                                                                                                    \n",
      "\n",
      "                                                                                                                                                    recall\n",
      "بازخوانی هرچه بیشتر باشد یعنی اینکه که مدل توانسته است تمام نمونه‌هایی که  واقعا مثبت هستند را به درستی شناسایی کند که توی مدل بی گرام این عدد ۱ شده یعنی ۱۰۰ درصد و توی تری گرام نزدیک یه ۱ شده و یعنی مدل بی گرام تونسته به خوبی اینکارو انجام بده ولی مدل تری گرام تعدادی از نمونه‌های مثبت واقعی را از دست داده است اما این تعداد بسیار کم است.                                                                                                                                         \n",
      "\n",
      "                                                                                                                                                Precision\n",
      "  این معیار هرچه بالاتر باشد به این معناست که تعداد زیادی از پیش‌بینی‌های مثبت درست بوده‌ است که در مدل تری گرام این مقدار ۵ درصد بیشتر از بی گرام شده  است که نشان می‌دهد این مدل در پیش‌بینی دسته مثبت کمی بهتر عمل کرده است و تعداد بیشتری از پیش‌بینی‌های مثبت درست بوده‌اند.                                                       \n",
      "    \n",
      "\n",
      "======================================================================================================================================================\n",
      " قسمت پنجم                                                                                                                                    \n",
      "\n",
      "استفاده از این طبقه بندی می‌تواند در شرایطی مفید باشد که میخواهیم سریع پیاده سازی و اموزش را شروع کنیم و  منابع محاسباتی محدودی داریم و یا داده‌های آموزشی کافی نداریم. این مدل‌ها به دلیل سادگی و تفسیرپذیری خوبی که دارند میتوان برای پروژه‌هایی مانند تحلیل خیلی ساده متن، و پردازش زبان‌هایکه دارای منابع کمی هستند  استفاده کرد. در کاربردهای پیچیده و نیاز به دقت بالا به این صورت نبوده که اصلا جواب ندهند و حتی ممکن است  در بعضی مواقع در مدل‌های پیچیده‌تر عملکرد بهتری داشته باشند.\n",
      "\n",
      "\n",
      "https://library.fiveable.me/key-terms/introduction-to-advanced-programming-in-r/n-gram-models\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **سوال چهارم:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش اول:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T16:16:15.754456Z",
     "start_time": "2024-10-17T16:16:15.749807Z"
    }
   },
   "source": [
    "print(\"\"\"\n",
    "ترنسفورمرها مدل‌های یادگیری ماشین هستند که از توجه به عنوان مکانیزم اصلی یادگیری استفاده می‌کنند و به سرعت به بهترین روش‌ها در وظایف توالی به توالی مانند       ترجمه زبان تبدیل شدند.                                                                                                                                \n",
    "مقاله \"An Image is Worth 16x16 Words\" این ترنسفورمرها را برای حل مسائل طبقه‌بندی تصویر اصلاح کرده و Vision Transformer (ViT) را ایجاد کرد. ViT از همان مکانیزم توجه استفاده می‌کند، اما تنها شامل یک بخش انکودر است و خروجی آن به یک شبکه عصبی برای پیش‌بینی ارسال می‌شود.                                         \n",
    "نقطه ضعف ViT این است که برای عملکرد بهینه به پیش‌آموزش روی داده‌های بزرگ نیاز دارد. بهترین مدل‌ها بر روی مجموعه داده JFT-300M پیش‌آموزش دیده‌اند،            در حالی که مدل‌های پیش‌آموزش دیده بر روی ImageNet-21k عملکرد مشابهی با مدل‌های ResNet دارند.                                                   \n",
    "\\nhttps://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ترنسفورمرها مدل‌های یادگیری ماشین هستند که از توجه به عنوان مکانیزم اصلی یادگیری استفاده می‌کنند و به سرعت به بهترین روش‌ها در وظایف توالی به توالی مانند       ترجمه زبان تبدیل شدند.                                                                                                                                \n",
      "مقاله \"An Image is Worth 16x16 Words\" این ترنسفورمرها را برای حل مسائل طبقه‌بندی تصویر اصلاح کرده و Vision Transformer (ViT) را ایجاد کرد. ViT از همان مکانیزم توجه استفاده می‌کند، اما تنها شامل یک بخش انکودر است و خروجی آن به یک شبکه عصبی برای پیش‌بینی ارسال می‌شود.                                         \n",
      "نقطه ضعف ViT این است که برای عملکرد بهینه به پیش‌آموزش روی داده‌های بزرگ نیاز دارد. بهترین مدل‌ها بر روی مجموعه داده JFT-300M پیش‌آموزش دیده‌اند،            در حالی که مدل‌های پیش‌آموزش دیده بر روی ImageNet-21k عملکرد مشابهی با مدل‌های ResNet دارند.                                                   \n",
      "\n",
      "https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8\n",
      "\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*بخش دوم:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:40:16.004328Z",
     "start_time": "2024-10-17T18:40:15.997290Z"
    }
   },
   "source": [
    "#write your code here\n",
    "print(\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xI_2knW0NkyY"
   },
   "source": [
    "# **نکات مهم**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEig2qIiMDhR"
   },
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "    <p><strong>مهلت تحویل بدون جریمه:</strong> ۲۷ مهر ۱۴۰۲</p>\n",
    "    <p><strong>مهلت تحویل با تاخیر (با جریمه):</strong> ۴ آبان ۱۴۰۲</p>\n",
    "</div>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">فایل ارسالی شما باید با فرمت زیر نامگذاری شود: <code>NLP_CA1_LASTNAME_STUDENTID.ipynb</code></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuwVuIa0MDhR"
   },
   "source": [
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">نحوه انجام این تمرین:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>برخی سوالات نیاز به نوشتن کد پایتون و محاسبه نتایج دارند، و بقیه آنها دارای پاسخ‌های نوشتاری هستند. برای مسائل کدنویسی، باید تمام بلوک‌های کدی که با <code>#WRITE YOUR CODE HERE</code> مشخص شده‌اند را تکمیل کنید.</li> <li>برای پاسخ‌های متنی، باید متنی که می‌گوید \"پاسخ خود را اینجا بنویسید...\" را با پاسخ واقعی خود جایگزین کنید.</li> </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efm7DICPMDhS"
   },
   "source": [
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">صداقت علمی:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>ما نوت‌بوک‌های تعداد مشخصی از دانشجویان که به صورت تصادفی انتخاب می‌شوند، بررسی خواهیم کرد. این بررسی‌ها اطمینان حاصل می‌کنند که کدی که نوشتید واقعاً پاسخ‌های موجود در نوت‌بوک شما را تولید می‌کند. اگر پاسخ‌های صحیح را در نوت‌بوک خود بدون کدی که واقعاً آن پاسخ‌ها را تولید کند تحویل دهید، این یک مورد جدی از عدم صداقت علمی محسوب می‌شود.</li> <li>ما همچنین بررسی‌های خودکاری را برای تشخیص سرقت علمی در نوت‌بوک‌های کولب انجام خواهیم داد. کپی کردن کد از دیگران نیز یک مورد جدی از عدم صداقت علمی محسوب می‌شود.</li> </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqNfFt5AMDhS"
   },
   "source": [
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">توضیحات تکمیلی:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "خوانایی و دقت بررسی‌ها در گزارش نهایی از اهمیت ویژه‌ای برخوردار است. به تمرین‌هایی که به صورت کاغذی تحویل داده شوند یا به صورت عکس در سایت بارگذاری شوند، ترتیب اثری داده نخواهد شد.</li>\n",
    "<li>\n",
    " همه‌ی کدهای پیوست گزارش بایستی قابلیت اجرای مجدد داشته باشند. در صورتی که برای اجرا مجدد آن‌ها نیاز به تنظیمات خاصی می‌باشد، بایستی تنظیمات مورد نیاز را نیز در گزارش خود ذکر کنید.  دقت کنید که  تمامی کدها باید توسط شما اجرا شده باشند و نتایج اجرا در فایل کدهای ارسالی مشخص باشد. به کدهایی که نتایج اجرای آن‌ها در فایل ارسالی مشخص نباشد نمره‌ای تعلق نمی‌گیرد.\n",
    "</li>\n",
    "<li>\n",
    "تمرین تا یک هفته بعد از مهلت تعیین شده با تاخیر تحویل گرفته می‌شود. دقت کنید که شما جمعاً برای تمام تکالیف، ۱۴ روز زمان تحویل بدون جریمه دارید که تنها از ۷ روز آن برای هر تمرین می‌توانید استفاده کنید. در صورتی که این ۱۴ روز به اتمام رسیده باشد، به ازای هر روز تاخیر ده درصد جریمه می‌شود.\n",
    "</li>\n",
    "<li>توجه کنید این تمرین باید به صورت تک‌نفره انجام شود و پاسخ‌های ارائه شده باید نتیجه فعالیت فرد نویسنده باشد (همفکری و به اتفاق هم نوشتن تمرین نیز ممنوع است). در صورت مشاهده\n",
    " تشابه به همه افراد مشارکت‌کننده، نمره تمرین صفر و به استاد گزارش می‌گردد.\n",
    " </li>\n",
    " <li>برای مطالعه بیشتر درباره‌ی فرمت مارک‌دون می‌توانید از <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">این لینک</a> مطالعه کنید.\n",
    " </li>\n",
    "\n",
    " </ul>\n",
    " </div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

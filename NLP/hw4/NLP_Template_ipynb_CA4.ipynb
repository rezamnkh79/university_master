{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlrrsi07MDhO"
   },
   "source": [
    "<h1 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">تمرین شماره 4: درس پردازش زبان طبیعی - دانشگاه تهران، پائیز ۱۴۰۳</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T42xpmfrMDhQ"
   },
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 18px;\">\n",
    "نام:\n",
    "<br/>\n",
    "شماره دانشجویی:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58azWVGxMDhQ"
   },
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "سوالات خودتان را می‌توانید از طریق ایمیل\n",
    "<code>ali.fartout@ut.ac.ir</code>\n",
    " از طراح تمرین 4 برای سوال اول بپرسید."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foW-gR1HMDhR"
   },
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 18px; color: red; font-weight: bold;\">\n",
    "قوانین و توضیحاتی آخر فایل تمرین حتما به دقت مطالعه شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYOgIt4psPHa"
   },
   "source": [
    "# **مجموعه داده:**"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T19:49:09.007528Z",
     "start_time": "2024-12-24T19:49:09.004184Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"https://chatgpt.com/share/676b101d-c81c-8006-b803-720db788fa81\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://chatgpt.com/share/676b101d-c81c-8006-b803-720db788fa81\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1rN-aSosPHa"
   },
   "source": [
    "*بخش اول:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T17:45:12.367413Z",
     "start_time": "2024-12-14T17:44:56.263522Z"
    },
    "vscode": {
     "languageId": "plaintext"
    },
    "id": "73CHDYfysPHb"
   },
   "outputs": [],
   "source": [
    "#write your code here\n",
    "from datasets import load_dataset\n",
    "\n",
    "persian_dataset = load_dataset(\"AliFartout/PEYMA-ARMAN-Mixed\")\n",
    "english_dataset = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T16:21:58.914155Z",
     "start_time": "2024-12-14T16:21:58.905648Z"
    },
    "id": "CDFJ7sMOsPHb",
    "outputId": "05388f60-20fd-4667-cb32-d60b43b13d4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ترکیب دیتاست‌های فارسی و انگلیسی با چالش‌هایی چون ناهماهنگی در فرمت داده‌ها و برچسب‌ها، عدم تعادل داده‌ها و کلاس‌ها، و تفاوت‌های زبانی مانند نوشتار راست به چپ و کلمات مرکب همراه است. همچنین، پیش‌پردازش‌های خاص برای هر زبان و مشکلات توکن‌سازی می‌تواند پیچیدگی‌هایی ایجاد کند. انتقال دانش از مدل‌های جداگانه برای هر زبان ممکن است باعث کاهش کارایی شود، و تفاوت‌های فرهنگی و معنایی بین دو زبان نیز ممکن است مدل را به چالش بکشد. در نهایت، ارزیابی مدل در دو زبان مختلف با معیارهای متفاوت ممکن است دقت را تحت تاثیر قرار دهد.                                                         \n",
      "\n",
      "راه حل:                                                                         \n",
      "برای یکسان‌سازی فرمت‌ها، ابتدا دیتاست‌ها را با هم تطابق میدهیم و فرمت‌ها را یکسان میکنیم. سپس داده‌ها را پیش‌پردازش کرده و برچسب‌ها را به فرمت BIO تبدیل میکنیم. بعد از پردازش، دیتاست‌ها را مرج میکنیم تا یک مجموعه داده یکپارچه ایجاد شود. برای هر زبان، توکن‌سازی مناسب را اعمال کرده تا مدل بتواند ویژگی‌های زبانی مختلف را به درستی شناسایی کند. در نهایت، با استفاده از تکنیک‌های تعادل داده‌ها، مشکلات عدم تعادل  کلاس‌ها را رفع کرده و عملکرد مدل را بهبود دهید.                                     \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "ترکیب دیتاست‌های فارسی و انگلیسی با چالش‌هایی چون ناهماهنگی در فرمت داده‌ها و برچسب‌ها، عدم تعادل داده‌ها و کلاس‌ها، و تفاوت‌های زبانی مانند نوشتار راست به چپ و کلمات مرکب همراه است. همچنین، پیش‌پردازش‌های خاص برای هر زبان و مشکلات توکن‌سازی می‌تواند پیچیدگی‌هایی ایجاد کند. انتقال دانش از مدل‌های جداگانه برای هر زبان ممکن است باعث کاهش کارایی شود، و تفاوت‌های فرهنگی و معنایی بین دو زبان نیز ممکن است مدل را به چالش بکشد. در نهایت، ارزیابی مدل در دو زبان مختلف با معیارهای متفاوت ممکن است دقت را تحت تاثیر قرار دهد.\n",
    "\n",
    "راه حل:\n",
    "برای یکسان‌سازی فرمت‌ها، ابتدا دیتاست‌ها را با هم تطابق میدهیم و فرمت‌ها را یکسان میکنیم. سپس داده‌ها را پیش‌پردازش کرده و برچسب‌ها را به فرمت BIO تبدیل میکنیم. بعد از پردازش، دیتاست‌ها را مرج میکنیم تا یک مجموعه داده یکپارچه ایجاد شود. برای هر زبان، توکن‌سازی مناسب را اعمال کرده تا مدل بتواند ویژگی‌های زبانی مختلف را به درستی شناسایی کند. در نهایت، با استفاده از تکنیک‌های تعادل داده‌ها، مشکلات عدم تعادل  کلاس‌ها را رفع کرده و عملکرد مدل را بهبود دهید.\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52tAk_6dMWnF"
   },
   "source": [
    "# **سوال اول:**"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZrAiEJAOvyt",
    "outputId": "938ac1eb-2917-434e-d04c-c13d4a28c9fe"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m480.6/480.6 kB\u001B[0m \u001B[31m14.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m116.3/116.3 kB\u001B[0m \u001B[31m11.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m179.3/179.3 kB\u001B[0m \u001B[31m19.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.8/134.8 kB\u001B[0m \u001B[31m15.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.1/194.1 kB\u001B[0m \u001B[31m16.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkAAw0haMdyS"
   },
   "source": [
    "*بخش اول:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T11:58:19.972756Z",
     "start_time": "2024-12-16T11:58:02.601079Z"
    },
    "id": "WpB_Eep5MDhR",
    "vscode": {
     "languageId": "plaintext"
    },
    "outputId": "74fe2dc9-e122-48c3-8c7b-42fd62ea4783",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 905,
     "referenced_widgets": [
      "7caada25255a4a2ea79fd4946350ce6f",
      "bcabe00f8cd44488a60fe2ef9720d544",
      "4be3c92d41484d9990e2f427f70c05fb",
      "3e7ea66a14d344e29137dfa1b70ae242",
      "78152d8d7f4d42f082d958eefff2c456",
      "d6702447046f4c0eb22443e46b8d267c",
      "fd5f54010d134e788b24dedaff4a245c",
      "e88f13cba91d469b908a97a2c349c401",
      "c123173aa3fb48fd842c3d432e0c1852",
      "573cb53a00784628b8b1a70e9b44a819",
      "591ecaf181ce46998684df7398bd0d04",
      "1204782198724a6489a97a1360def9b7",
      "c2288099b79b4f01b989110606af3f99",
      "5258ad3d9c614684901455cdccca5464",
      "0081b87457d74ff296ccf0bf7a44c1ea",
      "c29abfebe6d64184aec032e1fc5f386a",
      "6866f5e2121c45c99056d17aff0389e3",
      "6223496a27a74c018d32c640def1602c",
      "e192f92c737245bcb543ade156c17734",
      "6591d105216c4cffad68fe6a9f08b5e0",
      "8691d7f7c463456e8d8a61edd8e14424",
      "180a7f59f3504de39ceeaf196c344349",
      "9e2a6aaa33ea48579b2e48156809677e",
      "56752dbb306c491b96950db734080ea6",
      "5f4b7c1b6638482e9f91ff02350f796c",
      "f444c7714a3e497b974bb6cba3f94f0f",
      "f0b196963d82478cbfaeb8952f226d40",
      "b76e1b20069349b385b879c3a508683d",
      "c27391262e87428b97bf2af3c9ebf4f8",
      "4be7077397484a66be454d94fdb7d29a",
      "c53e5ad6d48c42879c70286d8b5e026d",
      "76dcb0f4857f4b00ae053048bc7b2439",
      "c3bc90276a734f058b59568bdbf5cb76",
      "0b384c1a1fe04a88b99402d863be256b",
      "ee8a859a7e3f4051af73afe3596284c6",
      "472d3e34c7be4f7ab1880cc1a2590ec3",
      "0e71556a6d18400daa83b898c37fbfc8",
      "066074ed0023419fa79c3dc0735836ef",
      "bc734652ac54484ba21e111aa1124b7c",
      "1219365db1ee430e820c57cc69766d21",
      "45ff2cd9367448208c03418cccb1e163",
      "676acf218792492eae3b6a76fd27a76f",
      "3ea4a612a0c44b8d9e403e31665b6ee8",
      "b4932ec1661f419cbe9355bf0213a4a3",
      "5773a61f5e83417b87da26415ed9f64e",
      "e301df9235a14e21b912aee03994bc54",
      "f2daa4e9e411415ca1d16879be88d1d6",
      "f9ba0df5051149a29f4a3ba34c6b17f7",
      "a306af4e7f5d4ed5a0e695b6cba1438f",
      "dd068ea7c9d84b54aa7c2d08e93e0c55",
      "d5ff20bcb8304853aad3a37bd8d62206",
      "88c543eac77548f9a568c04d95026500",
      "09800f60c0ac4fc8b80fb8ed792ae5df",
      "39bb3ee089844427b02fd004abe24fb2",
      "9c732a1cc6a64c89950a69b5b3ca3f7e",
      "351b7477ea0049a6852f0cc80c64f785",
      "72ab6d70b7c7432a840a691e267ea7f8",
      "6d15bbf859de4125a090d4dfc7e52d91",
      "4bcf1637435c4b3eb7840bb3a309a50a",
      "a1ce14a7425d477bb45ce7d417dc21fb",
      "8ff0cbc21fdb4733972f56ac299c12dc",
      "0b4672e88cea4a7bbf3d7c5b035c3367",
      "3dde3fbd007941bb94e814586a556a73",
      "5f254fa72f6f4e9281da4fbbaa1e8ec5",
      "8a5d4411b8244d79bd515f563ee07f91",
      "81ac9011c8ef4cffb1a6f7ca8eca94f6",
      "efef75d0e65c4048b6cf3dbf600e1503",
      "e8b4ec3474a14e2e893feac4b0429d33",
      "c2afb53b64ff42fdbbca34b950072055",
      "c365d5e6d609433aa4f5777f70fdfa13",
      "0ebc9049dce948faac4eb677ead240c8",
      "33b97d361e5640e791e455878cd72afb",
      "40f17fa1eb3248b8b725b4333173a5ff",
      "2ab0ea6224964bbab151ebef3c6ef23c",
      "2d1a9ce408094d1dbf00bfb87a068b73",
      "7bfaaff485e64f7fb4bba5a42f33b629",
      "cb08945e33c14d88b9cf82ca58e357e2",
      "6e1cb8126fd34316a399d45119233698",
      "a8c13cd56ad445a3a0eee1944d616b54",
      "823ff306347e4b89a922914b2e799d20",
      "d3d120d6aab1489bba742bed82be3663",
      "980d1bf3e8b34d6d85df0126020ee64f",
      "bd14e958c84e4975bc8391ae766833b0",
      "6bbfe0e4aafd4b0a889167a1120b9957",
      "6ee55133e5ea479d9582a8ecc4b68074",
      "93ff055c233f4074b698547455cf85cf",
      "df4a322fdbd04a10860c8a3d03b8e584",
      "c7bfaf4f6a0742c4ab71ae2151646c3f",
      "5b5d45d3637448c8a379fa31de96fe33",
      "660d1ba3ad464a1fb05562852893acee",
      "970e5a5684a64ee7ba5ebf9f0e3db14c",
      "6356ea46d8674dab9b9eb610e30e5247",
      "af26c7aed95240dfadc723d799153e70",
      "cc4fef0370ab42ea8ddeb0fc65a3b6db",
      "1dd751001a6c49fb908c0d82cef674ea",
      "93ffbea052b24d4fb4371a8a7d964db6",
      "e3f71056b3f6478f96c66f21bd1966ec",
      "f210ca931aa44d83a9cb4e38d20d85cf",
      "d8773a146c85401eaca7b27e2e60f521",
      "2a1cad5fde8f4cb6a5b7113f3668499b",
      "ca4da00629df41b19ca878ccc50c3d47",
      "6170cfb70e344f04bda70ee1b6e06d1a",
      "d1c789f25bba4bac9fade77b51f1a8ba",
      "d15765fa3759480d8d4a20f3076140e4",
      "c2c138db0ab148f7a675173b658f25a2",
      "0538617c89204a4d81c51eab56ec8b20",
      "b6ab15911ade4ab49c254f2d35ec526a",
      "22633176e9cd45bba5c5f40b82323ac2",
      "6c94f00ff7314a0088f85abf3cb6ca30",
      "e10ff2bfcafe4b958e0f2ebacb449692",
      "e78555d73b4d4b219e8f24b051f3cb9a",
      "ee755bbe9772403aa8fb11cc38e1b527",
      "ccee26c5278f43c2a6d905d64d681834",
      "dd6d881ab3d64210bf33879d8b7f26d9",
      "8a68efc546af451e9ddc97acc1212843",
      "d7e08ab0594948829138409404fcc159",
      "22b61430c18d490a9e2da6dd6c908f15",
      "75b0d5cbdfcf4ed29f4a1c66f45a0a56",
      "b5cdb8fdc5114ffda6d6f6f9811b80af",
      "a37ddf293b074744b167aaec6010b0ad",
      "fa4e4fbee31946db97c9b920043285b7",
      "efa64a232dd447b98d04dc484c20ac16",
      "642f89f482924223a6dc0a9cb44567c2",
      "e93ead4cb03d4dec88dad637fa73b01e",
      "57ad02fe9363477cba106bf18b05a738",
      "01db17a99d5540dd81f2d4841452945f",
      "4302b9fb8ed0470b8a53755aade7b080",
      "c5dc97f74d0545d8b6833737e67c1274",
      "8d6fd2c7600e45178cd8ec8fd21e15c3",
      "6ced072c075d4d44aee0b6e9260f383d",
      "8fba092d15b941bebe7f054729d3d88d",
      "ccfc5899af0d4ae78e3ac10f4ec9c25f",
      "e79b8697b65f487b81268f8f966546c1",
      "39be24d48be94c509700c1196bf6f992",
      "624b8f69d3804ffe8a49143ea91f90cc",
      "770e519121c34cf9abe80d1d4b84a0a2",
      "e95fd5d405f74514a571ee3e71a7dda2",
      "11117652e28047d988aa4897766c58f8",
      "9733c58d2941491e868db3a3c6448209",
      "aa601664bdae4d1cab1709dce8b7677f",
      "2ddd0ee8cbe443a7916e791db747bbd0",
      "b071c191fa8947cdbe9dbd0c3bd36ef9",
      "6de100dda56c456b9aa75162a8cef5c6"
     ]
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/3.27k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7caada25255a4a2ea79fd4946350ce6f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.31M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1204782198724a6489a97a1360def9b7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/431k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e2a6aaa33ea48579b2e48156809677e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/423k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b384c1a1fe04a88b99402d863be256b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/26384 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5773a61f5e83417b87da26415ed9f64e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/3296 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "351b7477ea0049a6852f0cc80c64f785"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/3296 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efef75d0e65c4048b6cf3dbf600e1503"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Persian Dataset Columns: ['tokens', 'ner_tags', 'ner_tags_names']\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e1cb8126fd34316a399d45119233698"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "conll2003.py:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b5d45d3637448c8a379fa31de96fe33"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N] y\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a1cad5fde8f4cb6a5b7113f3668499b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e78555d73b4d4b219e8f24b051f3cb9a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efa64a232dd447b98d04dc484c20ac16"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e79b8697b65f487b81268f8f966546c1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "English Dataset Columns: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags']\n",
      "Combined Dataset Sample:\n",
      "                                              tokens  \\\n",
      "0  یوکوویچ متولد دانمارک است اما والدین او صرب هس...   \n",
      "1  0 خانواده ایرانی از اوکی هارو یاسوائوکا وزیر د...   \n",
      "2  وي با بیان این كه قرار شد خبرگان ملت مواظبت بی...   \n",
      "3  به گزارش ایسنا ، اطهره نژادی معاون هماهنگی و ب...   \n",
      "4  به گزارش خبرنگار پارلمانی ایسنا ، ترکیب اعضای ...   \n",
      "\n",
      "                                            ner_tags  \n",
      "0                         5 20 1 20 20 20 20 1 20 20  \n",
      "1  20 20 20 20 5 12 12 20 3 10 20 20 20 20 20 20 ...  \n",
      "2  20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 2...  \n",
      "3  20 20 3 20 5 12 20 20 20 20 3 10 20 20 20 20 5...  \n",
      "4  20 20 20 20 3 20 20 20 3 10 10 10 20 20 20 20 ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_persian_dataset = load_dataset(\"AliFartout/PEYMA-ARMAN-Mixed\", split=\"train\")\n",
    "\n",
    "print(\"Persian Dataset Columns:\", train_persian_dataset.column_names) \n",
    "\n",
    "persian_df = pd.DataFrame({\n",
    "    'tokens': [' '.join(tokens) for tokens in train_persian_dataset['tokens']],\n",
    "    'ner_tags': [' '.join(map(str, tags)) for tags in train_persian_dataset['ner_tags']],  \n",
    "})\n",
    "\n",
    "train_english_dataset = load_dataset(\"conll2003\", split=\"train\")\n",
    "\n",
    "print(\"English Dataset Columns:\",\n",
    "      train_english_dataset.column_names) \n",
    "\n",
    "english_df = pd.DataFrame({\n",
    "    'tokens': [' '.join(tokens) for tokens in train_english_dataset['tokens']],  \n",
    "    'ner_tags': [' '.join(map(str, tags)) for tags in train_english_dataset['ner_tags']], \n",
    "})\n",
    "train_combined_df = pd.concat([persian_df, english_df], ignore_index=True)\n",
    "print(\"Combined Dataset Sample:\")\n",
    "print(train_combined_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5vi1eJVMkCH"
   },
   "source": [
    "*بخش دوم:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhQuXmbUsPHd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1f2d8c1c-d305-49fa-bc36-2eadb1e3169e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "چرا باید Alignment انجام شود\n",
      "در مسائل طبقه‌بندی توکن (مانند شناسایی موجودیت‌های نام‌گذاری شده)، مدل‌های مبتنی بر تقسیم توکن به بخش‌های کوچک‌تر ممکن است یک کلمه را به چندین توکن تقسیم کنند، بنابراین تعداد توکن‌ها با تعداد برچسب‌ها هماهنگ نیست. این امر نیاز به هم‌راستایی دارد، یعنی تخصیص صحیح برچسب‌ها به توکن‌ها. توکن‌های فرعی باید فقط برچسب کلمه اصلی را دریافت کنند و بقیه توکن‌ها باید با مقداری مشخص برچسب‌گذاری شوند. همچنین، هنگام استفاده از پدینگ، باید اطمینان حاصل کرد که پدینگ‌ها در محاسبه خطا نادیده گرفته شوند.\n",
      "\n",
      "راه حل پیشنهادی و چگونگی حل مشکل\n",
      "برای حل مشکل هم‌راستایی در مدل‌های طبقه‌بندی توکن، ابتدا متون را با استفاده از توکنایزر تقسیم‌کننده به توکن‌های بخش‌های کوچک‌تر تقسیم می‌کنیم. سپس برچسب‌ها به طور دقیق به توکن‌های اصلی هر کلمه اختصاص داده می‌شوند و برای توکن‌های فرعی برچسب ویژه‌ای می‌دهیم تا مدل آن‌ها را نادیده بگیرد. در نهایت، پدینگ‌ها را در هنگام محاسبه خطا نادیده می‌گیریم تا تأثیری بر محاسبه نداشته باشند. این فرآیند باعث هم‌راستایی صحیح توکن‌ها و برچسب‌ها و پیش‌بینی دقیق‌تر مدل می‌شود.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "چرا باید Alignment انجام شود\n",
    "در مسائل طبقه‌بندی توکن (مانند شناسایی موجودیت‌های نام‌گذاری شده)، مدل‌های مبتنی بر تقسیم توکن به بخش‌های کوچک‌تر ممکن است یک کلمه را به چندین توکن تقسیم کنند، بنابراین تعداد توکن‌ها با تعداد برچسب‌ها هماهنگ نیست. این امر نیاز به هم‌راستایی دارد، یعنی تخصیص صحیح برچسب‌ها به توکن‌ها. توکن‌های فرعی باید فقط برچسب کلمه اصلی را دریافت کنند و بقیه توکن‌ها باید با مقداری مشخص برچسب‌گذاری شوند. همچنین، هنگام استفاده از پدینگ، باید اطمینان حاصل کرد که پدینگ‌ها در محاسبه خطا نادیده گرفته شوند.\n",
    "\n",
    "راه حل پیشنهادی و چگونگی حل مشکل\n",
    "برای حل مشکل هم‌راستایی در مدل‌های طبقه‌بندی توکن، ابتدا متون را با استفاده از توکنایزر تقسیم‌کننده به توکن‌های بخش‌های کوچک‌تر تقسیم می‌کنیم. سپس برچسب‌ها به طور دقیق به توکن‌های اصلی هر کلمه اختصاص داده می‌شوند و برای توکن‌های فرعی برچسب ویژه‌ای می‌دهیم تا مدل آن‌ها را نادیده بگیرد. در نهایت، پدینگ‌ها را در هنگام محاسبه خطا نادیده می‌گیریم تا تأثیری بر محاسبه نداشته باشند. این فرآیند باعث هم‌راستایی صحیح توکن‌ها و برچسب‌ها و پیش‌بینی دقیق‌تر مدل می‌شود.\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-16T11:59:40.466970Z"
    },
    "jupyter": {
     "is_executing": true
    },
    "id": "YJuIhL_OsPHd",
    "outputId": "62e1f4a8-220a-465d-ea24-ed2b35d0f0ac",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e90d96525b434f2e91736d030129b935",
      "fc1215796e3c4e2588a922ee7e5a0d8c",
      "5a1c0565b3574d3d826f597942aeacc6",
      "8b641f0a4b664c048fb4dc63d3152e21",
      "52f616c9e004473e891376d48d3b034f",
      "99cf32c1b07c4b0a966f3e9359e03a89",
      "8006ce3b7b8740b7a31d0f2878695da0",
      "77692a74cf9d4b18bb6abef4f4c01b06",
      "65cc0c0f112647df89cda1ce613e057b",
      "853df5f5b24a487eab1eb212d953cad4",
      "b0b6aa3442b642c19a83a69e1f098efd",
      "ce8fabadfb0b49c399264e03c5c80512",
      "a2f33b7c0b8148ba87c35c29a032b837",
      "1a4bd0c6b90744058263256a92f85fb4",
      "38b1f2749c2c491e8178ff727eeceb63",
      "351f39be330f45c69372637194c370af",
      "ce713fc43e414c3b82b33a0c29a32db9",
      "c1f857b71c554182be6412cf74bc29c4",
      "fc96f05b39514f2fa1f9556f3ff26722",
      "0e5455fcda034a7c96ff6ff473fdbd89",
      "3195f2f67e33454bbca7f19c9c5fbd0a",
      "be656821ff4748199a15d0dd4bd166dc",
      "1b8c2e9cad9a4f6a945f94ea5cf531e2",
      "fdf8b0fa4e0e4d8da06aed72c9561f6a",
      "d255dfb86b4d462e84883acad0cd1cbb",
      "f75b8522cea143fc86eca83aab1b18bf",
      "980e3c44abbd49cc852af65838efe594",
      "b3f0f4e29b484597af7fbc7241740cc4",
      "762d54fc44444633acef4634b8ecf849",
      "7f476ceb4a404b55878fde3ab336c6d4",
      "8bb5fab06baf426081ad003ed56ecf3a",
      "f5e47ee35b874ba6a7f435a1b1fd8c55",
      "995fc8fed17e43dca6b6a8b634c063d8",
      "b778bb683adc4a96ae39a98c4f348b6d",
      "2d57fbffabdd4014b250141a3598c59c",
      "26ca477c68084b938aed846905ab07ff",
      "c6383433cf40443a8d94068c463d318c",
      "a4ff9e79ab244fd1998b87ca3af31779",
      "c752a7d2e29a40379165ecc78c95900b",
      "bff1276dee3141fb89a25b0260e98160",
      "59feaf523a1c4f1d9e4a80a45989cafc",
      "2384fda426f54d03a57d620a0ac3058f",
      "7be3b29483f8492c93ba4a6844f28e09",
      "a894ef720a464d97aa058d692986a2d2",
      "135ad4f73aa14752aef9f17395f141ed",
      "9119f811bbb74dafa56c716b9ac7ab7d",
      "011ef2810e604f638e1b6409c5da3e12",
      "0ef9fefd6cce471ba7b7c1f6388c22b0",
      "8de229941d9349c7995e9784a3435cb5",
      "192b22f0af3f4a669f118d1146472898",
      "2685df12faf84d50abcbc0f8d831eec8",
      "1ac0975ac6834d8084337138df628829",
      "f8cdb01566444ebf91bff5b033bc8458",
      "cac20a4b199d4129aff696b17af4cc69",
      "f762c14507bf4af99c449bbb01992f24",
      "05ca30b2b0a8417986902dcfb4be483d",
      "31a500ef0eb8433bb3c6adc06a28a68e",
      "313cfcef43f14fdab8658988443938a5",
      "8486b3704212407a9c87d5f548f7e36a",
      "957f1247667249bca7f6397a16b69ce6",
      "dfe113f5e0ad4162a76db9d6dc4d6e55",
      "e1892d6ebc714348bbc6794c17693ff6",
      "b49958cceae6434988bdff8808443546",
      "c9bfd2dfbd1f4a46b24b6df332e95a3a",
      "24535e1ab4944c3280b5a8e26cab0eae",
      "3a0986e168f84358ba6c4b505a0eacbe"
     ]
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e90d96525b434f2e91736d030129b935"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce8fabadfb0b49c399264e03c5c80512"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b8c2e9cad9a4f6a945f94ea5cf531e2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b778bb683adc4a96ae39a98c4f348b6d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "135ad4f73aa14752aef9f17395f141ed"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05ca30b2b0a8417986902dcfb4be483d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-6-bc3e3f0d7a02>:155: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241223_155930-bokndk6y</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rezamansourikhah-student/huggingface/runs/bokndk6y' target=\"_blank\">./output</a></strong> to <a href='https://wandb.ai/rezamansourikhah-student/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/rezamansourikhah-student/huggingface' target=\"_blank\">https://wandb.ai/rezamansourikhah-student/huggingface</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/rezamansourikhah-student/huggingface/runs/bokndk6y' target=\"_blank\">https://wandb.ai/rezamansourikhah-student/huggingface/runs/bokndk6y</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2526' max='2526' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2526/2526 28:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.587500</td>\n",
       "      <td>0.596129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.079900</td>\n",
       "      <td>0.465346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.526600</td>\n",
       "      <td>0.386122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.462900</td>\n",
       "      <td>0.350279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.568600</td>\n",
       "      <td>0.333421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.241100</td>\n",
       "      <td>0.306356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.056900</td>\n",
       "      <td>0.284705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.003700</td>\n",
       "      <td>0.268118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.076500</td>\n",
       "      <td>0.260206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.151800</td>\n",
       "      <td>0.248899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.098300</td>\n",
       "      <td>0.243461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.098700</td>\n",
       "      <td>0.235142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1636' max='1636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1636/1636 00:30]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 0.2351417988538742, 'eval_runtime': 31.3387, 'eval_samples_per_second': 208.879, 'eval_steps_per_second': 52.236, 'epoch': 0.9998020977637048}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "class TokenClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=16): \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            is_split_into_words=True\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].flatten()\n",
    "        attention_mask = encoding[\"attention_mask\"].flatten()\n",
    "        num_tokens = len(input_ids)\n",
    "        label = list(label)\n",
    "\n",
    "        if len(label) < num_tokens:\n",
    "            label = label + [0] * (num_tokens - len(label)) \n",
    "        elif len(label) > num_tokens:\n",
    "            label = label[:num_tokens]  \n",
    "        labels = torch.tensor(label, dtype=torch.long)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "def preprocess_data(df):\n",
    "    texts = df['tokens'].tolist()\n",
    "    labels = [list(map(int, tag.split())) for tag in df['ner_tags'].tolist()]\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = preprocess_data(train_combined_df)\n",
    "test_texts, test_labels = preprocess_data(test_combined_df)\n",
    "label_encoder = LabelEncoder()\n",
    "all_labels = np.concatenate(train_labels + test_labels)\n",
    "label_encoder.fit(all_labels)\n",
    "train_labels = [label_encoder.transform(label) for label in train_labels]\n",
    "test_labels = [label_encoder.transform(label) for label in test_labels]\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "train_dataset = TokenClassificationDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = TokenClassificationDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "class TokenClassificationModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(TokenClassificationModel, self).__init__()\n",
    "        self.xlm_roberta = XLMRobertaModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.xlm_roberta.config.hidden_size, num_labels)\n",
    "        self.loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.xlm_roberta(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state \n",
    "        logits = self.classifier(sequence_output) \n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "            return loss, logits\n",
    "        return logits\n",
    "\n",
    "model = TokenClassificationModel(model_name=\"xlm-roberta-base\", num_labels=num_labels)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    num_train_epochs=1,  \n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,  \n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    disable_tqdm=False,\n",
    "    seed=42,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=4,  \n",
    "    fp16=True, \n",
    "    dataloader_num_workers=8, \n",
    "    dataloader_pin_memory=True, \n",
    "    max_grad_norm=1.0,  \n",
    "    dataloader_drop_last=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lATubiqRsPHd"
   },
   "source": [
    "*بخش سوم:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T13:05:36.791086Z",
     "start_time": "2024-12-16T13:05:36.750128Z"
    },
    "vscode": {
     "languageId": "plaintext"
    },
    "id": "Ms82ha89sPHd",
    "outputId": "0efc9cf5-e305-44d2-9bcb-a5e64ddb4043",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: {'precision': 0.946338641622431, 'recall': 0.9852972436760433, 'f1-score': 0.9654250702954144, 'support': 28226.0}, 1: {'precision': 0.8288996372430472, 'recall': 0.6430581613508443, 'f1-score': 0.7242472266244057, 'support': 2132.0}, 2: {'precision': 0.9138576779026217, 'recall': 0.7778958554729012, 'f1-score': 0.8404133180252583, 'support': 941.0}, 3: {'precision': 0.8200836820083682, 'recall': 0.6426229508196721, 'f1-score': 0.7205882352941176, 'support': 2440.0}, 4: {'precision': 0.7794759825327511, 'recall': 0.649090909090909, 'f1-score': 0.7083333333333334, 'support': 550.0}, 5: {'precision': 0.8679430535100638, 'recall': 0.7491525423728813, 'f1-score': 0.804184671366841, 'support': 2360.0}, 6: {'precision': 0.8206896551724138, 'recall': 0.5458715596330275, 'f1-score': 0.6556473829201102, 'support': 218.0}, 7: {'precision': 0.7870722433460076, 'recall': 0.5376623376623376, 'f1-score': 0.6388888888888888, 'support': 770.0}, 8: {'precision': 0.7391304347826086, 'recall': 0.40669856459330145, 'f1-score': 0.5246913580246914, 'support': 418.0}, 9: {'precision': 0.43478260869565216, 'recall': 0.47619047619047616, 'f1-score': 0.45454545454545453, 'support': 21.0}, 10: {'precision': 0.8235632183908046, 'recall': 0.7704301075268817, 'f1-score': 0.7961111111111111, 'support': 1860.0}, 11: {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 13.0}, 12: {'precision': 0.8737864077669902, 'recall': 0.7058823529411765, 'f1-score': 0.7809110629067245, 'support': 510.0}, 13: {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 32.0}, 14: {'precision': 0.5263157894736842, 'recall': 0.36585365853658536, 'f1-score': 0.4316546762589928, 'support': 82.0}, 15: {'precision': 0.4878048780487805, 'recall': 0.5, 'f1-score': 0.49382716049382713, 'support': 160.0}, 16: {'precision': 0.5192307692307693, 'recall': 0.27835051546391754, 'f1-score': 0.3624161073825503, 'support': 97.0}, 17: {'precision': 0.7041420118343196, 'recall': 0.6819484240687679, 'f1-score': 0.6928675400291121, 'support': 349.0}, 18: {'precision': 0.7777777777777778, 'recall': 0.4666666666666667, 'f1-score': 0.5833333333333334, 'support': 120.0}, 19: {'precision': 0.6923076923076923, 'recall': 0.6136363636363636, 'f1-score': 0.6506024096385542, 'support': 88.0}, 20: {'precision': 0.951196823596143, 'recall': 0.9801052016364699, 'f1-score': 0.9654346574553828, 'support': 42775.0}, 'accuracy': 0.9329863834034362, 'macro avg': {'precision': 0.680685665963949, 'recall': 0.5607816138732964, 'f1-score': 0.6092439522822907, 'support': 84162.0}, 'weighted avg': {'precision': 0.9286386959098631, 'recall': 0.9329863834034362, 'f1-score': 0.9290246605124725, 'support': 84162.0}}\n",
      "Predicted labels: [1 0 5 0 0 0 0 0 0]\n",
      "\n",
      "استفاده از تنها دقت برای ارزیابی عملکرد مدل در شناسایی موجودیت‌های نام‌گذاری شده کافی نیست، زیرا در مشکلات با توزیع نابرابر کلاس‌ها، دقت نمی‌تواند عملکرد مدل در شناسایی کلاس‌های نادر را نشان دهد. به‌ویژه، مدل ممکن است برچسب‌های پرتکرار مانند را به درستی پیش‌بینی کند، اما در شناسایی موجودیت‌های مهم عملکرد ضعیفی داشته باشد. به همین دلیل، باید از معیارهای دقت مثبت، یادآوری و امتیاز ترکیبی برای ارزیابی دقیق‌تر مدل در شناسایی هر کلاس استفاده کنیم.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=2)\n",
    "\n",
    "true_labels = []\n",
    "pred_labels_flat = []\n",
    "\n",
    "for true, pred in zip(test_labels, pred_labels):\n",
    "    true_labels.extend([t for t, p in zip(true, pred) if t != -100])\n",
    "    pred_labels_flat.extend([p for t, p in zip(true, pred) if t != -100])\n",
    "\n",
    "report = classification_report(true_labels, pred_labels_flat, target_names=label_encoder.classes_, output_dict=True)\n",
    "print(report)\n",
    "\n",
    "sample_text = [\"John visited Paris last summer.\"]\n",
    "encoding = tokenizer(sample_text, padding=True, truncation=True, return_tensors=\"pt\", max_length=16)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_ids = encoding[\"input_ids\"].to(device)\n",
    "attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "model.eval()  \n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=encoding[\"input_ids\"].to(device), attention_mask=encoding[\"attention_mask\"].to(device))\n",
    "\n",
    "predictions = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "predicted_labels = predictions.flatten().cpu().numpy()\n",
    "\n",
    "predicted_label_names = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "\n",
    "print(\"Predicted labels:\", predicted_label_names)\n",
    "\n",
    "print(\"\"\"\n",
    "استفاده از تنها دقت برای ارزیابی عملکرد مدل در شناسایی موجودیت‌های نام‌گذاری شده کافی نیست، زیرا در مشکلات با توزیع نابرابر کلاس‌ها، دقت نمی‌تواند عملکرد مدل در شناسایی کلاس‌های نادر را نشان دهد. به‌ویژه، مدل ممکن است برچسب‌های پرتکرار مانند را به درستی پیش‌بینی کند، اما در شناسایی موجودیت‌های مهم عملکرد ضعیفی داشته باشد. به همین دلیل، باید از معیارهای دقت مثبت، یادآوری و امتیاز ترکیبی برای ارزیابی دقیق‌تر مدل در شناسایی هر کلاس استفاده کنیم.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3u2F7h_YMmvp"
   },
   "source": [
    "# **سوال دوم:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "id": "K9vruwaVsPHe",
    "outputId": "43633056-ff67-47b0-e80f-6c3e8544a7d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter's latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\n",
      "Summary: Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
      "Young actor says he has no plans to fritter his cash away .\n",
      "Radcliffe's earnings from first five Potter films have been held in trust fund .\n",
      "Min text length: 8, Max: 2347, Avg: 691.8703263175126\n",
      "Min summary length: 4, Max: 1296, Avg: 51.574101486174435\n",
      "\n",
      "آیا نیاز به پیش پردازش دادگان برای این سوال وجود دارد؟                                                                                          \n",
      "مدل‌های LLM مانند LLAMA محدودیت‌هایی در تعداد توکن‌ها دارند، بنابراین برای متون طولانی باید از تکنیک‌های برش (Truncation) یا تقسیم به بخش‌های کوتاه‌تر استفاده کرد. متن‌ها معمولاً حاوی نویزهایی مانند لینک‌ها، نام‌ها و کلمات تکراری هستند که باید حذف شوند تا مدل روی محتوای اصلی تمرکز کند. برای پردازش، داده‌ها باید به واحدهای توکن تبدیل شوند که نیاز به استفاده از کتابخانه‌های توکن‌سازی دارند. در تسک خلاصه‌سازی، داده‌ها معمولاً شامل متن اصلی و خلاصه هستند که باید به‌طور دقیق از هم جدا شوند. همچنین، ممکن است نیاز باشد که طول خلاصه‌ها تنظیم شود تا مدل به طور بهینه آن‌ها را تولید کند.                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "\n",
    "train_data = dataset['train'].select(range(400))\n",
    "val_data = dataset['validation'].select(range(100))\n",
    "test_data = dataset['test'].select(range(100))\n",
    "\n",
    "sample = train_data[0]\n",
    "print(f\"Text: {sample['article']}\")\n",
    "print(f\"Summary: {sample['highlights']}\")\n",
    "\n",
    "lengths_text = [len(sample['article'].split()) for sample in dataset['train']]\n",
    "lengths_summary = [len(sample['highlights'].split()) for sample in dataset['train']]\n",
    "\n",
    "min_text_length = min(lengths_text)\n",
    "max_text_length = max(lengths_text)\n",
    "avg_text_length = sum(lengths_text) / len(lengths_text)\n",
    "\n",
    "min_summary_length = min(lengths_summary)\n",
    "max_summary_length = max(lengths_summary)\n",
    "avg_summary_length = sum(lengths_summary) / len(lengths_summary)\n",
    "\n",
    "print(f\"Min text length: {min_text_length}, Max: {max_text_length}, Avg: {avg_text_length}\")\n",
    "print(f\"Min summary length: {min_summary_length}, Max: {max_summary_length}, Avg: {avg_summary_length}\")\n",
    "\n",
    "print(\"\"\"\n",
    "آیا نیاز به پیش پردازش دادگان برای این سوال وجود دارد؟\n",
    "مدل‌های LLM مانند LLAMA محدودیت‌هایی در تعداد توکن‌ها دارند، بنابراین برای متون طولانی باید از تکنیک‌های برش (Truncation) یا تقسیم به بخش‌های کوتاه‌تر استفاده کرد. متن‌ها معمولاً حاوی نویزهایی مانند لینک‌ها، نام‌ها و کلمات تکراری هستند که باید حذف شوند تا مدل روی محتوای اصلی تمرکز کند. برای پردازش، داده‌ها باید به واحدهای توکن تبدیل شوند که نیاز به استفاده از کتابخانه‌های توکن‌سازی دارند. در تسک خلاصه‌سازی، داده‌ها معمولاً شامل متن اصلی و خلاصه هستند که باید به‌طور دقیق از هم جدا شوند. همچنین، ممکن است نیاز باشد که طول خلاصه‌ها تنظیم شود تا مدل به طور بهینه آن‌ها را تولید کند.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgyQYySusPHe"
   },
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "سوالات خودتان را می‌توانید از طریق ایمیل\n",
    "<code>mohamadgorjicode@gmail.com</code>\n",
    " از طراح تمرین 4 برای سوال دوم بپرسید."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vAFTulEMpUx"
   },
   "source": [
    "*بخش اول:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-19T09:34:33.208027Z"
    },
    "jupyter": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ObA0618bsPHe",
    "outputId": "6159a67b-aed9-434d-9a97-418d48a4d8e3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting transformers==4.36.2\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/126.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m126.8/126.8 kB\u001B[0m \u001B[31m10.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting datasets==2.16.1\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate==0.26.1\n",
      "  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting evaluate==0.4.1\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting bitsandbytes==0.42.0\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (2.32.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.2)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.2) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (17.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets==2.16.1)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.16.1)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (2.2.2)\n",
      "Collecting xxhash (from datasets==2.16.1)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.16.1)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.16.1) (3.11.10)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.1) (2.5.1+cu121)\n",
      "Collecting responses<0.19 (from evaluate==0.4.1)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.42.0) (1.13.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.16.1) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.2) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.1) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.26.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.26.1) (1.3.0)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.16.1)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.16.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.16.1) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.16.1) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.26.1) (3.0.2)\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.2/8.2 MB\u001B[0m \u001B[31m82.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m507.1/507.1 kB\u001B[0m \u001B[31m35.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m270.9/270.9 kB\u001B[0m \u001B[31m23.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m8.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m105.0/105.0 MB\u001B[0m \u001B[31m8.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m115.3/115.3 kB\u001B[0m \u001B[31m11.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.4/166.4 kB\u001B[0m \u001B[31m16.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m98.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.8/134.8 kB\u001B[0m \u001B[31m13.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.1/194.1 kB\u001B[0m \u001B[31m20.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: xxhash, pyarrow-hotfix, fsspec, dill, responses, multiprocess, bitsandbytes, tokenizers, accelerate, transformers, datasets, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.2.1\n",
      "    Uninstalling accelerate-1.2.1:\n",
      "      Successfully uninstalled accelerate-1.2.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.1\n",
      "    Uninstalling transformers-4.47.1:\n",
      "      Successfully uninstalled transformers-4.47.1\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2023.10.0 which is incompatible.\n",
      "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.2 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed accelerate-0.26.1 bitsandbytes-0.42.0 datasets-2.16.1 dill-0.3.7 evaluate-0.4.1 fsspec-2023.10.0 multiprocess-0.70.15 pyarrow-hotfix-0.6 responses-0.18.0 tokenizers-0.15.2 transformers-4.36.2 xxhash-3.5.0\n",
      "Collecting git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e\n",
      "  Cloning https://github.com/huggingface/trl (to revision a3c5b7178ac4f65569975efadc97db2f3749c65e) to /tmp/pip-req-build-5y_gl88g\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/trl /tmp/pip-req-build-5y_gl88g\n",
      "  Running command git rev-parse -q --verify 'sha^a3c5b7178ac4f65569975efadc97db2f3749c65e'\n",
      "  Running command git fetch -q https://github.com/huggingface/trl a3c5b7178ac4f65569975efadc97db2f3749c65e\n",
      "  Running command git checkout -q a3c5b7178ac4f65569975efadc97db2f3749c65e\n",
      "  Resolved https://github.com/huggingface/trl to commit a3c5b7178ac4f65569975efadc97db2f3749c65e\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.7.11.dev0) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.7.11.dev0) (4.36.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl==0.7.11.dev0) (1.26.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl==0.7.11.dev0) (0.26.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl==0.7.11.dev0) (2.16.1)\n",
      "Collecting tyro>=0.5.11 (from trl==0.7.11.dev0)\n",
      "  Downloading tyro-0.9.4-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.7.11.dev0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.7.11.dev0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.7.11.dev0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.7.11.dev0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.7.11.dev0) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.7.11.dev0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.4.0->trl==0.7.11.dev0) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (0.27.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl==0.7.11.dev0) (4.67.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.7.11.dev0) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.7.11.dev0) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.7.11.dev0)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.7.11.dev0) (4.4.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl==0.7.11.dev0) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.7.11.dev0) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.7.11.dev0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.7.11.dev0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.7.11.dev0) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.7.11.dev0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.7.11.dev0) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.7.11.dev0) (3.11.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.7.11.dev0) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.7.11.dev0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.7.11.dev0) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.7.11.dev0) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.7.11.dev0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.7.11.dev0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.7.11.dev0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.7.11.dev0) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl==0.7.11.dev0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl==0.7.11.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl==0.7.11.dev0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl==0.7.11.dev0) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11.dev0) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl==0.7.11.dev0) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.7.11.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.7.11.dev0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.7.11.dev0) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.11.dev0) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.7.11.dev0) (1.17.0)\n",
      "Downloading tyro-0.9.4-py3-none-any.whl (112 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m112.4/112.4 kB\u001B[0m \u001B[31m8.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: trl\n",
      "  Building wheel for trl (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for trl: filename=trl-0.7.11.dev0-py3-none-any.whl size=151116 sha256=6124ef5042a8e6168e5e28d22bad249e3fb306bb9c9e7ba639528b4a938033ac\n",
      "  Stored in directory: /root/.cache/pip/wheels/69/fc/ec/e718a169cea1752bd9c3eb798e9d8a56686d0752c49f97e1c1\n",
      "Successfully built trl\n",
      "Installing collected packages: shtab, tyro, trl\n",
      "Successfully installed shtab-1.7.1 trl-0.7.11.dev0 tyro-0.9.4\n",
      "Collecting git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f\n",
      "  Cloning https://github.com/huggingface/peft (to revision 4a1559582281fc3c9283892caea8ccef1d6f5a4f) to /tmp/pip-req-build-z3fwgskk\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-z3fwgskk\n",
      "  Running command git rev-parse -q --verify 'sha^4a1559582281fc3c9283892caea8ccef1d6f5a4f'\n",
      "  Running command git fetch -q https://github.com/huggingface/peft 4a1559582281fc3c9283892caea8ccef1d6f5a4f\n",
      "  Running command git checkout -q 4a1559582281fc3c9283892caea8ccef1d6f5a4f\n",
      "  Resolved https://github.com/huggingface/peft to commit 4a1559582281fc3c9283892caea8ccef1d6f5a4f\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (4.36.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (0.26.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (0.4.5)\n",
      "Requirement already satisfied: huggingface_hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.7.2.dev0) (0.27.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.17.0->peft==0.7.2.dev0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.17.0->peft==0.7.2.dev0) (2023.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.17.0->peft==0.7.2.dev0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.17.0->peft==0.7.2.dev0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.7.2.dev0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.7.2.dev0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.7.2.dev0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.7.2.dev0) (0.15.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.7.2.dev0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.17.0->peft==0.7.2.dev0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.17.0->peft==0.7.2.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.17.0->peft==0.7.2.dev0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.17.0->peft==0.7.2.dev0) (2024.12.14)\n",
      "Building wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for peft: filename=peft-0.7.2.dev0-py3-none-any.whl size=183141 sha256=cfcf1011e24a0fcbb2ec0bd190ec86fe30fb8cfb1571b844e793c8a3e0d93b5c\n",
      "  Stored in directory: /root/.cache/pip/wheels/94/88/30/67a50390d00daa42c311fb8b162fb3b320e9cdab6bd0bb8568\n",
      "Successfully built peft\n",
      "Installing collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.14.0\n",
      "    Uninstalling peft-0.14.0:\n",
      "      Successfully uninstalled peft-0.14.0\n",
      "Successfully installed peft-0.7.2.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip install  --upgrade \\\n",
    "  \"transformers==4.36.2\" \\\n",
    "  \"datasets==2.16.1\" \\\n",
    "  \"accelerate==0.26.1\" \\\n",
    "  \"evaluate==0.4.1\" \\\n",
    "  \"bitsandbytes==0.42.0\" \\\n",
    "\n",
    "!pip install git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e --upgrade\n",
    "!pip install git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "\n",
    "train_data = dataset['train'].select(range(5000))\n",
    "val_data = dataset['validation'].select(range(500))\n",
    "test_data = dataset['test'].select(range(100))\n",
    "\n",
    "sample = train_data[0]\n",
    "print(f\"Text: {sample['article']}\")\n",
    "print(f\"Summary: {sample['highlights']}\")\n",
    "\n",
    "lengths_text = [len(sample['article'].split()) for sample in dataset['train']]\n",
    "lengths_summary = [len(sample['highlights'].split()) for sample in dataset['train']]\n",
    "\n",
    "min_text_length = min(lengths_text)\n",
    "max_text_length = max(lengths_text)\n",
    "avg_text_length = sum(lengths_text) / len(lengths_text)\n",
    "\n",
    "min_summary_length = min(lengths_summary)\n",
    "max_summary_length = max(lengths_summary)\n",
    "avg_summary_length = sum(lengths_summary) / len(lengths_summary)\n",
    "\n",
    "print(f\"Min text length: {min_text_length}, Max: {max_text_length}, Avg: {avg_text_length}\")\n",
    "print(f\"Min summary length: {min_summary_length}, Max: {max_summary_length}, Avg: {avg_summary_length}\")\n",
    "\n",
    "print(\"\"\"\n",
    "آیا نیاز به پیش پردازش دادگان برای این سوال وجود دارد؟\n",
    "مدل‌های LLM مانند LLAMA محدودیت‌هایی در تعداد توکن‌ها دارند، بنابراین برای متون طولانی باید از تکنیک‌های برش (Truncation) یا تقسیم به بخش‌های کوتاه‌تر استفاده کرد. متن‌ها معمولاً حاوی نویزهایی مانند لینک‌ها، نام‌ها و کلمات تکراری هستند که باید حذف شوند تا مدل روی محتوای اصلی تمرکز کند. برای پردازش، داده‌ها باید به واحدهای توکن تبدیل شوند که نیاز به استفاده از کتابخانه‌های توکن‌سازی دارند. در تسک خلاصه‌سازی، داده‌ها معمولاً شامل متن اصلی و خلاصه هستند که باید به‌طور دقیق از هم جدا شوند. همچنین، ممکن است نیاز باشد که طول خلاصه‌ها تنظیم شود تا مدل به طور بهینه آن‌ها را تولید کند.\n",
    "\"\"\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-SdWKpnbwq-u",
    "outputId": "d9c72fa6-a727-46a3-c5a5-501704c75625"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text: LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter's latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\n",
      "Summary: Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
      "Young actor says he has no plans to fritter his cash away .\n",
      "Radcliffe's earnings from first five Potter films have been held in trust fund .\n",
      "Min text length: 8, Max: 2347, Avg: 691.8703263175126\n",
      "Min summary length: 4, Max: 1296, Avg: 51.574101486174435\n",
      "\n",
      "آیا نیاز به پیش پردازش دادگان برای این سوال وجود دارد؟\n",
      "مدل‌های LLM مانند LLAMA محدودیت‌هایی در تعداد توکن‌ها دارند، بنابراین برای متون طولانی باید از تکنیک‌های برش (Truncation) یا تقسیم به بخش‌های کوتاه‌تر استفاده کرد. متن‌ها معمولاً حاوی نویزهایی مانند لینک‌ها، نام‌ها و کلمات تکراری هستند که باید حذف شوند تا مدل روی محتوای اصلی تمرکز کند. برای پردازش، داده‌ها باید به واحدهای توکن تبدیل شوند که نیاز به استفاده از کتابخانه‌های توکن‌سازی دارند. در تسک خلاصه‌سازی، داده‌ها معمولاً شامل متن اصلی و خلاصه هستند که باید به‌طور دقیق از هم جدا شوند. همچنین، ممکن است نیاز باشد که طول خلاصه‌ها تنظیم شود تا مدل به طور بهینه آن‌ها را تولید کند.\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PgEUgHeJsPHf",
    "outputId": "31775734-7d9f-4c74-9b1c-31375771fcfd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "model_id = \"HuggingFaceTB/SmolLM2-1.7B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "import peft\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, LoraConfig\n",
    "\n"
   ],
   "metadata": {
    "id": "nENrjv_X3rUd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4bf13b3f-1b8d-4d1b-96db-edb442fd2534"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install trl\n",
    "from trl import SFTTrainer"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MJb3K06ZLGbR",
    "outputId": "20e55b66-af78-4839-828c-37a80d212937"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.7.11.dev0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.36.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl) (1.26.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl) (0.26.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl) (2.16.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.9.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.27.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (4.67.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (4.4.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.11.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->trl) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.17.0)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Trainer"
   ],
   "metadata": {
    "id": "CT1pQIDH-9-v"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from peft import get_peft_model"
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "def preprocess_function(examples):\n",
    "    output_texts = []\n",
    "    for i in range(len(examples)):\n",
    "        article_text = examples['article'][i] \n",
    "        highlight_text = examples['highlights'][i]  \n",
    "        text = f\"### Article: {article_text} \\n ### Summary: {highlight_text}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                \n",
    "    lora_alpha=32,      \n",
    "    lora_dropout=0.1,   \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    bias=\"none\",       \n",
    "    task_type=\"CAUSAL_LM\" \n",
    ")\n",
    "\n",
    "print(dataset['train'][0])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",               \n",
    "    evaluation_strategy=\"epoch\",        \n",
    "    learning_rate=2e-5,                   \n",
    "    per_device_train_batch_size=8,        \n",
    "    per_device_eval_batch_size=8,         \n",
    "    num_train_epochs=3,                   \n",
    "    weight_decay=0.01,                    \n",
    "    logging_dir='./logs',                 \n",
    "    logging_steps=200,                    \n",
    "    save_steps=1000,                      \n",
    "    save_total_limit=2,                   \n",
    ")\n",
    "peft_config =LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                \n",
    "    args=training_args,              \n",
    "    train_dataset=train_data,        \n",
    "    eval_dataset=val_data,            \n",
    "    peft_config=peft_config,         \n",
    "    tokenizer=tokenizer,           \n",
    "    formatting_func=preprocess_function,\n",
    "    )\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293,
     "referenced_widgets": [
      "e9694627796b4f5786ff700dab3b170b",
      "b5ccba4f497a4c0c97ef8e6a96469844",
      "2bd1d8c3555f41ceabfb1573912152d9",
      "06074ba7900949d9b2519dfea83c94e4",
      "2379ccf18d0f4b6bb934486b0619adf0",
      "630bbc44049a46e3b325760b73e24a56",
      "183c716e62684f8a8852618d4451991e",
      "307d91a57fe44fc2a1290144f035bddb",
      "42ab117718da4cb09cdb2055f766ec1f",
      "febf915104234064b548caeae5822c88",
      "c822d8858bb346e2b6cd554e5472675a"
     ]
    },
    "id": "TagzxV29wiYW",
    "outputId": "15038599-534d-444c-9432-a2bd0e495c4f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.', 'highlights': \"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\", 'id': '42c027e4ff9730fbb3de84c1af0d2c506e41c3e4'}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9694627796b4f5786ff700dab3b170b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.717671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.698275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.689795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=9.71615219116211, metrics={'train_runtime': 5.6022, 'train_samples_per_second': 8.033, 'train_steps_per_second': 1.071, 'total_flos': 4090484551680.0, 'train_loss': 9.71615219116211, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install rouge-score\n",
    "!pip install bert-score"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hp3TitfKOT1I",
    "outputId": "ed01f9bb-aecf-4400-a9d9-c7f6ec511c1c"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.67.1)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=98338c61244df2594ba2527529297055e04676d6ede0b165dd3ee0bd0bf5d45f\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.5.1+cu121)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.2.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.36.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.27.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2024.12.14)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.1/61.1 kB\u001B[0m \u001B[31m6.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: bert-score\n",
      "Successfully installed bert-score-0.3.13\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "sample_article = test_data[0][\"article\"]\n",
    "expected_summary = test_data[0][\"highlights\"]\n",
    "\n",
    "generated_summary = summarizer(sample_article, max_length=150, min_length=50, do_sample=False)\n",
    "\n",
    "print(\"Generated Summary: \", generated_summary[0]['summary_text'])\n",
    "print(\"Expected Summary: \", expected_summary)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = scorer.score(expected_summary, generated_summary[0]['summary_text'])\n",
    "\n",
    "print(\"\\nROUGE Scores: \")\n",
    "print(rouge_scores)\n",
    "\n",
    "references = [expected_summary]\n",
    "candidates = [generated_summary[0]['summary_text']]\n",
    "\n",
    "P, R, F1 = score(candidates, references, lang='en')\n",
    "\n",
    "print(\"\\nBERTScore:\")\n",
    "print(f\"Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516,
     "referenced_widgets": [
      "18073111638147ff9f0955bbf81d0b05",
      "90d61c7523294e328ee30706f5a2d5db",
      "a458ea4c5e38449d9d3bcf340b4f5380",
      "fc8f2111100a4ae8b719e942fe7e0ff3",
      "7bb7bf42314e4c9f9273a6ebecb3cdb9",
      "56cbaac128e24d6e86ac5f24dae3451d",
      "dbdd1eb2ab104058b225892890743e43",
      "b698bcc7fb8a45afaca5f3725a496a8b",
      "1cf14c31ed624840a9ea685001badedc",
      "b699f76445694685a79b974c496aa5c7",
      "9f596f30b7244f19ba3e146a4f478991",
      "d239422e9076457cbb57b5fd8e42d4b9",
      "a86c70356888413e9b583650cbd36627",
      "f455532c7c0e4559a9961a83d0ba957b",
      "2a6c76f5f3334f83bd32141470b2037e",
      "2932f7c4450e47d49bb0bfc9e5b59123",
      "a09bff9b10034f3a936532a2ad59e1fe",
      "07cd0c71457c47c6853e9db6f341aad5",
      "2c12126e4d094ebba02c8ebac5901485",
      "c56a8d81f12f4622a61fea35f90fd294",
      "94e47b6e9fba461d93686337d9cc2df1",
      "277316b857694074ab7a596825887979",
      "fd5a7d13ac2f4cb3b3b3c5ffb4f88e5c",
      "2321c7ce6343443882c0e294451cc404",
      "c6b54372970549bfbb2d4c8b77128fc6",
      "23d7e33aaa3a4d96a782491688748e09",
      "5d22dcb79a134ca281a3d50a805ab4b7",
      "d2f1fcc486a8460ca049b97279bcf6d5",
      "cc6efec711f44fba9a7f4eb0035b3d34",
      "bf490ff44ef947e1986a0517c004a095",
      "a4b6c0aed69247c4bdd0610449db1166",
      "ff18ad906a0c497a91ef3bdf8cafb7c8",
      "cefec0596137440cbd8e54b5fc738029",
      "7f657ee09a724d29a00071ee7c6bd78b",
      "899248dc8031417da6225c1dc3e35547",
      "d31ec9b854ab4c14a18b9c4244c4de09",
      "1846f523c01b4bc3a5af821fb056b0dc",
      "c2118dd6916d4a8b89d0b4dfa70060e0",
      "14e255109295400587d4f38d8ccc8b75",
      "24294a1a72f64d658f4cf8bea5153555",
      "6dc977c1d42948d7a3abd0b409a45ee7",
      "f4e17b485e764d84b8c592383300903b",
      "5f17a236f4874cf0ab1e0a221f4d3ba2",
      "087000335fc44e99a64cb7bdd58e5b48",
      "9cb8c4f02f58498ab92931b0970e9fdd",
      "592d98108542433cb3a4ea93cce0b535",
      "1e10429495eb4ee6b67d988cc505da44",
      "1052aa85173a4978a09f10475e333eff",
      "3854edd65288486ea8603dab10fe70a0",
      "597982f47ea24974827927d0bbe1fd97",
      "0b94ce4927654d89b89f3af141bb9cd7",
      "076e03cf988f49229a375134d8c42c8e",
      "0baf4fd20d4142c6a611c5e9c28130a8",
      "d9fe0351536b4842a8915797af3fb31d",
      "d87cac6b2d21481eb02424d60815ae36",
      "be58fa99be5047618084db9a572296ad",
      "80c700c897e94d3983646cfc32c83120",
      "242f980b60ed401dbaeb3cb6c33f7a84",
      "ac94fbee2609457aa24d64ffbeee9214",
      "e60fc2ef178445b9ac4e052a160d0144",
      "74bfb5fab9da4f05ba3477aecb317966",
      "7189308a93e741dab59f7bcb0e92f6d2",
      "d79f1f78bb774e4bb779b28ae0439b81",
      "efca7867bf7f49d1b2f8263181dd2466",
      "da5a2d88353c4b1cae7f75f57f59e82d",
      "cdb4681e2112474697e64ae77b34fbf6"
     ]
    },
    "id": "d43owxjSM6Ec",
    "outputId": "da11a3ba-8f08-4297-fab5-a96d4ddff2c1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model 'LlamaForCausalLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generated Summary:  (CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court's decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.\n",
      "\n",
      "Expected Summary:  Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\n",
      "Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n",
      "\n",
      "ROUGE Scores: \n",
      "{'rouge1': Score(precision=0.05536332179930796, recall=0.9411764705882353, fmeasure=0.10457516339869281), 'rouge2': Score(precision=0.03119584055459272, recall=0.5454545454545454, fmeasure=0.05901639344262296), 'rougeL': Score(precision=0.04844290657439446, recall=0.8235294117647058, fmeasure=0.0915032679738562)}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18073111638147ff9f0955bbf81d0b05"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d239422e9076457cbb57b5fd8e42d4b9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd5a7d13ac2f4cb3b3b3c5ffb4f88e5c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f657ee09a724d29a00071ee7c6bd78b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9cb8c4f02f58498ab92931b0970e9fdd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be58fa99be5047618084db9a572296ad"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "BERTScore:\n",
      "Precision: 0.8005, Recall: 0.9034, F1: 0.8488\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "sample_indices = random.sample(range(len(train_data)), 3)\n",
    "sample_data = [train_data[i] for i in sample_indices]\n",
    "\n",
    "few_shot_input = \"\"\n",
    "for sample in sample_data:\n",
    "    article_text = sample['article']\n",
    "    highlight_text = sample['highlights']\n",
    "    few_shot_input += f\"Article: {article_text}\\nSummary: {highlight_text}\\n\\n\"\n",
    "\n",
    "print(\"Few-Shot Input Examples:\\n\", few_shot_input)\n",
    "\n",
    "test_article = test_data[0]['article']\n",
    "\n",
    "input_text = few_shot_input + f\"Article: {test_article}\\nSummary:\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=1024, padding=True)\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=150, num_beams=4, early_stopping=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Summary: \", generated_summary)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "expected_summary = test_data[0][\"highlights\"]\n",
    "rouge_scores = scorer.score(expected_summary, generated_summary)\n",
    "\n",
    "print(\"\\nROUGE Scores: \")\n",
    "print(rouge_scores)\n",
    "\n",
    "references = [expected_summary]\n",
    "candidates = [generated_summary]\n",
    "\n",
    "P, R, F1 = score(candidates, references, lang='en')\n",
    "print(\"\\nBERTScore:\")\n",
    "print(f\"Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRUTOdbjQLe5",
    "outputId": "97eb48a0-ef30-4986-a4de-ca14caa7e782"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Few-Shot Input Examples:\n",
      " Article: ATLANTA, Georgia (CNN)  -- The two men who claimed to have found the carcass of Bigfoot have surfaced to say: Hey, it was just a joke. Matt Whitton has been fired from his job as a police officer because of his role in the hoax. Not everyone is laughing. In an exclusive interview with CNN affiliate WSB, the two hoaxers -- car salesman Rick Dyer and now-fired police officer Matt Whitton -- said the whole situation began as a joke and then got out of hand. \"It's just a big hoax, a big joke,\" Dyer said. \"It's Bigfoot,\" Dyer explained. \"Bigfoot doesn't exist.\" Whitton chimed in: \"All this was a big joke. It got into something way bigger than it was supposed to be.\"  Watch the two men explain their \"joke\" » . At a news conference in California last week, the two men had stood by their claims that they had discovered Bigfoot's corpse and had it on ice. Scientific analysis would prove it, they said. Not quite. Now the two Georgia men admit that the hairy, icy blob was an Internet-purchased Sasquatch costume stuffed with possum roadkill and slaughterhouse leftovers. Whitton and Dyer say that when they came up with the hoax, they had no idea it would become a media circus. \"It got legs and ran. It's crazy now,\" Dyer told WSB. Co-hoaxer Whitton agrees: \"It started off as some YouTube videos and a Web site. We're all about having fun.\" \"Fun\" isn't exactly how Clayton County Police Chief Jeff Turner sees it. He has kicked Whitton off the police force. \"He lied on national TV,\" Turner says of Whitton, \"so a defense attorney now could say, 'How do we know you're not lying now?' \" Whitton and Dyer had announced that they had found the body of a 7-foot-7-inch, 500-pound half-ape, half-human creature while hiking in the north Georgia mountains in June. They also said they had spotted about three similar living creatures. Still unclear is how much money Whitton and Dyer got out of the hoax. Steve Kulls, who maintains the SquatchDetective Web site and hosts a similarly named Internet radio program, first interviewed Dyer on July 28 for the radio program. On August 12, Kulls said, Dyer and Whitton \"requested an undisclosed sum of money as an advance, expected from the marketing and promotion.\" Two days later, after signing a receipt and counting the money, Dyer and Whitton showed the Searching for Bigfoot team the freezer containing what they claimed was the carcass: \"Something appearing large, hairy and frozen in ice,\" Kulls wrote on the Web site. It was, as many had suspected, an ape-like costume stuffed with entrails. After the news conference last week, Dyer and Whitton disappeared from view. The truth came out over the weekend. In a Web posting this week, Kulls wrote that \"action is being instigated against the perpetrators.\" The two hoaxers have hired attorney Steve Lister to represent them. \"There have been some threats made to them for both civil and criminal prosecution,\" Lister said. The attorney says the Bigfoot incident \"got out of hand.\" Dyer, asked whether he ever thought that the hoopla had become more than just a joke, implied that everyone should have known it was a hoax. \"Well, we told 10 different stories,\" he said. \"Everyone knew we were lying.\"\n",
      "Summary: Two men surface to say Bigfoot hoax was just a joke .\n",
      "Men say their idea of \"having fun\" turned into something bigger than expected .\n",
      "Attorney for men says incident \"got out of hand\"\n",
      "\n",
      "Article: (CNN)  -- Pakistani President Asif Ali Zardari denied his nation was involved in last week's deadly attacks on Mumbai, India, and told CNN on Tuesday he's seen no evidence that a suspect in custody is a Pakistani national as Indian officials claim. Pakistani President Asif Ali Zardari says he believes the Mumbai attackers were \"stateless actors.\" \"I think these are stateless actors who have been operating all throughout the region,\" Zardari said on CNN's \"Larry King Live\" in an interview set to air Tuesday night. \"The gunmen plus the planners, whoever they are, [are] stateless actors who have been holding hostage the whole world.\" At least 179 people were killed when a band of gunmen attacked 10 targets in Mumbai on Wednesday night, triggering three days of battles with police and Indian troops in the heart of the city -- the hub of India's financial and entertainment industries. Most of the deaths occurred at the city's top two hotels: the Oberoi and the Taj Mahal.  Watch Zardari blame \"stateless actors\" » . Indian officials have publicly blamed Pakistani militants for the attacks, and called on Pakistan to hand over a group of wanted militant leaders suspected of plotting them. On Tuesday, Pakistani Foreign Minister Shah Mahmood Qureshi proposed a joint investigation into the attacks and said, \"This is not the time to point fingers.\" Zardari confirmed he is willing to have Pakistani security officials participate with India in a joint investigation. \"The state of Pakistan is in no way responsible,\" Zardari told King. \"... Even the White House and the American CIA have said that today. The state of Pakistan is, of course, not involved. We're part of the victims, Larry. I'm a victim. The state of Pakistan is a victim. We are the victims of this war, and I am sorry for the Indians, and I feel sorry for them.\" Indian officials have said that the only suspected attacker in custody has told police he is a Pakistani national. Indian intelligence sources have told CNN's sister network, CNN-IBN, that police believe all the attackers  were Pakistanis. Indian police say nine of the 10 attackers were killed by Indian forces. Asked about the suspect in custody, Zardari said: \"We have not been given any tangible proof to say that he is definitely a Pakistani. I very much doubt it, Larry, that he is a Pakistani.\" He said Pakistan is looking into the allegation, but added, \"Like I said, these are stateless individuals. ... We've had incidents the past two days in Karachi where we've lost more than 40 to 45 people, hundreds injured. These are stateless actors who are moving throughout this region.\" India summoned Pakistan's high commissioner, the top-ranking Pakistani diplomat in New Delhi, to External Affairs Minister Pranab Mukherjee's office Monday to inform him that last week's massacre in Mumbai \"was carried out by elements from Pakistan.\" It renewed a demand that Pakistan hand over a group of militant leaders whose extradition it has sought since a 2001 attack on India's Parliament that brought the South Asian nuclear rivals to the brink of war. \"The government expects that strong action would be taken against those elements, whosoever they may be, responsible for this outrage,\" a statement from India's Foreign Ministry said. \"It was conveyed to the Pakistan high commissioner that Pakistan's actions needed to match the sentiments expressed by its leadership that it wishes to have a qualitatively new relationship with India.\" The list reportedly includes Hafiz Mohammed, the head of Lashkar-e-Tayyiba, a now-banned Islamic militant group that denied last week that it was involved in the Mumbai attack. The group is blamed for the 2001 attack on India's Parliament. \"I am definitely going to look into all the possibility of any proof that is given to us,\" Zardari said. \"At the moment, these are just names of individuals. No proof, no investigation, nothing has been brought forward.\" If proof of the individuals' involvement is provided, he said, \"We would try them in our courts, we would try them in our land, and we would sentence them.\" Indian authorities said the suspect in custody was trained by Lashkar-e-Tayyiba. Zardari told CNN that Lashkar-e-Tayyiba is a \"banned organization\" in Pakistan and around the world. \"If indeed they are involved, we would not know,\" he said. \"Again, they are people who operate outside the system. They operate like -- al Qaeda, for instance, is not state-oriented. They operate something on that mechanism, and ... I've already offered India full cooperation on this incident, and we intend to do that.\" \"I'm firmly committed to fighting terrorism per se,\" he said. \"That's why we are fighting them every day, Larry.\" Asked about the possibility of Indian military strikes against terrorist camps in Pakistan, Zardari said: \"I would not agree with that because this is a time to come together and do a joint investigation and look at the problem in the larger context. We have a larger threat on our hands ... it's a threat throughout the region. So that would be counterproductive.\" Pakistan and India, both nuclear powers, have a tense relationship and have fought three wars since the subcontinent was divided in 1947. On whether the Mumbai attacks could trigger a fourth war, Zardari said: \"Larry, democracies don't go to war. All those wars you're talking about did not take place in any democracy. They all happened in the times of dictators. ... \"The whole nation of Pakistan is united to ... becoming friends with India,\" he said.\n",
      "Summary: Pakistani President Zardari: \"Stateless actors\" behind attacks in Mumbai, India .\n",
      "Indian intelligence sources tell CNN-IBN they believe attackers were Pakistani .\n",
      "Zardari says he doubts suspect in custody is Pakistani .\n",
      "He says he's willing to have Pakistan participate in a joint investigation with India .\n",
      "\n",
      "Article: NEW YORK (CNN) -- It's been five years since Carrie Bradshaw journeyed to Paris in search of true love on the series finale of \"Sex and the City.\" She appeared to have found it in the arms of Mr. Big, and she returned to New York -- and her now-settled friends -- ready for a new start. Sarah Jessica Parker was a driving force in creating the \"Sex and the City\" movie. Then came the inevitable cry: That's it? What happens next? Sarah Jessica Parker, who played Carrie, wanted to find out as well. But the situation had to be right, she said, which prompted a cascade of rumors as plans for a movie came together, fell apart and came together again. Now that the movie is out, Parker -- who's a producer of the film as well as one of its stars -- talked about the journey to making a big-screen \"Sex and the City\" with \"Showbiz Tonight\" anchor A.J. Hammer. The following is an edited version of that interview. CNN: I think a lot of fans, maybe a lot of people, and those of you among the cast, didn't think this day would actually ever come ... but here we are. So how are you feeling deep inside, Sarah? Sarah Jessica Parker: I feel extraordinarily privileged. I've spent the last two years cobbling this movie together. ... It's a once-in-a-lifetime kind of professional experience and one really shouldn't be greedy enough to ask for it twice.  Watch the cast talk about the thrill of \"Sex\" » . I have to say that, this last six, eight months, was better than those seven years [the show aired] and I think it's because we all recognize how lucky and unique those seven years were and that this is a story that you don't get to tell twice. It has been, I must say, worth every obstacle and dead end, and fit and start, and every moment that was seemingly impossible. It has been a dream. CNN: And I imagine sitting down for the first time in that room together for the table read, which was, from my understanding, the first time you all actually were in the same room [together] regardless of how much you kept in touch. Tell me a little bit about that moment. Parker: I started putting [the] script back together in April of 2006 ... and that [table read] was a really extraordinary day, because just the perfunctory details of getting people to a table read were complicated. Kim [Cattrall] had been away and Kristin [Davis] had been away and Chris [Noth] had been [doing] his other job, and this magnificent script had arrived and had been everything we hoped for and more. It was basically like being in an alternate universe for about three hours. ... It was a kind of reunion that is very, very special, because you really want to be there. It's not the reunion where you're forced by your parents to meet your aunts and uncles that you see rarely. It's the reunion that you want. I think even more so was that first day on [the] set. When we thought, good God, [writer/director] Michael Patrick [King] and I are actually making this movie, like we got it done, we're here, we're doing it -- what a privilege. CNN: I think we as fans and viewers actually got a sense of what that feeling was like when we saw you all together on \"Oprah,\" because the energy was ... palpable. ... But we're talking about the perceived drama around this whole project. (Rumors have abounded about friction between the stars.) One of my producers said while we were watching the TV, \"Are they going to sit near each other?\" \"Why are they putting Chris in the middle of all of them?\" Hearing that, does it make you mad? Parker: I find it slightly -- it's not that I'm mad. I expect better from people. I think it's really beneath me, to keep defending myself. I have a 35-year career. I have an impeccable character, I really pride myself in my work ethic and the way I treat people. And I think Kim would say the same and Cynthia would say the same. And I love Kim, and I wanted her to be in this movie. We couldn't have done it without her, we couldn't have done it without Kristin, we couldn't have done it without Cynthia [Nixon], nor could we have done it without Chris.  Watch Cattrall address the rumors » . You know, this is a story that people like to tell about women. Why? I don't know. Is it that interesting? Probably not. So once again I just have to say it doesn't define the experience. Nobody can take the experience away. It's far more interesting for me to talk about my affection for this cast than to start to deny a sad old beaten tale. CNN: You seem to have what goes on in the media in perspective -- and you have to, being at the game as long as you've been. To that end, one of the things that I've always admired about you and [husband] Matthew [Broderick] and I think a lot of people do, is how you've managed your public or your private life while being in the glare of the spotlight. ... Now I know one of the main reasons you guys are able to make it work is because you don't talk about your private lives, which is great and I applaud that. That said, is there something that you can tell me about what it is about your relationship that enables you to make it work separate from that? Parker: I wouldn't make any proclamations about why I have a marriage that, to me, is successful. I would just say that we've chosen to live in a city where we are not the most interesting people. This is a city that is about industry and finance and publishing and architecture and the arts and education and academia, and the movie industry fits into it in some small way, but there are a lot of people of important interest and I think that it's a conscious choice to live in a place where we're bumping up against humanity. We run to the market on our own, we take the subway, and we integrate into our city, and we become a part of the fabric and I think it's really been to our benefit and certainly to our son. Does it mean that we are not scrutinized and that we don't have paparazzi every single day at our house? No, but it is a city where you can't live behind a gate, you can't drive up in a car and be protected. You walk out the door and it is what it is. So you reconcile those things and you make the best choices you can. CNN: Is it the end of \"Sex and the City\"? What does your gut tell you? Parker: My gut tells me it's up to you and your colleagues and the critics and the people who show up. The future is dictated not by us at this point, and I think Michael Patrick and I have been so focused at getting this movie up on the big screen, that we haven't thought about the future. Doing this was more than we could have asked for. So anything beyond that is really kind of out of our hands in a perfectly wonderful way.\n",
      "Summary: Sarah Jessica Parker: \"Sex and the City\" \"has been a dream\"\n",
      "Rumors of friction? \"Beneath me to keep defending myself\"\n",
      "Marriage to Matthew Broderick works because pair lives normal N.Y. life .\n",
      "\n",
      "\n",
      "Generated Summary:  Article: ATLANTA, Georgia (CNN)  -- The two men who claimed to have found the carcass of Bigfoot have surfaced to say: Hey, it was just a joke. Matt Whitton has been fired from his job as a police officer because of his role in the hoax. Not everyone is laughing. In an exclusive interview with CNN affiliate WSB, the two hoaxers -- car salesman Rick Dyer and now-fired police officer Matt Whitton -- said the whole situation began as a joke and then got out of hand. \"It's just a big hoax, a big joke,\" Dyer said. \"It's Bigfoot,\" Dyer explained. \"Bigfoot doesn't exist.\" Whitton chimed in: \"All this was a big joke. It got into something way bigger than it was supposed to be.\"  Watch the two men explain their \"joke\" » . At a news conference in California last week, the two men had stood by their claims that they had discovered Bigfoot's corpse and had it on ice. Scientific analysis would prove it, they said. Not quite. Now the two Georgia men admit that the hairy, icy blob was an Internet-purchased Sasquatch costume stuffed with possum roadkill and slaughterhouse leftovers. Whitton and Dyer say that when they came up with the hoax, they had no idea it would become a media circus. \"It got legs and ran. It's crazy now,\" Dyer told WSB. Co-hoaxer Whitton agrees: \"It started off as some YouTube videos and a Web site. We're all about having fun.\" \"Fun\" isn't exactly how Clayton County Police Chief Jeff Turner sees it. He has kicked Whitton off the police force. \"He lied on national TV,\" Turner says of Whitton, \"so a defense attorney now could say, 'How do we know you're not lying now?' \" Whitton and Dyer had announced that they had found the body of a 7-foot-7-inch, 500-pound half-ape, half-human creature while hiking in the north Georgia mountains in June. They also said they had spotted about three similar living creatures. Still unclear is how much money Whitton and Dyer got out of the hoax. Steve Kulls, who maintains the SquatchDetective Web site and hosts a similarly named Internet radio program, first interviewed Dyer on July 28 for the radio program. On August 12, Kulls said, Dyer and Whitton \"requested an undisclosed sum of money as an advance, expected from the marketing and promotion.\" Two days later, after signing a receipt and counting the money, Dyer and Whitton showed the Searching for Bigfoot team the freezer containing what they claimed was the carcass: \"Something appearing large, hairy and frozen in ice,\" Kulls wrote on the Web site. It was, as many had suspected, an ape-like costume stuffed with entrails. After the news conference last week, Dyer and Whitton disappeared from view. The truth came out over the weekend. In a Web posting this week, Kulls wrote that \"action is being instigated against the perpetrators.\" The two hoaxers have hired attorney Steve Lister to represent them. \"There have been some threats made to them for both civil and criminal prosecution,\" Lister said. The attorney says the Bigfoot incident \"got out of hand.\" Dyer, asked whether he ever thought that the hoopla had become more than just a joke, implied that everyone should have known it was a hoax. \"Well, we told 10 different stories,\" he said. \"Everyone knew we were lying.\"\n",
      "Summary: Two men surface to say Bigfoot hoax was just a joke .\n",
      "Men say their idea of \"having fun\" turned into something bigger than expected .\n",
      "Attorney for men says incident \"got out of hand\"\n",
      "\n",
      "Article: (CNN)  -- Pakistani President Asif Ali Zardari denied his nation was involved in last week's deadly attacks on Mumbai, India, and told CNN on Tuesday he's seen no evidence that a suspect in custody is a Pakistani national as Indian officials claim. Pakistani President Asif Ali Zardari says he believes the Mumbai attackers were \"stateless actors.\" \"I think these are stateless actors who have been operating all throughout the region,\" Zardari said on CNN's \"Larry King Live\" in an interview set to air Tuesday night. \"The gunmen plus the planners, whoever they are, [are] stateless actors who have been holding hostage the whole world.\" At least 179 people were killed when a band of gunmen attacked 10 targets in Mumbai on Wednesday night, triggering three days of battles with police and Indian troops in the heart of the city -- the hub of India's financial and entertainment industries. Most of the deaths occurred at the city's top two hotels: the Taj Mahal Palace Hotel and the Oberoi Trident Hotel. Zardari said he's seen no evidence that a suspect in custody is a Pakistani national as Indian officials claim. \"I think these are stateless actors who have been operating all throughout the region,\" Zardari said on CNN's \"Larry King Live\" in an interview set to air Tuesday night. \"The gunmen plus the planners, whoever they are, [are] stateless actors who have been holding hostage the whole world.\" At least 179 people were killed when a band of gunmen attacked 10 targets in Mumbai on Wednesday night, triggering three days of battles with police and Indian troops in the heart of the city -- the hub of India's\n",
      "\n",
      "ROUGE Scores: \n",
      "{'rouge1': Score(precision=0.013559322033898305, recall=0.35294117647058826, fmeasure=0.026115342763873773), 'rouge2': Score(precision=0.0011312217194570137, recall=0.030303030303030304, fmeasure=0.0021810250817884407), 'rougeL': Score(precision=0.010169491525423728, recall=0.2647058823529412, fmeasure=0.019586507072905327)}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "BERTScore:\n",
      "Precision: 0.7652, Recall: 0.8253, F1: 0.7941\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\"\"\n",
    "                                                                                                                                                                      چالش‌های ارزیابی متن تولیدی با استفاده از معیارهای امتیاز برت و روژ\n",
    "روژ بیشتر به شباهت‌های واژگانی می‌پردازد، اما برای ارزیابی معنای عمیق‌تر مناسب نیست. امتیاز برت در ارزیابی معنایی عملکرد بهتری دارد اما نیاز به منابع بیشتر و محاسبات پیچیده‌تر دارد. انتخاب بین این دو بستگی به اولویت بین دقت معنایی و سرعت محاسبات دارد.\n",
    "\n",
    "                                                                                                                                                                                            چرا باید از کوانتایزیشن ۴ بیتی استفاده کرد؟\n",
    "کوانتایزیشن ۴ بیتی باعث کاهش اندازه مدل و بهبود سرعت پردازش می‌شود که برای منابع محدود مفید است. این روش مصرف حافظه را کاهش داده و در دستگاه‌های موبایل یا کم‌قدرت کاربرد دارد، اما دقت مدل کاهش می‌یابد. مناسب برای کاربردهایی با نیاز به سرعت بالا است.\n",
    "\n",
    "                                                                                                                                                                                             چه معیارهایی بهترین انتخاب برای سناریوهای کم‌شات هستند؟\n",
    "در سناریوهای کم‌شات، معیارهایی مانند امتیاز برت بهتر از روژ عمل می‌کنند چون دقت بیشتری در ارزیابی تطابق معنایی دارند. این سناریوها معمولاً به مدل‌هایی نیاز دارند که درک عمیقی از معنای متن داشته باشند. معیار مناسب بستگی به نوع متن و کاربرد خاص دارد.\n",
    "\n",
    "                                                                                                                                                                           تفاوت بین نسخه‌های ساده یا نسخه تنظیم‌شده برای مدل‌ها\n",
    "مدل‌های ساده برای وظایف عمومی طراحی می‌شوند و نیاز به تنظیمات اضافی دارند. مدل‌های تنظیم‌شده برای وظایف خاص، مانند خلاصه‌سازی، بهینه شده‌اند و می‌توانند عملکرد بهتری داشته باشند. انتخاب بستگی به نوع وظیفه و منابع موجود دارد.\n",
    "\n",
    "https://ai.google.dev/edge/litert/models/post_training_quantization\n",
    "https://huggingface.co/spaces/evaluate-metric/bertscore\n",
    "https://medium.com/towardsdev/unlocking-the-potential-of-few-shot-learning-a-synthetic-dataset-approach-to-efficient-machine-284deeda1294\n",
    "\"\"\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jn7-Rwy8SmzR",
    "outputId": "e471ba78-d4d1-48bd-bbc9-94c3fade7777"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "                                                                                                                                                                      چالش‌های ارزیابی متن تولیدی با استفاده از معیارهای امتیاز برت و روژ\n",
      "روژ بیشتر به شباهت‌های واژگانی می‌پردازد، اما برای ارزیابی معنای عمیق‌تر مناسب نیست. امتیاز برت در ارزیابی معنایی عملکرد بهتری دارد اما نیاز به منابع بیشتر و محاسبات پیچیده‌تر دارد. انتخاب بین این دو بستگی به اولویت بین دقت معنایی و سرعت محاسبات دارد.\n",
      "\n",
      "                                                                                                                                                                                            چرا باید از کوانتایزیشن ۴ بیتی استفاده کرد؟ \n",
      "کوانتایزیشن ۴ بیتی باعث کاهش اندازه مدل و بهبود سرعت پردازش می‌شود که برای منابع محدود مفید است. این روش مصرف حافظه را کاهش داده و در دستگاه‌های موبایل یا کم‌قدرت کاربرد دارد، اما دقت مدل کاهش می‌یابد. مناسب برای کاربردهایی با نیاز به سرعت بالا است.\n",
      "\n",
      "                                                                                                                                                                                             چه معیارهایی بهترین انتخاب برای سناریوهای کم‌شات هستند؟\n",
      "در سناریوهای کم‌شات، معیارهایی مانند امتیاز برت بهتر از روژ عمل می‌کنند چون دقت بیشتری در ارزیابی تطابق معنایی دارند. این سناریوها معمولاً به مدل‌هایی نیاز دارند که درک عمیقی از معنای متن داشته باشند. معیار مناسب بستگی به نوع متن و کاربرد خاص دارد.\n",
      "\n",
      "                                                                                                                                                                           تفاوت بین نسخه‌های ساده یا نسخه تنظیم‌شده برای مدل‌ها\n",
      "مدل‌های ساده برای وظایف عمومی طراحی می‌شوند و نیاز به تنظیمات اضافی دارند. مدل‌های تنظیم‌شده برای وظایف خاص، مانند خلاصه‌سازی، بهینه شده‌اند و می‌توانند عملکرد بهتری داشته باشند. انتخاب بستگی به نوع وظیفه و منابع موجود دارد.\n",
      "\n",
      "https://ai.google.dev/edge/litert/models/post_training_quantization\n",
      "https://huggingface.co/spaces/evaluate-metric/bertscore\n",
      "https://medium.com/towardsdev/unlocking-the-potential-of-few-shot-learning-a-synthetic-dataset-approach-to-efficient-machine-284deeda1294\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjwd3--csPHf"
   },
   "source": [
    "*بخش دوم:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "! pip install bitsandbytes\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HxNx1wyElvAW",
    "outputId": "926aa6e3-b9da-4c5e-91f1-e4d85e05d37d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->bitsandbytes) (1.26.4)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:00:32.993528Z",
     "start_time": "2024-12-19T08:00:15.629945Z"
    },
    "id": "MTebNhZVsPHf"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_id = \"HuggingFaceTB/SmolLM2-360M\"  \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_use_double_quant=True,  \n",
    "    bnb_4bit_quant_type=\"nf4\",  \n",
    "    bnb_4bit_compute_dtype=torch.float16  \n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T08:09:09.384987Z",
     "start_time": "2024-12-19T08:09:07.500714Z"
    },
    "id": "W_8hj9wKsPHf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "outputId": "98cb72ef-c829-41ba-8f30-1747b2cd3c56"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.865652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.865570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.865474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.865410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.865255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    output_texts = []\n",
    "    for i in range(len(examples)):\n",
    "        article_text = examples['article'][i] \n",
    "        highlight_text = examples['highlights'][i] \n",
    "        text = f\"### Article: {article_text} \\n ### Summary: {highlight_text}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                \n",
    "    lora_alpha=32,      \n",
    "    lora_dropout=0.1,   \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    bias=\"none\",        \n",
    "    task_type=\"CAUSAL_LM\"  \n",
    ")\n",
    "\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "train_data = dataset['train'].select(range(5000))\n",
    "val_data = dataset['validation'].select(range(500))\n",
    "test_data = dataset['test'].select(range(100))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",               \n",
    "    evaluation_strategy=\"epoch\",          \n",
    "    learning_rate=2e-5,                   \n",
    "    per_device_train_batch_size=2,        \n",
    "    per_device_eval_batch_size=2,         \n",
    "    num_train_epochs=5,                   \n",
    "    weight_decay=0.01,                    \n",
    "    logging_dir='./logs',                 \n",
    "    logging_steps=200,                    \n",
    "    save_steps=1000,                      \n",
    "    save_total_limit=2,                   \n",
    "    fp16=True,                            \n",
    "    gradient_accumulation_steps=8,        \n",
    "    max_grad_norm=1.0,                    \n",
    "    dataloader_num_workers=4,             \n",
    ")\n",
    "\n",
    "peft_config =LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                \n",
    "    args=training_args,              \n",
    "    train_dataset=train_data,         \n",
    "    eval_dataset=val_data,            \n",
    "    peft_config=peft_config,         \n",
    "    tokenizer=tokenizer,         \n",
    "    formatting_func=preprocess_function,\n",
    "    )\n",
    "trainer.train()\n",
    "trainer.save_model(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    sample_article = test_data[i][\"article\"]\n",
    "    expected_summary = test_data[i][\"highlights\"]\n",
    "\n",
    "    generated_summary = summarizer(sample_article, max_length=150, min_length=50, do_sample=False)\n",
    "\n",
    "    generated_summary_text = generated_summary[0]['summary_text']\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(expected_summary, generated_summary_text)\n",
    "\n",
    "    rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "\n",
    "    references = [expected_summary]\n",
    "    candidates = [generated_summary_text]\n",
    "\n",
    "    P, R, F1 = score(candidates, references, lang='en')\n",
    "\n",
    "    precision_scores.append(P.mean().item()) \n",
    "    recall_scores.append(R.mean().item())\n",
    "    f1_scores.append(F1.mean().item())\n",
    "\n",
    "avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "avg_recall = sum(recall_scores) / len(recall_scores)\n",
    "avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print(\"\\nAverage ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {avg_rouge1:.4f}\")\n",
    "print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n",
    "print(f\"ROUGE-L: {avg_rougeL:.4f}\")\n",
    "print(\"\\nAverage BERTScore:\")\n",
    "print(f\"Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, F1: {avg_f1:.4f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "eab432c4acb04f0b936a0bd5899f29bf",
      "5196cb64fb4f4bda97c6545dc9f65e81",
      "ffee2c1d9ef8400095373494e131c6c5",
      "380aec84eb1544b3999a732ec671c385",
      "0202d562c6484b79970cc687f9818e53",
      "006ba9e9a1184ad0964b7cdbdade1ea3",
      "e480823dfc724f928c40c05b3a9b6baa",
      "04f407a173584ca286d66bf25175dc08",
      "b763559aad484f3a99f3b9bb6e8faef8",
      "45f8a1fc9f384dab945e2db2da9979b6",
      "be7fe67b4b1048909d17d3f42e3c4359",
      "d7862d5423844ef297e6ea1556cc247f",
      "f650743c65b04be584e5efb68ce6428b",
      "9f4be734797d44659f1cc7fff4388c9a",
      "34bfd5c375e94d2d9c4530c8abe142f2",
      "17e38805f88548c4909bbb6f9309a62c",
      "4de0c773fe59434c8fa8f9ecef17da4d",
      "b170865c91794352addec7549f7f7edb",
      "6de28b34e8374a72b5694e4478446cd3",
      "89f10c4cfaa44424adc54328a62c2b88",
      "82ab2586529248f18555f8cdee44e3e1",
      "0f97fa14fcb04fc6baf86b3e8579a9c8",
      "413bab98de724648a1c0268851d353d6",
      "ac432e42ed5f4b398890a07dd8e0b25c",
      "e3c47102172144f1a859bb52441695e1",
      "d45e11c95e9a43faa1c67b98b5b54f4c",
      "b77865c232734782ab7875a89365bad9",
      "dbe55f668dcc4a36a3a937dec252e09c",
      "97739abf70e849b3afe47969d93d15ae",
      "152964a4a68a4bf5b30a7a43fc4a7a32",
      "17c163e26dd74c81bcffc84a9e3ac304",
      "d3a919fa9f5d414483373a4fd16ad551",
      "4c421abe451d432d9313d1ea1a0ce50d",
      "4ddea80f1bd44bf2a3f47532230554ce",
      "72e832e88bba421f91f73387ad6fbf9f",
      "554288cc81ff470abee2fcb950da5d87",
      "8c62428b3635486986f5069a8625afa3",
      "7d691f3bc547472097fbe59ef85f8583",
      "59d5c60c520c4b00916e43bdaee592df",
      "057987f750d74e439eacbc7ec5bb645b",
      "5977480e65fd418bbcaf96a9943e1c2e",
      "01489587cb41469ca5905f08a0301aea",
      "044bbc64e2e044cb809cba03e119d99d",
      "beed7bd115844affa65a31b0a00bf63b",
      "853441bf4798409b96442626fcd898ec",
      "1e0c1bcee7c845e998d2a313dd705dac",
      "90ad93fb42bf40bf82f4aeeec2a1e35d",
      "432395dc6aa04dbba7242d1994cf9138",
      "9d0cc52f7c4c4cbdacaa98a0d354e44a",
      "18c9681cdb844845ae2e61bfadf22ef1",
      "48704318a284471ea88f61b111114080",
      "5fa883f7aa2244ab8b839713c3a3038b",
      "14ca400510c845d9bef73c448f0bd7c1",
      "ee9e83493a8a41e5b2d71651d76f2503",
      "c59280b697fb4f78976b95d3bad62360",
      "484385bb0f964adfbbb894fbe6a1a6cd",
      "e0a3e168a37a4799b5708e4134bf582e",
      "43762be424e84b93a81effc525c22fd3",
      "c0698dae2e8a45138fbc7ca8a3a83607",
      "3529dee8539c41e4893ab21f06686fbe",
      "7bb879d313f543d7a28aea2a9867fc78",
      "85a4db0019994a5daedc3eaaa5bd782b",
      "41c37905f551467d966c94e2ea10e858",
      "d4c6e32402d74a84b6d6aea59884d59b",
      "10310e310dcf49fca5088ad989f07cf6",
      "29c25c51eb604113b32e927b7f28429d"
     ]
    },
    "id": "L5pqwsmlfLz-",
    "outputId": "012f76eb-93e8-4c32-915e-ece23d774a32"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The model 'LlamaForCausalLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 720, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eab432c4acb04f0b936a0bd5899f29bf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7862d5423844ef297e6ea1556cc247f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "413bab98de724648a1c0268851d353d6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ddea80f1bd44bf2a3f47532230554ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "853441bf4798409b96442626fcd898ec"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "484385bb0f964adfbbb894fbe6a1a6cd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 511, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 958, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 245, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 442, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 604, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1033, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 491, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 399, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 151, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Your max_length is set to 150, but your input_length is only 147. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=73)\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 646, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 613, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1235, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 376, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1340, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 904, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1501, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1156, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 522, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1181, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 991, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1208, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 457, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 509, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1295, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 368, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 568, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 802, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 831, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 480, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1967, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 619, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 570, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 416, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1015, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1327, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1511, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 687, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 2457, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 293, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1937, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 392, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 465, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 944, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 275, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 375, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1258, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 620, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 254, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1165, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 879, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 807, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 623, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 954, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 343, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 295, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 950, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 652, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 595, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 309, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1632, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 184, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 152, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 255, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 175, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1220, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 474, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 289, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 241, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 382, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 759, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1325, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Your max_length is set to 150, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 758, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 354, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 767, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1030, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 178, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1613, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 379, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 482, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 512, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 202, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1244, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 438, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 537, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 464, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 498, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 215, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 612, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 952, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1086, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 559, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1239, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 308, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 515, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1281, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Average ROUGE Scores:\n",
      "ROUGE-1: 0.1347\n",
      "ROUGE-2: 0.0695\n",
      "ROUGE-L: 0.1004\n",
      "\n",
      "Average BERTScore:\n",
      "Precision: 0.8034, Recall: 0.8861, F1: 0.8426\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\"\"\n",
    "عملکرد سریع‌تر و استفاده کمتر از منابع در مدل اسمول‌ال‌ام۲-۳۶۰M می‌تواند انتخاب بهتری باشد. با توجه به نتایج روژ و برترسکور که برای این مدل مشابه مدل بزرگ‌تر است، این مدل با مصرف منابع کمتر و سرعت پردازش بالاتر به کار می‌آید.\n",
    "\n",
    "اما اگر نیاز به دقت بالاتر و توانایی تنظیم دقیق‌تر دارم و منابع کافی برای پردازش در اختیار دارم، مدل اسمول‌ال‌ام۲-۱.۷B می‌تواند گزینه بهتری باشد. اگرچه در حال حاضر تفاوت عملکردی قابل توجهی با مدل کوچک‌تر ندارد، اما در شرایط خاص یا پس از تنظیم دقیق‌تر، احتمالاً عملکرد بهتری خواهد داشت. البته من در هردو از تنظیم دقیق مدل با استفاده از آداپتورهای یادگیری سریع (SFT) استفاده کردم و نمی‌دانستم که قرار است مقایسه‌ای انجام شود و فکر می‌کنم اگر از SFT استفاده نمی‌کردم، نتیجه مدل اسمول‌ال‌ام۲-۳۶۰M دقیق‌تر می‌شد.\n",
    "\"\"\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcytw01uit9V",
    "outputId": "10faacd2-4022-4598-a505-cf3ea2475129"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "عملکرد سریع‌تر و استفاده کمتر از منابع در مدل اسمول‌ال‌ام۲-۳۶۰M می‌تواند انتخاب بهتری باشد. با توجه به نتایج روژ و برترسکور که برای این مدل مشابه مدل بزرگ‌تر است، این مدل با مصرف منابع کمتر و سرعت پردازش بالاتر به کار می‌آید.\n",
      "\n",
      "اما اگر نیاز به دقت بالاتر و توانایی تنظیم دقیق‌تر دارم و منابع کافی برای پردازش در اختیار دارم، مدل اسمول‌ال‌ام۲-۱.۷B می‌تواند گزینه بهتری باشد. اگرچه در حال حاضر تفاوت عملکردی قابل توجهی با مدل کوچک‌تر ندارد، اما در شرایط خاص یا پس از تنظیم دقیق‌تر، احتمالاً عملکرد بهتری خواهد داشت. البته من در هردو از تنظیم دقیق مدل با استفاده از آداپتورهای یادگیری سریع (SFT) استفاده کردم و نمی‌دانستم که قرار است مقایسه‌ای انجام شود و فکر می‌کنم اگر از SFT استفاده نمی‌کردم، نتیجه مدل اسمول‌ال‌ام۲-۳۶۰M دقیق‌تر می‌شد.\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"\"\"\n",
    "    استفاده از مدل‌های عام‌منظوره بزرگ (بدون آموزش) در چه سناریویی مناسب است؟\n",
    "    مدل‌های عام‌منظوره بزرگ برای وظایف عمومی که نیاز به پردازش داده‌های متنوع دارند مناسب هستند. در صورتی که منابع محاسباتی محدودی داریم و نیاز به یک مدل آماده برای انواع مختلف وظایف داریم، استفاده از این مدل‌ها مناسب است. این مدل‌ها بدون نیاز به آموزش مجدد می‌توانند در بسیاری از موارد کارایی خوبی داشته باشند.\n",
    "\n",
    "    استفاده از تنظیم دقیق مدل‌های کوچک‌تر در چه شرایطی منطقی است؟\n",
    "    مدل‌های کوچک‌تر زمانی که نیاز به دقت بالا در یک وظیفه خاص داریم، باید تنظیم دقیق شوند. در صورتی که بخواهیم مدل برای یک دامنه خاص بهینه‌سازی شود، تنظیم دقیق اهمیت پیدا می‌کند. همچنین، اگر منابع پردازشی محدود باشد، تنظیم دقیق می‌تواند به بهبود عملکرد مدل‌های کوچک‌تر کمک کند.\n",
    "\n",
    "    استفاده از دقت عددی نصفه (FP16) چه تاثیری بر کارایی مدل دارد؟\n",
    "    استفاده از دقت عددی نصفه به کاهش مصرف حافظه و افزایش سرعت پردازش مدل کمک می‌کند. در این حالت حافظه کمتری استفاده می‌شود و زمان پردازش کوتاه‌تر می‌شود، در حالی که دقت مدل معمولاً تحت تاثیر قرار نمی‌گیرد. این روش برای مدل‌های بزرگ که نیاز به پردازش سریع دارند بسیار مفید است.\n",
    "\n",
    "    مدل‌های زبانی سری سمول‌ال‌ام2 و معماری آن\n",
    "    مدل‌های سمول‌ال‌ام2 برای مصرف کمتر منابع طراحی شده‌اند و معمولاً از تکنیک‌های کوانتیزاسیون و آداپتور استفاده می‌کنند. این مدل‌ها قابلیت انجام وظایف مختلف مانند تولید متن و خلاصه‌سازی را با استفاده از منابع محدود دارند. معماری این مدل‌ها شامل کاهش دقت و اندازه مدل به منظور بهبود کارایی است.\n",
    "\n",
    "    تکنیک‌های آموزش مدل‌های کوچک با دقت قابل قبول\n",
    "    برای آموزش مدل‌های کوچک با دقت قابل قبول از تکنیک‌های کوانتیزاسیون، تنظیم دقیق و آداپتورها استفاده می‌شود. این روش‌ها به مدل کمک می‌کنند تا با منابع کمتر و بدون کاهش زیاد دقت، عملکرد مناسبی ارائه دهد. به نظر می‌رسد در مدل‌های سمول‌ال‌ام2 از این تکنیک‌ها برای بهبود کارایی استفاده شده است.\n",
    "\n",
    "    استفاده از نسخه آموزش دستورالعمل یا نسخه ساده برای تنظیم دقیق مدل\n",
    "    نسخه آموزش دستورالعمل برای تنظیم دقیق مدل‌هایی که نیاز به تعامل بیشتر با دستورات دارند، بهتر است. این نسخه به مدل کمک می‌کند تا در انجام وظایف خاص با دستورالعمل‌های واضح بهتر عمل کند. اگر فقط نیاز به بهینه‌سازی مدل برای وظایف عمومی داریم، نسخه ساده‌تر می‌تواند کافی باشد.\n",
    "\n",
    "\n",
    "\"\"\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7omaBSVSkoVo",
    "outputId": "6c618fa9-9f96-4878-9649-7be817f0b4ef"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "    استفاده از مدل‌های عام‌منظوره بزرگ (بدون آموزش) در چه سناریویی مناسب است؟\n",
      "    مدل‌های عام‌منظوره بزرگ برای وظایف عمومی که نیاز به پردازش داده‌های متنوع دارند مناسب هستند. در صورتی که منابع محاسباتی محدودی داریم و نیاز به یک مدل آماده برای انواع مختلف وظایف داریم، استفاده از این مدل‌ها مناسب است. این مدل‌ها بدون نیاز به آموزش مجدد می‌توانند در بسیاری از موارد کارایی خوبی داشته باشند.\n",
      "\n",
      "    استفاده از تنظیم دقیق مدل‌های کوچک‌تر در چه شرایطی منطقی است؟\n",
      "    مدل‌های کوچک‌تر زمانی که نیاز به دقت بالا در یک وظیفه خاص داریم، باید تنظیم دقیق شوند. در صورتی که بخواهیم مدل برای یک دامنه خاص بهینه‌سازی شود، تنظیم دقیق اهمیت پیدا می‌کند. همچنین، اگر منابع پردازشی محدود باشد، تنظیم دقیق می‌تواند به بهبود عملکرد مدل‌های کوچک‌تر کمک کند.\n",
      "\n",
      "    استفاده از دقت عددی نصفه (FP16) چه تاثیری بر کارایی مدل دارد؟\n",
      "    استفاده از دقت عددی نصفه به کاهش مصرف حافظه و افزایش سرعت پردازش مدل کمک می‌کند. در این حالت حافظه کمتری استفاده می‌شود و زمان پردازش کوتاه‌تر می‌شود، در حالی که دقت مدل معمولاً تحت تاثیر قرار نمی‌گیرد. این روش برای مدل‌های بزرگ که نیاز به پردازش سریع دارند بسیار مفید است.\n",
      "\n",
      "    مدل‌های زبانی سری سمول‌ال‌ام2 و معماری آن\n",
      "    مدل‌های سمول‌ال‌ام2 برای مصرف کمتر منابع طراحی شده‌اند و معمولاً از تکنیک‌های کوانتیزاسیون و آداپتور استفاده می‌کنند. این مدل‌ها قابلیت انجام وظایف مختلف مانند تولید متن و خلاصه‌سازی را با استفاده از منابع محدود دارند. معماری این مدل‌ها شامل کاهش دقت و اندازه مدل به منظور بهبود کارایی است.\n",
      "\n",
      "    تکنیک‌های آموزش مدل‌های کوچک با دقت قابل قبول\n",
      "    برای آموزش مدل‌های کوچک با دقت قابل قبول از تکنیک‌های کوانتیزاسیون، تنظیم دقیق و آداپتورها استفاده می‌شود. این روش‌ها به مدل کمک می‌کنند تا با منابع کمتر و بدون کاهش زیاد دقت، عملکرد مناسبی ارائه دهد. به نظر می‌رسد در مدل‌های سمول‌ال‌ام2 از این تکنیک‌ها برای بهبود کارایی استفاده شده است.\n",
      "\n",
      "    استفاده از نسخه آموزش دستورالعمل یا نسخه ساده برای تنظیم دقیق مدل\n",
      "    نسخه آموزش دستورالعمل برای تنظیم دقیق مدل‌هایی که نیاز به تعامل بیشتر با دستورات دارند، بهتر است. این نسخه به مدل کمک می‌کند تا در انجام وظایف خاص با دستورالعمل‌های واضح بهتر عمل کند. اگر فقط نیاز به بهینه‌سازی مدل برای وظایف عمومی داریم، نسخه ساده‌تر می‌تواند کافی باشد.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7QWVmp_sPHf"
   },
   "source": [
    "*بخش سوم:*\n",
    "---\n",
    "# پاسخ خود را اینجا بنویسید:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "from transformers import pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "def apply_l1_pruning(model, amount=0.3):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "\n",
    "apply_l1_pruning(model, amount=0.3)\n",
    "model.save_pretrained(\"./fine_tuned_pruned_model\")\n",
    "def calculate_sparsity(model):\n",
    "    total_params = 0\n",
    "    total_zeros = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            weight = module.weight\n",
    "            total_params += weight.numel()  \n",
    "            total_zeros += torch.sum(weight == 0).item() \n",
    "            layer_sparsity = torch.sum(weight == 0).item() / weight.numel()\n",
    "            print(f\"Sparsity of {name}: {layer_sparsity:.4f}\")\n",
    "\n",
    "    overall_sparsity = total_zeros / total_params\n",
    "    print(f\"Overall sparsity: {overall_sparsity:.4f}\")\n",
    "    return overall_sparsity\n",
    "calculate_sparsity(model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BinVlGMpfI8A",
    "outputId": "0fdee18a-f053-4e85-db32-c9a4d6115ab8"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sparsity of model.layers.0.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.0.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.0.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.0.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.0.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.0.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.0.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.0.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.0.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.0.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.0.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.1.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.1.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.1.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.1.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.1.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.1.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.1.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.1.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.1.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.1.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.1.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.2.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.2.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.2.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.2.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.2.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.2.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.2.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.2.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.2.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.2.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.2.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.3.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.3.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.3.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.3.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.3.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.3.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.3.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.3.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.3.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.3.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.3.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.4.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.4.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.4.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.4.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.4.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.4.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.4.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.4.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.4.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.4.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.4.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.5.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.5.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.5.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.5.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.5.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.5.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.5.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.5.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.5.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.5.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.5.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.6.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.6.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.6.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.6.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.6.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.6.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.6.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.6.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.6.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.6.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.6.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.7.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.7.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.7.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.7.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.7.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.7.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.7.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.7.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.7.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.7.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.7.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.8.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.8.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.8.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.8.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.8.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.8.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.8.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.8.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.8.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.8.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.8.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.9.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.9.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.9.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.9.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.9.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.9.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.9.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.9.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.9.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.9.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.9.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.10.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.10.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.10.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.10.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.10.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.10.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.10.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.10.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.10.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.10.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.10.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.11.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.11.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.11.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.11.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.11.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.11.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.11.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.11.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.11.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.11.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.11.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.12.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.12.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.12.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.12.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.12.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.12.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.12.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.12.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.12.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.12.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.12.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.13.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.13.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.13.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.13.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.13.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.13.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.13.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.13.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.13.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.13.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.13.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.14.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.14.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.14.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.14.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.14.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.14.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.14.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.14.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.14.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.14.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.14.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.15.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.15.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.15.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.15.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.15.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.15.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.15.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.15.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.15.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.15.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.15.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.16.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.16.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.16.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.16.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.16.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.16.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.16.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.16.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.16.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.16.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.16.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.17.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.17.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.17.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.17.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.17.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.17.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.17.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.17.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.17.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.17.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.17.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.18.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.18.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.18.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.18.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.18.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.18.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.18.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.18.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.18.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.18.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.18.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.19.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.19.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.19.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.19.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.19.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.19.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.19.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.19.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.19.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.19.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.19.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.20.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.20.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.20.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.20.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.20.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.20.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.20.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.20.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.20.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.20.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.20.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.21.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.21.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.21.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.21.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.21.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.21.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.21.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.21.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.21.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.21.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.21.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.22.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.22.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.22.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.22.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.22.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.22.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.22.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.22.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.22.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.22.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.22.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.23.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.23.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.23.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.23.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.23.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.23.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.23.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.23.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.23.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.23.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.23.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.24.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.24.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.24.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.24.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.24.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.24.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.24.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.24.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.24.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.24.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.24.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.25.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.25.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.25.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.25.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.25.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.25.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.25.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.25.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.25.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.25.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.25.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.26.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.26.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.26.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.26.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.26.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.26.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.26.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.26.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.26.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.26.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.26.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.27.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.27.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.27.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.27.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.27.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.27.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.27.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.27.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.27.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.27.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.27.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.28.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.28.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.28.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.28.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.28.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.28.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.28.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.28.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.28.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.28.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.28.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.29.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.29.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.29.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.29.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.29.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.29.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.29.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.29.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.29.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.29.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.29.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.30.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.30.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.30.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.30.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.30.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.30.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.30.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.30.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.30.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.30.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.30.mlp.down_proj: 0.3000\n",
      "Sparsity of model.layers.31.self_attn.q_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.31.self_attn.q_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.31.self_attn.q_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.31.self_attn.k_proj: 0.3000\n",
      "Sparsity of model.layers.31.self_attn.v_proj.base_layer: 0.3000\n",
      "Sparsity of model.layers.31.self_attn.v_proj.lora_A.default: 0.3000\n",
      "Sparsity of model.layers.31.self_attn.v_proj.lora_B.default: 0.3000\n",
      "Sparsity of model.layers.31.self_attn.o_proj: 0.3000\n",
      "Sparsity of model.layers.31.mlp.gate_proj: 0.3000\n",
      "Sparsity of model.layers.31.mlp.up_proj: 0.3000\n",
      "Sparsity of model.layers.31.mlp.down_proj: 0.3000\n",
      "Sparsity of lm_head: 0.3000\n",
      "Overall sparsity: 0.3000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_pruned_model\")\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    sample_article = test_data[i][\"article\"]\n",
    "    expected_summary = test_data[i][\"highlights\"]\n",
    "\n",
    "    generated_summary = summarizer(sample_article, max_length=150, min_length=50, do_sample=False)\n",
    "\n",
    "    generated_summary_text = generated_summary[0]['summary_text']\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(expected_summary, generated_summary_text)\n",
    "\n",
    "    rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "\n",
    "    references = [expected_summary]\n",
    "    candidates = [generated_summary_text]\n",
    "\n",
    "    P, R, F1 = score(candidates, references, lang='en')\n",
    "\n",
    "    precision_scores.append(P.mean().item()) \n",
    "    recall_scores.append(R.mean().item())\n",
    "    f1_scores.append(F1.mean().item())\n",
    "\n",
    "avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "avg_recall = sum(recall_scores) / len(recall_scores)\n",
    "avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "print(\"\\nAverage ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {avg_rouge1:.4f}\")\n",
    "print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n",
    "print(f\"ROUGE-L: {avg_rougeL:.4f}\")\n",
    "\n",
    "print(\"\\nAverage BERTScore:\")\n",
    "print(f\"Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, F1: {avg_f1:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gwgkoho6l53y",
    "outputId": "68b4e52c-be3a-4b9d-faa1-c69511809d06"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading adapter weights from ./fine_tuned_pruned_model led to unexpected keys not found in the model:  ['model.layers.0.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.0.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.0.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.0.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.0.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.0.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.0.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.0.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.1.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.1.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.1.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.1.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.1.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.1.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.1.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.1.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.2.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.2.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.2.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.2.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.2.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.2.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.2.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.2.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.3.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.3.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.3.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.3.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.3.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.3.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.3.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.3.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.4.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.4.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.4.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.4.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.4.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.4.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.4.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.4.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.5.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.5.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.5.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.5.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.5.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.5.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.5.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.5.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.6.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.6.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.6.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.6.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.6.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.6.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.6.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.6.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.7.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.7.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.7.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.7.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.7.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.7.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.7.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.7.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.8.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.8.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.8.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.8.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.8.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.8.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.8.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.8.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.9.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.9.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.9.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.9.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.9.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.9.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.9.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.9.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.10.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.10.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.10.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.10.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.10.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.10.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.10.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.10.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.11.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.11.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.11.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.11.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.11.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.11.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.11.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.11.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.12.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.12.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.12.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.12.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.12.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.12.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.12.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.12.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.13.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.13.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.13.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.13.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.13.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.13.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.13.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.13.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.14.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.14.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.14.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.14.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.14.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.14.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.14.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.14.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.15.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.15.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.15.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.15.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.15.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.15.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.15.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.15.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.16.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.16.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.16.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.16.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.16.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.16.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.16.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.16.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.17.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.17.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.17.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.17.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.17.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.17.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.17.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.17.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.18.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.18.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.18.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.18.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.18.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.18.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.18.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.18.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.19.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.19.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.19.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.19.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.19.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.19.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.19.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.19.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.20.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.20.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.20.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.20.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.20.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.20.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.20.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.20.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.21.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.21.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.21.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.21.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.21.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.21.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.21.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.21.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.22.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.22.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.22.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.22.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.22.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.22.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.22.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.22.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.23.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.23.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.23.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.23.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.23.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.23.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.23.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.23.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.24.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.24.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.24.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.24.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.24.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.24.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.24.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.24.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.25.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.25.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.25.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.25.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.25.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.25.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.25.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.25.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.26.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.26.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.26.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.26.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.26.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.26.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.26.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.26.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.27.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.27.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.27.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.27.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.27.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.27.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.27.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.27.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.28.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.28.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.28.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.28.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.28.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.28.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.28.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.28.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.29.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.29.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.29.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.29.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.29.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.29.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.29.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.29.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.30.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.30.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.30.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.30.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.30.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.30.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.30.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.30.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.31.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.31.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.31.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.31.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.31.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.31.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.31.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.31.self_attn.v_proj.lora_B.default.weight_orig']. \n",
      "The model 'LlamaForCausalLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 720, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 511, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 958, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 245, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 442, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 604, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1033, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 491, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 399, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 151, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Your max_length is set to 150, but your input_length is only 147. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=73)\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 646, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 613, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1235, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 376, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1340, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 904, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1501, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1156, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 522, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1181, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 991, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1208, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 457, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 509, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1295, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 368, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 568, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 802, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 831, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 480, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1967, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 619, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 570, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 416, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1015, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1327, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1511, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 687, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 2457, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 293, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1937, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 392, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 465, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 944, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 275, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 375, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1258, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 620, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 254, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1165, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 879, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 807, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 623, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 954, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 343, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 295, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 950, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 652, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 595, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 309, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1632, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 184, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 152, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 255, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 175, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1220, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 474, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 289, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 241, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 382, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 759, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1325, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Your max_length is set to 150, but your input_length is only 127. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 758, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 354, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 767, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1030, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 178, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1613, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 379, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 482, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 512, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 202, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1244, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 438, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 537, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 464, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 498, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 215, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 612, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 952, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1086, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 559, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1239, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 308, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 515, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 1281, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Average ROUGE Scores:\n",
      "ROUGE-1: 0.1347\n",
      "ROUGE-2: 0.0695\n",
      "ROUGE-L: 0.1004\n",
      "\n",
      "Average BERTScore:\n",
      "Precision: 0.8034, Recall: 0.8861, F1: 0.8426\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_pruned_model\")\n",
    "def preprocess_function(examples):\n",
    "    output_texts = []\n",
    "    for i in range(len(examples['article'])):\n",
    "        article_text = examples['article'][i]\n",
    "        highlight_text = examples['highlights'][i]\n",
    "        text = f\"### Article: {article_text} \\n ### Summary: {highlight_text}\"\n",
    "        output_texts.append(text)\n",
    "    print(f\"Type of output_texts: {type(output_texts)}\")\n",
    "    print(f\"Sample of output_texts: {output_texts[:2]}\")  \n",
    "    return tokenizer(output_texts, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",               \n",
    "    evaluation_strategy=\"epoch\",          \n",
    "    learning_rate=2e-5,                   \n",
    "    per_device_train_batch_size=2,        \n",
    "    per_device_eval_batch_size=2,         \n",
    "    num_train_epochs=5,                   \n",
    "    weight_decay=0.01,                    \n",
    "    logging_dir='./logs',                 \n",
    "    logging_steps=200,                    \n",
    "    save_steps=1000,                      \n",
    "    save_total_limit=2,                   \n",
    "    fp16=True,                            \n",
    "    gradient_accumulation_steps=8,        \n",
    "    max_grad_norm=1.0,                    \n",
    "    dataloader_num_workers=4,             \n",
    ")\n",
    "\n",
    "peft_config =LoraConfig(\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                \n",
    "    args=training_args,         \n",
    "    train_dataset=train_data,   \n",
    "    eval_dataset=val_data,      \n",
    "    peft_config=peft_config,    \n",
    "    tokenizer=tokenizer,        \n",
    "    dataset_text_field=\"article\",  \n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./fine_tuned_pruned_model\")\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {results}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726,
     "referenced_widgets": [
      "5ee3d1c1d052451ea261197cb3766d31",
      "353d5d42749f4325afe266d69c7a9160",
      "943f0f4b48af4b76b46f21f7630ca330",
      "8f8dad2609c14532822d83980ec70d64",
      "644dbea1ca3241f4813d84640e700b32",
      "c33d5d56896c4cae96990509e97b7a82",
      "457de2cbb68348358193375eef33ab54",
      "0f6c82407a32472c836bc7e13389b56d",
      "919084c557ac4659834b49a8b287c99c",
      "3c576e151a074222b1ff7d53f022b525",
      "9a8016d4af6e4ec8bb39e4509f7193b2",
      "654ee06a022b415889fb58a8b2844f58",
      "016a37c16b5b493883266dfa8752942c",
      "cce0e4d65d564f7d86cc7698f0b8e256",
      "23962af1e37c44119c5f3a758163cbec",
      "66184be99e50474a803d4eb8773c66dd",
      "163db47e910a4fdb8bdc634426274b99",
      "03bdb2056c884e7083ec677edc202678",
      "e8b61d810ec14f55b9863f472517dd30",
      "e8a765fdb96442cfb9aff168e831828b",
      "3af560e00a394bf4888df621d65be6d5",
      "fe2dca9ff372452f84ed26ea75bcfca1"
     ]
    },
    "id": "Va4x1J4xjbPh",
    "outputId": "94148579-1231-4fa3-8130-7e3fac72b83e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading adapter weights from ./fine_tuned_pruned_model led to unexpected keys not found in the model:  ['model.layers.0.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.0.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.0.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.0.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.0.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.0.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.0.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.0.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.1.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.1.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.1.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.1.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.1.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.1.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.1.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.1.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.2.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.2.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.2.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.2.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.2.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.2.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.2.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.2.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.3.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.3.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.3.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.3.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.3.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.3.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.3.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.3.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.4.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.4.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.4.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.4.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.4.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.4.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.4.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.4.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.5.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.5.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.5.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.5.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.5.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.5.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.5.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.5.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.6.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.6.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.6.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.6.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.6.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.6.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.6.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.6.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.7.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.7.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.7.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.7.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.7.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.7.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.7.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.7.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.8.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.8.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.8.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.8.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.8.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.8.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.8.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.8.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.9.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.9.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.9.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.9.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.9.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.9.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.9.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.9.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.10.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.10.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.10.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.10.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.10.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.10.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.10.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.10.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.11.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.11.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.11.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.11.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.11.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.11.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.11.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.11.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.12.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.12.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.12.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.12.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.12.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.12.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.12.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.12.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.13.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.13.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.13.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.13.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.13.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.13.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.13.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.13.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.14.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.14.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.14.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.14.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.14.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.14.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.14.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.14.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.15.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.15.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.15.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.15.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.15.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.15.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.15.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.15.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.16.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.16.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.16.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.16.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.16.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.16.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.16.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.16.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.17.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.17.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.17.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.17.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.17.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.17.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.17.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.17.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.18.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.18.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.18.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.18.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.18.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.18.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.18.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.18.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.19.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.19.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.19.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.19.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.19.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.19.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.19.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.19.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.20.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.20.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.20.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.20.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.20.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.20.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.20.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.20.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.21.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.21.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.21.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.21.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.21.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.21.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.21.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.21.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.22.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.22.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.22.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.22.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.22.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.22.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.22.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.22.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.23.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.23.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.23.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.23.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.23.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.23.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.23.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.23.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.24.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.24.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.24.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.24.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.24.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.24.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.24.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.24.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.25.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.25.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.25.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.25.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.25.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.25.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.25.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.25.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.26.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.26.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.26.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.26.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.26.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.26.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.26.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.26.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.27.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.27.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.27.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.27.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.27.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.27.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.27.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.27.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.28.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.28.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.28.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.28.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.28.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.28.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.28.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.28.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.29.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.29.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.29.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.29.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.29.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.29.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.29.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.29.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.30.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.30.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.30.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.30.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.30.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.30.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.30.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.30.self_attn.v_proj.lora_B.default.weight_orig', 'model.layers.31.self_attn.q_proj.lora_A.default.weight_mask', 'model.layers.31.self_attn.q_proj.lora_A.default.weight_orig', 'model.layers.31.self_attn.q_proj.lora_B.default.weight_mask', 'model.layers.31.self_attn.q_proj.lora_B.default.weight_orig', 'model.layers.31.self_attn.v_proj.lora_A.default.weight_mask', 'model.layers.31.self_attn.v_proj.lora_A.default.weight_orig', 'model.layers.31.self_attn.v_proj.lora_B.default.weight_mask', 'model.layers.31.self_attn.v_proj.lora_B.default.weight_orig']. \n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ee3d1c1d052451ea261197cb3766d31"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "654ee06a022b415889fb58a8b2844f58"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='678' max='1560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 678/1560 38:43 < 50:31, 0.29 it/s, Epoch 2.17/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>113.521000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T19:48:18.894205Z",
     "start_time": "2024-12-24T19:48:18.889207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\"\"\n",
    "مدلی ک من استفاده کردم قبل از هرس سیو کردم و دادم به تابع برای هرس کردن و بعد از هرس مدل رو سیو کردم و در مرحله بعد دقت و پارامتر هارو حساب کرده ولی تغییری نکرد سرچی که انجام دادم گفته شد ک یا تعداد ۳۰ درصد کم هست و نتوانسته است وزن های مهم را هرس کنه و یا از پارامترهایی ک در قسمت اموزش استفاده شده ک باعث شده مدل تحت تاثیر هرس قرار نگیره مثل نرخ یادگیری و بقیه موارد ولی چیزی ک بود بعد از هرس سرعت محساب پارامتر ها و اموزش مجدد خیلی کند تر شد لطفا مواردی ک گفته شد و زمان خیلی زیادی ازم گرفت رو لحاظ بفرمایید و نمره ای برای نتیحه بدست امده بدید با توجه به اینکه طبق فلوی سوال باید دقت کم میشد و بعد از اموزش با دقت بالا مجدد درست میشد.\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "مدلی ک من استفاده کردم قبل از هرس سیو کردم و دادم به تابع برای هرس کردن و بعد از هرس مدل رو سیو کردم و در مرحله بعد دقت و پارامتر هارو حساب کرده ولی تغییری نکرد سرچی که انجام دادم گفته شد ک یا تعداد ۳۰ درصد کم هست و نتوانسته است وزن های مهم را هرس کنه و یا از پارامترهایی ک در قسمت اموزش استفاده شده ک باعث شده مدل تحت تاثیر هرس قرار نگیره مثل نرخ یادگیری و بقیه موارد ولی چیزی ک بود بعد از هرس سرعت محساب پارامتر ها و اموزش مجدد خیلی کند تر شد لطفا مواردی ک گفته شد و زمان خیلی زیادی ازم گرفت رو لحاظ بفرمایید و نمره ای برای نتیحه بدست امده بدید با توجه به اینکه طبق فلوی سوال باید دقت کم میشد و بعد از اموزش با دقت بالا مجدد درست میشد.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T19:48:21.717485Z",
     "start_time": "2024-12-24T19:48:21.710667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\"\"\n",
    "\n",
    "سه تکنیک هرس مدل‌های زبانی به همراه مزایا و معایب \n",
    "\n",
    "    هرس بدون ساختار (Unstructured Pruning): وزن‌ها به‌طور تصادفی حذف می‌شوند. مزیت: سادگی و کاهش اندازه مدل. معایب: احتمال از دست دادن اطلاعات مهم و کاهش کارایی.\n",
    "    هرس ساختاری (Structured Pruning): لایه‌ها یا فیلترها به‌طور کامل حذف می‌شوند. مزیت: حفظ ساختار مدل و کاهش هزینه محاسباتی. معایب: پیچیدگی بیشتر و احتمال کاهش انعطاف‌پذیری.\n",
    "    هرس محلی (Local Pruning): انتخابی و در نواحی خاص انجام می‌شود. مزیت: حفظ اطلاعات مهم و بهینه‌سازی دقیق‌تر. معایب: پیچیدگی بیشتر و عدم تعمیم‌پذیری.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "  هرس مدل در مقایسه با دیگر روش‌های فشرده‌سازی مدل‌های زبانی چه مزایا و معایبی دارد \n",
    "    \n",
    "    در مقایسه با دیگر روش‌های فشرده‌سازی مدل‌های زبانی، هرس مدل می‌تواند منجر به کاهش اندازه مدل و افزایش سرعت پردازش شود، اما ممکن است دقت مدل کاهش یابد. فاکتورگیری رتبه پایین پیچیدگی محاسباتی مدل را کاهش می‌دهد و دقت را حفظ می‌کند، اما تنظیمات پیچیده‌تری نیاز دارد. دیستیلسیون مدل‌های کوچکتری تولید می‌کند که عملکرد مشابه مدل‌های بزرگتر را حفظ می‌کنند، ولی به یک مدل معلم نیاز دارد و ممکن است دقت را کاهش دهد. کوانتیزاسیون حجم حافظه مدل را کاهش می‌دهد و سرعت اجرا را بالا می‌برد، اما ممکن است دقت را تحت تاثیر قرار دهد و نیاز به سخت‌افزار خاصی داشته باشد.\n",
    "    \n",
    "    \n",
    "    تاثیر هرس بر عملکرد مدل چگونه است؟ چگونه هرس باعث کاهش حافظه و افزایش سرعت پردازش می‌شود؟ \n",
    "    هرس مدل باعث کاهش حافظه و افزایش سرعت پردازش با حذف پارامترهای غیرضروری می‌شود، که این امر به بهبود عملکرد در سخت‌افزارهای محدود منجر می‌شود. با این حال، هرس ممکن است دقت مدل را کاهش دهد چون برخی از پارامترهای مهم برای یادگیری ویژگی‌های پیچیده از بین می‌روند. حذف وزن‌ها می‌تواند ظرفیت مدل را برای شناسایی الگوهای دقیق کاهش دهد. بنابراین، هرس باید با دقت انجام شود تا همزمان با بهینه‌سازی منابع، دقت مدل حفظ شود. در کل، هرس تاثیر مثبت بر سرعت و حافظه دارد اما ممکن است بر دقت اثر منفی بگذارد.\n",
    "    \n",
    "    چرا تنظیم دقیق مجدد پس از هرس می‌تواند عملکرد مدل هرس‌شده را بهبود دهد؟ \n",
    "    تنظیم دقیق مجدد پس از هرس باعث می‌شود مدل بتواند از پارامترهای باقی‌مانده بهینه استفاده کند. این فرآیند به مدل کمک می‌کند تا ویژگی‌های مهم را مجدداً یاد بگیرد و عملکرد خود را بهبود دهد. همچنین، پس از حذف وزن‌های غیرضروری، تنظیم دقیق مجدد تعادل بین دقت و اندازه مدل را بازیابی می‌کند. در نتیجه، مدل می‌تواند به حداکثر عملکرد خود دست یابد و دقت را حفظ یا حتی بهبود بخشد.\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "سه تکنیک هرس مدل‌های زبانی به همراه مزایا و معایب \n",
      "\n",
      "    هرس بدون ساختار (Unstructured Pruning): وزن‌ها به‌طور تصادفی حذف می‌شوند. مزیت: سادگی و کاهش اندازه مدل. معایب: احتمال از دست دادن اطلاعات مهم و کاهش کارایی.\n",
      "    هرس ساختاری (Structured Pruning): لایه‌ها یا فیلترها به‌طور کامل حذف می‌شوند. مزیت: حفظ ساختار مدل و کاهش هزینه محاسباتی. معایب: پیچیدگی بیشتر و احتمال کاهش انعطاف‌پذیری.\n",
      "    هرس محلی (Local Pruning): انتخابی و در نواحی خاص انجام می‌شود. مزیت: حفظ اطلاعات مهم و بهینه‌سازی دقیق‌تر. معایب: پیچیدگی بیشتر و عدم تعمیم‌پذیری.\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "  هرس مدل در مقایسه با دیگر روش‌های فشرده‌سازی مدل‌های زبانی چه مزایا و معایبی دارد \n",
      "    \n",
      "    در مقایسه با دیگر روش‌های فشرده‌سازی مدل‌های زبانی، هرس مدل می‌تواند منجر به کاهش اندازه مدل و افزایش سرعت پردازش شود، اما ممکن است دقت مدل کاهش یابد. فاکتورگیری رتبه پایین پیچیدگی محاسباتی مدل را کاهش می‌دهد و دقت را حفظ می‌کند، اما تنظیمات پیچیده‌تری نیاز دارد. دیستیلسیون مدل‌های کوچکتری تولید می‌کند که عملکرد مشابه مدل‌های بزرگتر را حفظ می‌کنند، ولی به یک مدل معلم نیاز دارد و ممکن است دقت را کاهش دهد. کوانتیزاسیون حجم حافظه مدل را کاهش می‌دهد و سرعت اجرا را بالا می‌برد، اما ممکن است دقت را تحت تاثیر قرار دهد و نیاز به سخت‌افزار خاصی داشته باشد.\n",
      "    \n",
      "    \n",
      "    تاثیر هرس بر عملکرد مدل چگونه است؟ چگونه هرس باعث کاهش حافظه و افزایش سرعت پردازش می‌شود؟ \n",
      "    هرس مدل باعث کاهش حافظه و افزایش سرعت پردازش با حذف پارامترهای غیرضروری می‌شود، که این امر به بهبود عملکرد در سخت‌افزارهای محدود منجر می‌شود. با این حال، هرس ممکن است دقت مدل را کاهش دهد چون برخی از پارامترهای مهم برای یادگیری ویژگی‌های پیچیده از بین می‌روند. حذف وزن‌ها می‌تواند ظرفیت مدل را برای شناسایی الگوهای دقیق کاهش دهد. بنابراین، هرس باید با دقت انجام شود تا همزمان با بهینه‌سازی منابع، دقت مدل حفظ شود. در کل، هرس تاثیر مثبت بر سرعت و حافظه دارد اما ممکن است بر دقت اثر منفی بگذارد.\n",
      "    \n",
      "    چرا تنظیم دقیق مجدد پس از هرس می‌تواند عملکرد مدل هرس‌شده را بهبود دهد؟ \n",
      "    تنظیم دقیق مجدد پس از هرس باعث می‌شود مدل بتواند از پارامترهای باقی‌مانده بهینه استفاده کند. این فرآیند به مدل کمک می‌کند تا ویژگی‌های مهم را مجدداً یاد بگیرد و عملکرد خود را بهبود دهد. همچنین، پس از حذف وزن‌های غیرضروری، تنظیم دقیق مجدد تعادل بین دقت و اندازه مدل را بازیابی می‌کند. در نتیجه، مدل می‌تواند به حداکثر عملکرد خود دست یابد و دقت را حفظ یا حتی بهبود بخشد.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xI_2knW0NkyY"
   },
   "source": [
    "# **نکات مهم**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEig2qIiMDhR"
   },
   "source": [
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "\n",
    "</div>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">فایل ارسالی شما باید با فرمت زیر نامگذاری شود: <code>NLP_CA4_LASTNAME_STUDENTID.ipynb</code></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuwVuIa0MDhR"
   },
   "source": [
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">نحوه انجام این تمرین:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>برخی سوالات نیاز به نوشتن کد پایتون و محاسبه نتایج دارند، و بقیه آنها دارای پاسخ‌های نوشتاری هستند. برای مسائل کدنویسی، باید تمام بلوک‌های کدی که با <code>#WRITE YOUR CODE HERE</code> مشخص شده‌اند را تکمیل کنید.</li> <li>برای پاسخ‌های متنی، باید متنی که می‌گوید \"پاسخ خود را اینجا بنویسید...\" را با پاسخ واقعی خود جایگزین کنید.</li> </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efm7DICPMDhS"
   },
   "source": [
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">صداقت علمی:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>ما نوت‌بوک‌های تعداد مشخصی از دانشجویان که به صورت تصادفی انتخاب می‌شوند، بررسی خواهیم کرد. این بررسی‌ها اطمینان حاصل می‌کنند که کدی که نوشتید واقعاً پاسخ‌های موجود در نوت‌بوک شما را تولید می‌کند. اگر پاسخ‌های صحیح را در نوت‌بوک خود بدون کدی که واقعاً آن پاسخ‌ها را تولید کند تحویل دهید، این یک مورد جدی از عدم صداقت علمی محسوب می‌شود.</li> <li>ما همچنین بررسی‌های خودکاری را برای تشخیص سرقت علمی در نوت‌بوک‌های کولب انجام خواهیم داد. کپی کردن کد از دیگران نیز یک مورد جدی از عدم صداقت علمی محسوب می‌شود.</li> </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqNfFt5AMDhS"
   },
   "source": [
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">توضیحات تکمیلی:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "خوانایی و دقت بررسی‌ها در گزارش نهایی از اهمیت ویژه‌ای برخوردار است. به تمرین‌هایی که به صورت کاغذی تحویل داده شوند یا به صورت عکس در سایت بارگذاری شوند، ترتیب اثری داده نخواهد شد.</li>\n",
    "<li>\n",
    " همه‌ی کدهای پیوست گزارش بایستی قابلیت اجرای مجدد داشته باشند. در صورتی که برای اجرا مجدد آن‌ها نیاز به تنظیمات خاصی می‌باشد، بایستی تنظیمات مورد نیاز را نیز در گزارش خود ذکر کنید.  دقت کنید که  تمامی کدها باید توسط شما اجرا شده باشند و نتایج اجرا در فایل کدهای ارسالی مشخص باشد. به کدهایی که نتایج اجرای آن‌ها در فایل ارسالی مشخص نباشد نمره‌ای تعلق نمی‌گیرد.\n",
    "</li>\n",
    "<li>\n",
    "تمرین تا یک هفته بعد از مهلت تعیین شده با تاخیر تحویل گرفته می‌شود. دقت کنید که شما جمعاً برای تمام تکالیف، ۱۴ روز زمان تحویل بدون جریمه دارید که تنها از ۷ روز آن برای هر تمرین می‌توانید استفاده کنید. در صورتی که این ۱۴ روز به اتمام رسیده باشد، به ازای هر روز تاخیر ده درصد جریمه می‌شود.\n",
    "</li>\n",
    "<li>توجه کنید این تمرین باید به صورت تک‌نفره انجام شود و پاسخ‌های ارائه شده باید نتیجه فعالیت فرد نویسنده باشد (همفکری و به اتفاق هم نوشتن تمرین نیز ممنوع است). در صورت مشاهده\n",
    " تشابه به همه افراد مشارکت‌کننده، نمره تمرین صفر و به استاد گزارش می‌گردد.\n",
    " </li>\n",
    " <li>برای مطالعه بیشتر درباره‌ی فرمت مارک‌دون می‌توانید از <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">این لینک</a> مطالعه کنید.\n",
    " </li>\n",
    "\n",
    " </ul>\n",
    " </div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "e9694627796b4f5786ff700dab3b170b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b5ccba4f497a4c0c97ef8e6a96469844",
       "IPY_MODEL_2bd1d8c3555f41ceabfb1573912152d9",
       "IPY_MODEL_06074ba7900949d9b2519dfea83c94e4"
      ],
      "layout": "IPY_MODEL_2379ccf18d0f4b6bb934486b0619adf0"
     }
    },
    "b5ccba4f497a4c0c97ef8e6a96469844": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_630bbc44049a46e3b325760b73e24a56",
      "placeholder": "​",
      "style": "IPY_MODEL_183c716e62684f8a8852618d4451991e",
      "value": "Map: 100%"
     }
    },
    "2bd1d8c3555f41ceabfb1573912152d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_307d91a57fe44fc2a1290144f035bddb",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_42ab117718da4cb09cdb2055f766ec1f",
      "value": 500
     }
    },
    "06074ba7900949d9b2519dfea83c94e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_febf915104234064b548caeae5822c88",
      "placeholder": "​",
      "style": "IPY_MODEL_c822d8858bb346e2b6cd554e5472675a",
      "value": " 500/500 [00:00&lt;00:00, 14786.07 examples/s]"
     }
    },
    "2379ccf18d0f4b6bb934486b0619adf0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "630bbc44049a46e3b325760b73e24a56": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "183c716e62684f8a8852618d4451991e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "307d91a57fe44fc2a1290144f035bddb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42ab117718da4cb09cdb2055f766ec1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "febf915104234064b548caeae5822c88": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c822d8858bb346e2b6cd554e5472675a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "18073111638147ff9f0955bbf81d0b05": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_90d61c7523294e328ee30706f5a2d5db",
       "IPY_MODEL_a458ea4c5e38449d9d3bcf340b4f5380",
       "IPY_MODEL_fc8f2111100a4ae8b719e942fe7e0ff3"
      ],
      "layout": "IPY_MODEL_7bb7bf42314e4c9f9273a6ebecb3cdb9"
     }
    },
    "90d61c7523294e328ee30706f5a2d5db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56cbaac128e24d6e86ac5f24dae3451d",
      "placeholder": "​",
      "style": "IPY_MODEL_dbdd1eb2ab104058b225892890743e43",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "a458ea4c5e38449d9d3bcf340b4f5380": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b698bcc7fb8a45afaca5f3725a496a8b",
      "max": 25,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1cf14c31ed624840a9ea685001badedc",
      "value": 25
     }
    },
    "fc8f2111100a4ae8b719e942fe7e0ff3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b699f76445694685a79b974c496aa5c7",
      "placeholder": "​",
      "style": "IPY_MODEL_9f596f30b7244f19ba3e146a4f478991",
      "value": " 25.0/25.0 [00:00&lt;00:00, 714B/s]"
     }
    },
    "7bb7bf42314e4c9f9273a6ebecb3cdb9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56cbaac128e24d6e86ac5f24dae3451d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbdd1eb2ab104058b225892890743e43": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b698bcc7fb8a45afaca5f3725a496a8b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cf14c31ed624840a9ea685001badedc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b699f76445694685a79b974c496aa5c7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f596f30b7244f19ba3e146a4f478991": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d239422e9076457cbb57b5fd8e42d4b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a86c70356888413e9b583650cbd36627",
       "IPY_MODEL_f455532c7c0e4559a9961a83d0ba957b",
       "IPY_MODEL_2a6c76f5f3334f83bd32141470b2037e"
      ],
      "layout": "IPY_MODEL_2932f7c4450e47d49bb0bfc9e5b59123"
     }
    },
    "a86c70356888413e9b583650cbd36627": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a09bff9b10034f3a936532a2ad59e1fe",
      "placeholder": "​",
      "style": "IPY_MODEL_07cd0c71457c47c6853e9db6f341aad5",
      "value": "config.json: 100%"
     }
    },
    "f455532c7c0e4559a9961a83d0ba957b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c12126e4d094ebba02c8ebac5901485",
      "max": 482,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c56a8d81f12f4622a61fea35f90fd294",
      "value": 482
     }
    },
    "2a6c76f5f3334f83bd32141470b2037e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94e47b6e9fba461d93686337d9cc2df1",
      "placeholder": "​",
      "style": "IPY_MODEL_277316b857694074ab7a596825887979",
      "value": " 482/482 [00:00&lt;00:00, 8.56kB/s]"
     }
    },
    "2932f7c4450e47d49bb0bfc9e5b59123": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a09bff9b10034f3a936532a2ad59e1fe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07cd0c71457c47c6853e9db6f341aad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c12126e4d094ebba02c8ebac5901485": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c56a8d81f12f4622a61fea35f90fd294": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "94e47b6e9fba461d93686337d9cc2df1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "277316b857694074ab7a596825887979": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd5a7d13ac2f4cb3b3b3c5ffb4f88e5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2321c7ce6343443882c0e294451cc404",
       "IPY_MODEL_c6b54372970549bfbb2d4c8b77128fc6",
       "IPY_MODEL_23d7e33aaa3a4d96a782491688748e09"
      ],
      "layout": "IPY_MODEL_5d22dcb79a134ca281a3d50a805ab4b7"
     }
    },
    "2321c7ce6343443882c0e294451cc404": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2f1fcc486a8460ca049b97279bcf6d5",
      "placeholder": "​",
      "style": "IPY_MODEL_cc6efec711f44fba9a7f4eb0035b3d34",
      "value": "vocab.json: 100%"
     }
    },
    "c6b54372970549bfbb2d4c8b77128fc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf490ff44ef947e1986a0517c004a095",
      "max": 898823,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a4b6c0aed69247c4bdd0610449db1166",
      "value": 898823
     }
    },
    "23d7e33aaa3a4d96a782491688748e09": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff18ad906a0c497a91ef3bdf8cafb7c8",
      "placeholder": "​",
      "style": "IPY_MODEL_cefec0596137440cbd8e54b5fc738029",
      "value": " 899k/899k [00:00&lt;00:00, 1.31MB/s]"
     }
    },
    "5d22dcb79a134ca281a3d50a805ab4b7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2f1fcc486a8460ca049b97279bcf6d5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc6efec711f44fba9a7f4eb0035b3d34": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf490ff44ef947e1986a0517c004a095": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4b6c0aed69247c4bdd0610449db1166": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ff18ad906a0c497a91ef3bdf8cafb7c8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cefec0596137440cbd8e54b5fc738029": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f657ee09a724d29a00071ee7c6bd78b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_899248dc8031417da6225c1dc3e35547",
       "IPY_MODEL_d31ec9b854ab4c14a18b9c4244c4de09",
       "IPY_MODEL_1846f523c01b4bc3a5af821fb056b0dc"
      ],
      "layout": "IPY_MODEL_c2118dd6916d4a8b89d0b4dfa70060e0"
     }
    },
    "899248dc8031417da6225c1dc3e35547": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14e255109295400587d4f38d8ccc8b75",
      "placeholder": "​",
      "style": "IPY_MODEL_24294a1a72f64d658f4cf8bea5153555",
      "value": "merges.txt: 100%"
     }
    },
    "d31ec9b854ab4c14a18b9c4244c4de09": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6dc977c1d42948d7a3abd0b409a45ee7",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f4e17b485e764d84b8c592383300903b",
      "value": 456318
     }
    },
    "1846f523c01b4bc3a5af821fb056b0dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f17a236f4874cf0ab1e0a221f4d3ba2",
      "placeholder": "​",
      "style": "IPY_MODEL_087000335fc44e99a64cb7bdd58e5b48",
      "value": " 456k/456k [00:00&lt;00:00, 1.07MB/s]"
     }
    },
    "c2118dd6916d4a8b89d0b4dfa70060e0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14e255109295400587d4f38d8ccc8b75": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24294a1a72f64d658f4cf8bea5153555": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6dc977c1d42948d7a3abd0b409a45ee7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4e17b485e764d84b8c592383300903b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5f17a236f4874cf0ab1e0a221f4d3ba2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "087000335fc44e99a64cb7bdd58e5b48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9cb8c4f02f58498ab92931b0970e9fdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_592d98108542433cb3a4ea93cce0b535",
       "IPY_MODEL_1e10429495eb4ee6b67d988cc505da44",
       "IPY_MODEL_1052aa85173a4978a09f10475e333eff"
      ],
      "layout": "IPY_MODEL_3854edd65288486ea8603dab10fe70a0"
     }
    },
    "592d98108542433cb3a4ea93cce0b535": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_597982f47ea24974827927d0bbe1fd97",
      "placeholder": "​",
      "style": "IPY_MODEL_0b94ce4927654d89b89f3af141bb9cd7",
      "value": "tokenizer.json: 100%"
     }
    },
    "1e10429495eb4ee6b67d988cc505da44": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_076e03cf988f49229a375134d8c42c8e",
      "max": 1355863,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0baf4fd20d4142c6a611c5e9c28130a8",
      "value": 1355863
     }
    },
    "1052aa85173a4978a09f10475e333eff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9fe0351536b4842a8915797af3fb31d",
      "placeholder": "​",
      "style": "IPY_MODEL_d87cac6b2d21481eb02424d60815ae36",
      "value": " 1.36M/1.36M [00:01&lt;00:00, 1.19MB/s]"
     }
    },
    "3854edd65288486ea8603dab10fe70a0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "597982f47ea24974827927d0bbe1fd97": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b94ce4927654d89b89f3af141bb9cd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "076e03cf988f49229a375134d8c42c8e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0baf4fd20d4142c6a611c5e9c28130a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d9fe0351536b4842a8915797af3fb31d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d87cac6b2d21481eb02424d60815ae36": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be58fa99be5047618084db9a572296ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_80c700c897e94d3983646cfc32c83120",
       "IPY_MODEL_242f980b60ed401dbaeb3cb6c33f7a84",
       "IPY_MODEL_ac94fbee2609457aa24d64ffbeee9214"
      ],
      "layout": "IPY_MODEL_e60fc2ef178445b9ac4e052a160d0144"
     }
    },
    "80c700c897e94d3983646cfc32c83120": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74bfb5fab9da4f05ba3477aecb317966",
      "placeholder": "​",
      "style": "IPY_MODEL_7189308a93e741dab59f7bcb0e92f6d2",
      "value": "model.safetensors: 100%"
     }
    },
    "242f980b60ed401dbaeb3cb6c33f7a84": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d79f1f78bb774e4bb779b28ae0439b81",
      "max": 1421700479,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_efca7867bf7f49d1b2f8263181dd2466",
      "value": 1421700479
     }
    },
    "ac94fbee2609457aa24d64ffbeee9214": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da5a2d88353c4b1cae7f75f57f59e82d",
      "placeholder": "​",
      "style": "IPY_MODEL_cdb4681e2112474697e64ae77b34fbf6",
      "value": " 1.42G/1.42G [00:07&lt;00:00, 115MB/s]"
     }
    },
    "e60fc2ef178445b9ac4e052a160d0144": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74bfb5fab9da4f05ba3477aecb317966": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7189308a93e741dab59f7bcb0e92f6d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d79f1f78bb774e4bb779b28ae0439b81": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efca7867bf7f49d1b2f8263181dd2466": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "da5a2d88353c4b1cae7f75f57f59e82d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdb4681e2112474697e64ae77b34fbf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7caada25255a4a2ea79fd4946350ce6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bcabe00f8cd44488a60fe2ef9720d544",
       "IPY_MODEL_4be3c92d41484d9990e2f427f70c05fb",
       "IPY_MODEL_3e7ea66a14d344e29137dfa1b70ae242"
      ],
      "layout": "IPY_MODEL_78152d8d7f4d42f082d958eefff2c456"
     }
    },
    "bcabe00f8cd44488a60fe2ef9720d544": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6702447046f4c0eb22443e46b8d267c",
      "placeholder": "​",
      "style": "IPY_MODEL_fd5f54010d134e788b24dedaff4a245c",
      "value": "README.md: 100%"
     }
    },
    "4be3c92d41484d9990e2f427f70c05fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e88f13cba91d469b908a97a2c349c401",
      "max": 3266,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c123173aa3fb48fd842c3d432e0c1852",
      "value": 3266
     }
    },
    "3e7ea66a14d344e29137dfa1b70ae242": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_573cb53a00784628b8b1a70e9b44a819",
      "placeholder": "​",
      "style": "IPY_MODEL_591ecaf181ce46998684df7398bd0d04",
      "value": " 3.27k/3.27k [00:00&lt;00:00, 209kB/s]"
     }
    },
    "78152d8d7f4d42f082d958eefff2c456": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6702447046f4c0eb22443e46b8d267c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd5f54010d134e788b24dedaff4a245c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e88f13cba91d469b908a97a2c349c401": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c123173aa3fb48fd842c3d432e0c1852": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "573cb53a00784628b8b1a70e9b44a819": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "591ecaf181ce46998684df7398bd0d04": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1204782198724a6489a97a1360def9b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c2288099b79b4f01b989110606af3f99",
       "IPY_MODEL_5258ad3d9c614684901455cdccca5464",
       "IPY_MODEL_0081b87457d74ff296ccf0bf7a44c1ea"
      ],
      "layout": "IPY_MODEL_c29abfebe6d64184aec032e1fc5f386a"
     }
    },
    "c2288099b79b4f01b989110606af3f99": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6866f5e2121c45c99056d17aff0389e3",
      "placeholder": "​",
      "style": "IPY_MODEL_6223496a27a74c018d32c640def1602c",
      "value": "train-00000-of-00001.parquet: 100%"
     }
    },
    "5258ad3d9c614684901455cdccca5464": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e192f92c737245bcb543ade156c17734",
      "max": 3314963,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6591d105216c4cffad68fe6a9f08b5e0",
      "value": 3314963
     }
    },
    "0081b87457d74ff296ccf0bf7a44c1ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8691d7f7c463456e8d8a61edd8e14424",
      "placeholder": "​",
      "style": "IPY_MODEL_180a7f59f3504de39ceeaf196c344349",
      "value": " 3.31M/3.31M [00:00&lt;00:00, 14.5MB/s]"
     }
    },
    "c29abfebe6d64184aec032e1fc5f386a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6866f5e2121c45c99056d17aff0389e3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6223496a27a74c018d32c640def1602c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e192f92c737245bcb543ade156c17734": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6591d105216c4cffad68fe6a9f08b5e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8691d7f7c463456e8d8a61edd8e14424": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "180a7f59f3504de39ceeaf196c344349": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e2a6aaa33ea48579b2e48156809677e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56752dbb306c491b96950db734080ea6",
       "IPY_MODEL_5f4b7c1b6638482e9f91ff02350f796c",
       "IPY_MODEL_f444c7714a3e497b974bb6cba3f94f0f"
      ],
      "layout": "IPY_MODEL_f0b196963d82478cbfaeb8952f226d40"
     }
    },
    "56752dbb306c491b96950db734080ea6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b76e1b20069349b385b879c3a508683d",
      "placeholder": "​",
      "style": "IPY_MODEL_c27391262e87428b97bf2af3c9ebf4f8",
      "value": "validation-00000-of-00001.parquet: 100%"
     }
    },
    "5f4b7c1b6638482e9f91ff02350f796c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4be7077397484a66be454d94fdb7d29a",
      "max": 430645,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c53e5ad6d48c42879c70286d8b5e026d",
      "value": 430645
     }
    },
    "f444c7714a3e497b974bb6cba3f94f0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_76dcb0f4857f4b00ae053048bc7b2439",
      "placeholder": "​",
      "style": "IPY_MODEL_c3bc90276a734f058b59568bdbf5cb76",
      "value": " 431k/431k [00:00&lt;00:00, 21.7MB/s]"
     }
    },
    "f0b196963d82478cbfaeb8952f226d40": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b76e1b20069349b385b879c3a508683d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c27391262e87428b97bf2af3c9ebf4f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4be7077397484a66be454d94fdb7d29a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c53e5ad6d48c42879c70286d8b5e026d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "76dcb0f4857f4b00ae053048bc7b2439": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3bc90276a734f058b59568bdbf5cb76": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b384c1a1fe04a88b99402d863be256b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ee8a859a7e3f4051af73afe3596284c6",
       "IPY_MODEL_472d3e34c7be4f7ab1880cc1a2590ec3",
       "IPY_MODEL_0e71556a6d18400daa83b898c37fbfc8"
      ],
      "layout": "IPY_MODEL_066074ed0023419fa79c3dc0735836ef"
     }
    },
    "ee8a859a7e3f4051af73afe3596284c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc734652ac54484ba21e111aa1124b7c",
      "placeholder": "​",
      "style": "IPY_MODEL_1219365db1ee430e820c57cc69766d21",
      "value": "test-00000-of-00001.parquet: 100%"
     }
    },
    "472d3e34c7be4f7ab1880cc1a2590ec3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45ff2cd9367448208c03418cccb1e163",
      "max": 423065,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_676acf218792492eae3b6a76fd27a76f",
      "value": 423065
     }
    },
    "0e71556a6d18400daa83b898c37fbfc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ea4a612a0c44b8d9e403e31665b6ee8",
      "placeholder": "​",
      "style": "IPY_MODEL_b4932ec1661f419cbe9355bf0213a4a3",
      "value": " 423k/423k [00:00&lt;00:00, 26.8MB/s]"
     }
    },
    "066074ed0023419fa79c3dc0735836ef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc734652ac54484ba21e111aa1124b7c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1219365db1ee430e820c57cc69766d21": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45ff2cd9367448208c03418cccb1e163": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "676acf218792492eae3b6a76fd27a76f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ea4a612a0c44b8d9e403e31665b6ee8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4932ec1661f419cbe9355bf0213a4a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5773a61f5e83417b87da26415ed9f64e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e301df9235a14e21b912aee03994bc54",
       "IPY_MODEL_f2daa4e9e411415ca1d16879be88d1d6",
       "IPY_MODEL_f9ba0df5051149a29f4a3ba34c6b17f7"
      ],
      "layout": "IPY_MODEL_a306af4e7f5d4ed5a0e695b6cba1438f"
     }
    },
    "e301df9235a14e21b912aee03994bc54": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd068ea7c9d84b54aa7c2d08e93e0c55",
      "placeholder": "​",
      "style": "IPY_MODEL_d5ff20bcb8304853aad3a37bd8d62206",
      "value": "Generating train split: 100%"
     }
    },
    "f2daa4e9e411415ca1d16879be88d1d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88c543eac77548f9a568c04d95026500",
      "max": 26384,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_09800f60c0ac4fc8b80fb8ed792ae5df",
      "value": 26384
     }
    },
    "f9ba0df5051149a29f4a3ba34c6b17f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39bb3ee089844427b02fd004abe24fb2",
      "placeholder": "​",
      "style": "IPY_MODEL_9c732a1cc6a64c89950a69b5b3ca3f7e",
      "value": " 26384/26384 [00:00&lt;00:00, 132912.25 examples/s]"
     }
    },
    "a306af4e7f5d4ed5a0e695b6cba1438f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd068ea7c9d84b54aa7c2d08e93e0c55": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5ff20bcb8304853aad3a37bd8d62206": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88c543eac77548f9a568c04d95026500": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09800f60c0ac4fc8b80fb8ed792ae5df": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "39bb3ee089844427b02fd004abe24fb2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c732a1cc6a64c89950a69b5b3ca3f7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "351b7477ea0049a6852f0cc80c64f785": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72ab6d70b7c7432a840a691e267ea7f8",
       "IPY_MODEL_6d15bbf859de4125a090d4dfc7e52d91",
       "IPY_MODEL_4bcf1637435c4b3eb7840bb3a309a50a"
      ],
      "layout": "IPY_MODEL_a1ce14a7425d477bb45ce7d417dc21fb"
     }
    },
    "72ab6d70b7c7432a840a691e267ea7f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ff0cbc21fdb4733972f56ac299c12dc",
      "placeholder": "​",
      "style": "IPY_MODEL_0b4672e88cea4a7bbf3d7c5b035c3367",
      "value": "Generating validation split: 100%"
     }
    },
    "6d15bbf859de4125a090d4dfc7e52d91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3dde3fbd007941bb94e814586a556a73",
      "max": 3296,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5f254fa72f6f4e9281da4fbbaa1e8ec5",
      "value": 3296
     }
    },
    "4bcf1637435c4b3eb7840bb3a309a50a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a5d4411b8244d79bd515f563ee07f91",
      "placeholder": "​",
      "style": "IPY_MODEL_81ac9011c8ef4cffb1a6f7ca8eca94f6",
      "value": " 3296/3296 [00:00&lt;00:00, 57929.80 examples/s]"
     }
    },
    "a1ce14a7425d477bb45ce7d417dc21fb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ff0cbc21fdb4733972f56ac299c12dc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b4672e88cea4a7bbf3d7c5b035c3367": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3dde3fbd007941bb94e814586a556a73": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f254fa72f6f4e9281da4fbbaa1e8ec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8a5d4411b8244d79bd515f563ee07f91": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81ac9011c8ef4cffb1a6f7ca8eca94f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "efef75d0e65c4048b6cf3dbf600e1503": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e8b4ec3474a14e2e893feac4b0429d33",
       "IPY_MODEL_c2afb53b64ff42fdbbca34b950072055",
       "IPY_MODEL_c365d5e6d609433aa4f5777f70fdfa13"
      ],
      "layout": "IPY_MODEL_0ebc9049dce948faac4eb677ead240c8"
     }
    },
    "e8b4ec3474a14e2e893feac4b0429d33": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33b97d361e5640e791e455878cd72afb",
      "placeholder": "​",
      "style": "IPY_MODEL_40f17fa1eb3248b8b725b4333173a5ff",
      "value": "Generating test split: 100%"
     }
    },
    "c2afb53b64ff42fdbbca34b950072055": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ab0ea6224964bbab151ebef3c6ef23c",
      "max": 3296,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2d1a9ce408094d1dbf00bfb87a068b73",
      "value": 3296
     }
    },
    "c365d5e6d609433aa4f5777f70fdfa13": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bfaaff485e64f7fb4bba5a42f33b629",
      "placeholder": "​",
      "style": "IPY_MODEL_cb08945e33c14d88b9cf82ca58e357e2",
      "value": " 3296/3296 [00:00&lt;00:00, 70846.90 examples/s]"
     }
    },
    "0ebc9049dce948faac4eb677ead240c8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33b97d361e5640e791e455878cd72afb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40f17fa1eb3248b8b725b4333173a5ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ab0ea6224964bbab151ebef3c6ef23c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d1a9ce408094d1dbf00bfb87a068b73": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7bfaaff485e64f7fb4bba5a42f33b629": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb08945e33c14d88b9cf82ca58e357e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e1cb8126fd34316a399d45119233698": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a8c13cd56ad445a3a0eee1944d616b54",
       "IPY_MODEL_823ff306347e4b89a922914b2e799d20",
       "IPY_MODEL_d3d120d6aab1489bba742bed82be3663"
      ],
      "layout": "IPY_MODEL_980d1bf3e8b34d6d85df0126020ee64f"
     }
    },
    "a8c13cd56ad445a3a0eee1944d616b54": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd14e958c84e4975bc8391ae766833b0",
      "placeholder": "​",
      "style": "IPY_MODEL_6bbfe0e4aafd4b0a889167a1120b9957",
      "value": "README.md: 100%"
     }
    },
    "823ff306347e4b89a922914b2e799d20": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ee55133e5ea479d9582a8ecc4b68074",
      "max": 12330,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_93ff055c233f4074b698547455cf85cf",
      "value": 12330
     }
    },
    "d3d120d6aab1489bba742bed82be3663": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df4a322fdbd04a10860c8a3d03b8e584",
      "placeholder": "​",
      "style": "IPY_MODEL_c7bfaf4f6a0742c4ab71ae2151646c3f",
      "value": " 12.3k/12.3k [00:00&lt;00:00, 717kB/s]"
     }
    },
    "980d1bf3e8b34d6d85df0126020ee64f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd14e958c84e4975bc8391ae766833b0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bbfe0e4aafd4b0a889167a1120b9957": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ee55133e5ea479d9582a8ecc4b68074": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93ff055c233f4074b698547455cf85cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df4a322fdbd04a10860c8a3d03b8e584": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7bfaf4f6a0742c4ab71ae2151646c3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b5d45d3637448c8a379fa31de96fe33": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_660d1ba3ad464a1fb05562852893acee",
       "IPY_MODEL_970e5a5684a64ee7ba5ebf9f0e3db14c",
       "IPY_MODEL_6356ea46d8674dab9b9eb610e30e5247"
      ],
      "layout": "IPY_MODEL_af26c7aed95240dfadc723d799153e70"
     }
    },
    "660d1ba3ad464a1fb05562852893acee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc4fef0370ab42ea8ddeb0fc65a3b6db",
      "placeholder": "​",
      "style": "IPY_MODEL_1dd751001a6c49fb908c0d82cef674ea",
      "value": "conll2003.py: 100%"
     }
    },
    "970e5a5684a64ee7ba5ebf9f0e3db14c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93ffbea052b24d4fb4371a8a7d964db6",
      "max": 9570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e3f71056b3f6478f96c66f21bd1966ec",
      "value": 9570
     }
    },
    "6356ea46d8674dab9b9eb610e30e5247": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f210ca931aa44d83a9cb4e38d20d85cf",
      "placeholder": "​",
      "style": "IPY_MODEL_d8773a146c85401eaca7b27e2e60f521",
      "value": " 9.57k/9.57k [00:00&lt;00:00, 719kB/s]"
     }
    },
    "af26c7aed95240dfadc723d799153e70": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc4fef0370ab42ea8ddeb0fc65a3b6db": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1dd751001a6c49fb908c0d82cef674ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93ffbea052b24d4fb4371a8a7d964db6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3f71056b3f6478f96c66f21bd1966ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f210ca931aa44d83a9cb4e38d20d85cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8773a146c85401eaca7b27e2e60f521": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a1cad5fde8f4cb6a5b7113f3668499b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ca4da00629df41b19ca878ccc50c3d47",
       "IPY_MODEL_6170cfb70e344f04bda70ee1b6e06d1a",
       "IPY_MODEL_d1c789f25bba4bac9fade77b51f1a8ba"
      ],
      "layout": "IPY_MODEL_d15765fa3759480d8d4a20f3076140e4"
     }
    },
    "ca4da00629df41b19ca878ccc50c3d47": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2c138db0ab148f7a675173b658f25a2",
      "placeholder": "​",
      "style": "IPY_MODEL_0538617c89204a4d81c51eab56ec8b20",
      "value": "Downloading data: 100%"
     }
    },
    "6170cfb70e344f04bda70ee1b6e06d1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6ab15911ade4ab49c254f2d35ec526a",
      "max": 982975,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_22633176e9cd45bba5c5f40b82323ac2",
      "value": 982975
     }
    },
    "d1c789f25bba4bac9fade77b51f1a8ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c94f00ff7314a0088f85abf3cb6ca30",
      "placeholder": "​",
      "style": "IPY_MODEL_e10ff2bfcafe4b958e0f2ebacb449692",
      "value": " 983k/983k [00:00&lt;00:00, 5.43MB/s]"
     }
    },
    "d15765fa3759480d8d4a20f3076140e4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2c138db0ab148f7a675173b658f25a2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0538617c89204a4d81c51eab56ec8b20": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6ab15911ade4ab49c254f2d35ec526a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22633176e9cd45bba5c5f40b82323ac2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6c94f00ff7314a0088f85abf3cb6ca30": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e10ff2bfcafe4b958e0f2ebacb449692": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e78555d73b4d4b219e8f24b051f3cb9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ee755bbe9772403aa8fb11cc38e1b527",
       "IPY_MODEL_ccee26c5278f43c2a6d905d64d681834",
       "IPY_MODEL_dd6d881ab3d64210bf33879d8b7f26d9"
      ],
      "layout": "IPY_MODEL_8a68efc546af451e9ddc97acc1212843"
     }
    },
    "ee755bbe9772403aa8fb11cc38e1b527": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7e08ab0594948829138409404fcc159",
      "placeholder": "​",
      "style": "IPY_MODEL_22b61430c18d490a9e2da6dd6c908f15",
      "value": "Generating train split: 100%"
     }
    },
    "ccee26c5278f43c2a6d905d64d681834": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75b0d5cbdfcf4ed29f4a1c66f45a0a56",
      "max": 14041,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b5cdb8fdc5114ffda6d6f6f9811b80af",
      "value": 14041
     }
    },
    "dd6d881ab3d64210bf33879d8b7f26d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a37ddf293b074744b167aaec6010b0ad",
      "placeholder": "​",
      "style": "IPY_MODEL_fa4e4fbee31946db97c9b920043285b7",
      "value": " 14041/14041 [00:03&lt;00:00, 4637.87 examples/s]"
     }
    },
    "8a68efc546af451e9ddc97acc1212843": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7e08ab0594948829138409404fcc159": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22b61430c18d490a9e2da6dd6c908f15": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75b0d5cbdfcf4ed29f4a1c66f45a0a56": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5cdb8fdc5114ffda6d6f6f9811b80af": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a37ddf293b074744b167aaec6010b0ad": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa4e4fbee31946db97c9b920043285b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "efa64a232dd447b98d04dc484c20ac16": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_642f89f482924223a6dc0a9cb44567c2",
       "IPY_MODEL_e93ead4cb03d4dec88dad637fa73b01e",
       "IPY_MODEL_57ad02fe9363477cba106bf18b05a738"
      ],
      "layout": "IPY_MODEL_01db17a99d5540dd81f2d4841452945f"
     }
    },
    "642f89f482924223a6dc0a9cb44567c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4302b9fb8ed0470b8a53755aade7b080",
      "placeholder": "​",
      "style": "IPY_MODEL_c5dc97f74d0545d8b6833737e67c1274",
      "value": "Generating validation split: 100%"
     }
    },
    "e93ead4cb03d4dec88dad637fa73b01e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d6fd2c7600e45178cd8ec8fd21e15c3",
      "max": 3250,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6ced072c075d4d44aee0b6e9260f383d",
      "value": 3250
     }
    },
    "57ad02fe9363477cba106bf18b05a738": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fba092d15b941bebe7f054729d3d88d",
      "placeholder": "​",
      "style": "IPY_MODEL_ccfc5899af0d4ae78e3ac10f4ec9c25f",
      "value": " 3250/3250 [00:01&lt;00:00, 2284.18 examples/s]"
     }
    },
    "01db17a99d5540dd81f2d4841452945f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4302b9fb8ed0470b8a53755aade7b080": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5dc97f74d0545d8b6833737e67c1274": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d6fd2c7600e45178cd8ec8fd21e15c3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ced072c075d4d44aee0b6e9260f383d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8fba092d15b941bebe7f054729d3d88d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccfc5899af0d4ae78e3ac10f4ec9c25f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e79b8697b65f487b81268f8f966546c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_39be24d48be94c509700c1196bf6f992",
       "IPY_MODEL_624b8f69d3804ffe8a49143ea91f90cc",
       "IPY_MODEL_770e519121c34cf9abe80d1d4b84a0a2"
      ],
      "layout": "IPY_MODEL_e95fd5d405f74514a571ee3e71a7dda2"
     }
    },
    "39be24d48be94c509700c1196bf6f992": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11117652e28047d988aa4897766c58f8",
      "placeholder": "​",
      "style": "IPY_MODEL_9733c58d2941491e868db3a3c6448209",
      "value": "Generating test split: 100%"
     }
    },
    "624b8f69d3804ffe8a49143ea91f90cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa601664bdae4d1cab1709dce8b7677f",
      "max": 3453,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ddd0ee8cbe443a7916e791db747bbd0",
      "value": 3453
     }
    },
    "770e519121c34cf9abe80d1d4b84a0a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b071c191fa8947cdbe9dbd0c3bd36ef9",
      "placeholder": "​",
      "style": "IPY_MODEL_6de100dda56c456b9aa75162a8cef5c6",
      "value": " 3453/3453 [00:01&lt;00:00, 2342.01 examples/s]"
     }
    },
    "e95fd5d405f74514a571ee3e71a7dda2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11117652e28047d988aa4897766c58f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9733c58d2941491e868db3a3c6448209": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa601664bdae4d1cab1709dce8b7677f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ddd0ee8cbe443a7916e791db747bbd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b071c191fa8947cdbe9dbd0c3bd36ef9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6de100dda56c456b9aa75162a8cef5c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e90d96525b434f2e91736d030129b935": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc1215796e3c4e2588a922ee7e5a0d8c",
       "IPY_MODEL_5a1c0565b3574d3d826f597942aeacc6",
       "IPY_MODEL_8b641f0a4b664c048fb4dc63d3152e21"
      ],
      "layout": "IPY_MODEL_52f616c9e004473e891376d48d3b034f"
     }
    },
    "fc1215796e3c4e2588a922ee7e5a0d8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99cf32c1b07c4b0a966f3e9359e03a89",
      "placeholder": "​",
      "style": "IPY_MODEL_8006ce3b7b8740b7a31d0f2878695da0",
      "value": ""
     }
    },
    "5a1c0565b3574d3d826f597942aeacc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77692a74cf9d4b18bb6abef4f4c01b06",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65cc0c0f112647df89cda1ce613e057b",
      "value": 0
     }
    },
    "8b641f0a4b664c048fb4dc63d3152e21": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_853df5f5b24a487eab1eb212d953cad4",
      "placeholder": "​",
      "style": "IPY_MODEL_b0b6aa3442b642c19a83a69e1f098efd",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "52f616c9e004473e891376d48d3b034f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99cf32c1b07c4b0a966f3e9359e03a89": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8006ce3b7b8740b7a31d0f2878695da0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "77692a74cf9d4b18bb6abef4f4c01b06": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "65cc0c0f112647df89cda1ce613e057b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "853df5f5b24a487eab1eb212d953cad4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0b6aa3442b642c19a83a69e1f098efd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce8fabadfb0b49c399264e03c5c80512": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a2f33b7c0b8148ba87c35c29a032b837",
       "IPY_MODEL_1a4bd0c6b90744058263256a92f85fb4",
       "IPY_MODEL_38b1f2749c2c491e8178ff727eeceb63"
      ],
      "layout": "IPY_MODEL_351f39be330f45c69372637194c370af"
     }
    },
    "a2f33b7c0b8148ba87c35c29a032b837": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce713fc43e414c3b82b33a0c29a32db9",
      "placeholder": "​",
      "style": "IPY_MODEL_c1f857b71c554182be6412cf74bc29c4",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "1a4bd0c6b90744058263256a92f85fb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc96f05b39514f2fa1f9556f3ff26722",
      "max": 25,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0e5455fcda034a7c96ff6ff473fdbd89",
      "value": 25
     }
    },
    "38b1f2749c2c491e8178ff727eeceb63": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3195f2f67e33454bbca7f19c9c5fbd0a",
      "placeholder": "​",
      "style": "IPY_MODEL_be656821ff4748199a15d0dd4bd166dc",
      "value": " 25.0/25.0 [00:00&lt;00:00, 1.90kB/s]"
     }
    },
    "351f39be330f45c69372637194c370af": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce713fc43e414c3b82b33a0c29a32db9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1f857b71c554182be6412cf74bc29c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc96f05b39514f2fa1f9556f3ff26722": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e5455fcda034a7c96ff6ff473fdbd89": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3195f2f67e33454bbca7f19c9c5fbd0a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be656821ff4748199a15d0dd4bd166dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b8c2e9cad9a4f6a945f94ea5cf531e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fdf8b0fa4e0e4d8da06aed72c9561f6a",
       "IPY_MODEL_d255dfb86b4d462e84883acad0cd1cbb",
       "IPY_MODEL_f75b8522cea143fc86eca83aab1b18bf"
      ],
      "layout": "IPY_MODEL_980e3c44abbd49cc852af65838efe594"
     }
    },
    "fdf8b0fa4e0e4d8da06aed72c9561f6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3f0f4e29b484597af7fbc7241740cc4",
      "placeholder": "​",
      "style": "IPY_MODEL_762d54fc44444633acef4634b8ecf849",
      "value": "sentencepiece.bpe.model: 100%"
     }
    },
    "d255dfb86b4d462e84883acad0cd1cbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f476ceb4a404b55878fde3ab336c6d4",
      "max": 5069051,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8bb5fab06baf426081ad003ed56ecf3a",
      "value": 5069051
     }
    },
    "f75b8522cea143fc86eca83aab1b18bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5e47ee35b874ba6a7f435a1b1fd8c55",
      "placeholder": "​",
      "style": "IPY_MODEL_995fc8fed17e43dca6b6a8b634c063d8",
      "value": " 5.07M/5.07M [00:00&lt;00:00, 12.9MB/s]"
     }
    },
    "980e3c44abbd49cc852af65838efe594": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3f0f4e29b484597af7fbc7241740cc4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "762d54fc44444633acef4634b8ecf849": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f476ceb4a404b55878fde3ab336c6d4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bb5fab06baf426081ad003ed56ecf3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f5e47ee35b874ba6a7f435a1b1fd8c55": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "995fc8fed17e43dca6b6a8b634c063d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b778bb683adc4a96ae39a98c4f348b6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2d57fbffabdd4014b250141a3598c59c",
       "IPY_MODEL_26ca477c68084b938aed846905ab07ff",
       "IPY_MODEL_c6383433cf40443a8d94068c463d318c"
      ],
      "layout": "IPY_MODEL_a4ff9e79ab244fd1998b87ca3af31779"
     }
    },
    "2d57fbffabdd4014b250141a3598c59c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c752a7d2e29a40379165ecc78c95900b",
      "placeholder": "​",
      "style": "IPY_MODEL_bff1276dee3141fb89a25b0260e98160",
      "value": "tokenizer.json: 100%"
     }
    },
    "26ca477c68084b938aed846905ab07ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59feaf523a1c4f1d9e4a80a45989cafc",
      "max": 9096718,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2384fda426f54d03a57d620a0ac3058f",
      "value": 9096718
     }
    },
    "c6383433cf40443a8d94068c463d318c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7be3b29483f8492c93ba4a6844f28e09",
      "placeholder": "​",
      "style": "IPY_MODEL_a894ef720a464d97aa058d692986a2d2",
      "value": " 9.10M/9.10M [00:00&lt;00:00, 19.9MB/s]"
     }
    },
    "a4ff9e79ab244fd1998b87ca3af31779": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c752a7d2e29a40379165ecc78c95900b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bff1276dee3141fb89a25b0260e98160": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "59feaf523a1c4f1d9e4a80a45989cafc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2384fda426f54d03a57d620a0ac3058f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7be3b29483f8492c93ba4a6844f28e09": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a894ef720a464d97aa058d692986a2d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "135ad4f73aa14752aef9f17395f141ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9119f811bbb74dafa56c716b9ac7ab7d",
       "IPY_MODEL_011ef2810e604f638e1b6409c5da3e12",
       "IPY_MODEL_0ef9fefd6cce471ba7b7c1f6388c22b0"
      ],
      "layout": "IPY_MODEL_8de229941d9349c7995e9784a3435cb5"
     }
    },
    "9119f811bbb74dafa56c716b9ac7ab7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_192b22f0af3f4a669f118d1146472898",
      "placeholder": "​",
      "style": "IPY_MODEL_2685df12faf84d50abcbc0f8d831eec8",
      "value": "config.json: 100%"
     }
    },
    "011ef2810e604f638e1b6409c5da3e12": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ac0975ac6834d8084337138df628829",
      "max": 615,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f8cdb01566444ebf91bff5b033bc8458",
      "value": 615
     }
    },
    "0ef9fefd6cce471ba7b7c1f6388c22b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cac20a4b199d4129aff696b17af4cc69",
      "placeholder": "​",
      "style": "IPY_MODEL_f762c14507bf4af99c449bbb01992f24",
      "value": " 615/615 [00:00&lt;00:00, 39.9kB/s]"
     }
    },
    "8de229941d9349c7995e9784a3435cb5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "192b22f0af3f4a669f118d1146472898": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2685df12faf84d50abcbc0f8d831eec8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1ac0975ac6834d8084337138df628829": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8cdb01566444ebf91bff5b033bc8458": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cac20a4b199d4129aff696b17af4cc69": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f762c14507bf4af99c449bbb01992f24": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "05ca30b2b0a8417986902dcfb4be483d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_31a500ef0eb8433bb3c6adc06a28a68e",
       "IPY_MODEL_313cfcef43f14fdab8658988443938a5",
       "IPY_MODEL_8486b3704212407a9c87d5f548f7e36a"
      ],
      "layout": "IPY_MODEL_957f1247667249bca7f6397a16b69ce6"
     }
    },
    "31a500ef0eb8433bb3c6adc06a28a68e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dfe113f5e0ad4162a76db9d6dc4d6e55",
      "placeholder": "​",
      "style": "IPY_MODEL_e1892d6ebc714348bbc6794c17693ff6",
      "value": "model.safetensors: 100%"
     }
    },
    "313cfcef43f14fdab8658988443938a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b49958cceae6434988bdff8808443546",
      "max": 1115567652,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c9bfd2dfbd1f4a46b24b6df332e95a3a",
      "value": 1115567652
     }
    },
    "8486b3704212407a9c87d5f548f7e36a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24535e1ab4944c3280b5a8e26cab0eae",
      "placeholder": "​",
      "style": "IPY_MODEL_3a0986e168f84358ba6c4b505a0eacbe",
      "value": " 1.12G/1.12G [00:07&lt;00:00, 70.5MB/s]"
     }
    },
    "957f1247667249bca7f6397a16b69ce6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfe113f5e0ad4162a76db9d6dc4d6e55": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1892d6ebc714348bbc6794c17693ff6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b49958cceae6434988bdff8808443546": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9bfd2dfbd1f4a46b24b6df332e95a3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "24535e1ab4944c3280b5a8e26cab0eae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a0986e168f84358ba6c4b505a0eacbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eab432c4acb04f0b936a0bd5899f29bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5196cb64fb4f4bda97c6545dc9f65e81",
       "IPY_MODEL_ffee2c1d9ef8400095373494e131c6c5",
       "IPY_MODEL_380aec84eb1544b3999a732ec671c385"
      ],
      "layout": "IPY_MODEL_0202d562c6484b79970cc687f9818e53"
     }
    },
    "5196cb64fb4f4bda97c6545dc9f65e81": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_006ba9e9a1184ad0964b7cdbdade1ea3",
      "placeholder": "​",
      "style": "IPY_MODEL_e480823dfc724f928c40c05b3a9b6baa",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "ffee2c1d9ef8400095373494e131c6c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04f407a173584ca286d66bf25175dc08",
      "max": 25,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b763559aad484f3a99f3b9bb6e8faef8",
      "value": 25
     }
    },
    "380aec84eb1544b3999a732ec671c385": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45f8a1fc9f384dab945e2db2da9979b6",
      "placeholder": "​",
      "style": "IPY_MODEL_be7fe67b4b1048909d17d3f42e3c4359",
      "value": " 25.0/25.0 [00:00&lt;00:00, 1.92kB/s]"
     }
    },
    "0202d562c6484b79970cc687f9818e53": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "006ba9e9a1184ad0964b7cdbdade1ea3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e480823dfc724f928c40c05b3a9b6baa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "04f407a173584ca286d66bf25175dc08": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b763559aad484f3a99f3b9bb6e8faef8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "45f8a1fc9f384dab945e2db2da9979b6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be7fe67b4b1048909d17d3f42e3c4359": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7862d5423844ef297e6ea1556cc247f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f650743c65b04be584e5efb68ce6428b",
       "IPY_MODEL_9f4be734797d44659f1cc7fff4388c9a",
       "IPY_MODEL_34bfd5c375e94d2d9c4530c8abe142f2"
      ],
      "layout": "IPY_MODEL_17e38805f88548c4909bbb6f9309a62c"
     }
    },
    "f650743c65b04be584e5efb68ce6428b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4de0c773fe59434c8fa8f9ecef17da4d",
      "placeholder": "​",
      "style": "IPY_MODEL_b170865c91794352addec7549f7f7edb",
      "value": "config.json: 100%"
     }
    },
    "9f4be734797d44659f1cc7fff4388c9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6de28b34e8374a72b5694e4478446cd3",
      "max": 482,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_89f10c4cfaa44424adc54328a62c2b88",
      "value": 482
     }
    },
    "34bfd5c375e94d2d9c4530c8abe142f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82ab2586529248f18555f8cdee44e3e1",
      "placeholder": "​",
      "style": "IPY_MODEL_0f97fa14fcb04fc6baf86b3e8579a9c8",
      "value": " 482/482 [00:00&lt;00:00, 35.1kB/s]"
     }
    },
    "17e38805f88548c4909bbb6f9309a62c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4de0c773fe59434c8fa8f9ecef17da4d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b170865c91794352addec7549f7f7edb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6de28b34e8374a72b5694e4478446cd3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89f10c4cfaa44424adc54328a62c2b88": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "82ab2586529248f18555f8cdee44e3e1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f97fa14fcb04fc6baf86b3e8579a9c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "413bab98de724648a1c0268851d353d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ac432e42ed5f4b398890a07dd8e0b25c",
       "IPY_MODEL_e3c47102172144f1a859bb52441695e1",
       "IPY_MODEL_d45e11c95e9a43faa1c67b98b5b54f4c"
      ],
      "layout": "IPY_MODEL_b77865c232734782ab7875a89365bad9"
     }
    },
    "ac432e42ed5f4b398890a07dd8e0b25c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbe55f668dcc4a36a3a937dec252e09c",
      "placeholder": "​",
      "style": "IPY_MODEL_97739abf70e849b3afe47969d93d15ae",
      "value": "vocab.json: 100%"
     }
    },
    "e3c47102172144f1a859bb52441695e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_152964a4a68a4bf5b30a7a43fc4a7a32",
      "max": 898823,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_17c163e26dd74c81bcffc84a9e3ac304",
      "value": 898823
     }
    },
    "d45e11c95e9a43faa1c67b98b5b54f4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3a919fa9f5d414483373a4fd16ad551",
      "placeholder": "​",
      "style": "IPY_MODEL_4c421abe451d432d9313d1ea1a0ce50d",
      "value": " 899k/899k [00:00&lt;00:00, 42.3MB/s]"
     }
    },
    "b77865c232734782ab7875a89365bad9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbe55f668dcc4a36a3a937dec252e09c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97739abf70e849b3afe47969d93d15ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "152964a4a68a4bf5b30a7a43fc4a7a32": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17c163e26dd74c81bcffc84a9e3ac304": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d3a919fa9f5d414483373a4fd16ad551": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c421abe451d432d9313d1ea1a0ce50d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ddea80f1bd44bf2a3f47532230554ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72e832e88bba421f91f73387ad6fbf9f",
       "IPY_MODEL_554288cc81ff470abee2fcb950da5d87",
       "IPY_MODEL_8c62428b3635486986f5069a8625afa3"
      ],
      "layout": "IPY_MODEL_7d691f3bc547472097fbe59ef85f8583"
     }
    },
    "72e832e88bba421f91f73387ad6fbf9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59d5c60c520c4b00916e43bdaee592df",
      "placeholder": "​",
      "style": "IPY_MODEL_057987f750d74e439eacbc7ec5bb645b",
      "value": "merges.txt: 100%"
     }
    },
    "554288cc81ff470abee2fcb950da5d87": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5977480e65fd418bbcaf96a9943e1c2e",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_01489587cb41469ca5905f08a0301aea",
      "value": 456318
     }
    },
    "8c62428b3635486986f5069a8625afa3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_044bbc64e2e044cb809cba03e119d99d",
      "placeholder": "​",
      "style": "IPY_MODEL_beed7bd115844affa65a31b0a00bf63b",
      "value": " 456k/456k [00:00&lt;00:00, 29.2MB/s]"
     }
    },
    "7d691f3bc547472097fbe59ef85f8583": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59d5c60c520c4b00916e43bdaee592df": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "057987f750d74e439eacbc7ec5bb645b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5977480e65fd418bbcaf96a9943e1c2e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01489587cb41469ca5905f08a0301aea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "044bbc64e2e044cb809cba03e119d99d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "beed7bd115844affa65a31b0a00bf63b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "853441bf4798409b96442626fcd898ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1e0c1bcee7c845e998d2a313dd705dac",
       "IPY_MODEL_90ad93fb42bf40bf82f4aeeec2a1e35d",
       "IPY_MODEL_432395dc6aa04dbba7242d1994cf9138"
      ],
      "layout": "IPY_MODEL_9d0cc52f7c4c4cbdacaa98a0d354e44a"
     }
    },
    "1e0c1bcee7c845e998d2a313dd705dac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18c9681cdb844845ae2e61bfadf22ef1",
      "placeholder": "​",
      "style": "IPY_MODEL_48704318a284471ea88f61b111114080",
      "value": "tokenizer.json: 100%"
     }
    },
    "90ad93fb42bf40bf82f4aeeec2a1e35d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5fa883f7aa2244ab8b839713c3a3038b",
      "max": 1355863,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_14ca400510c845d9bef73c448f0bd7c1",
      "value": 1355863
     }
    },
    "432395dc6aa04dbba7242d1994cf9138": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee9e83493a8a41e5b2d71651d76f2503",
      "placeholder": "​",
      "style": "IPY_MODEL_c59280b697fb4f78976b95d3bad62360",
      "value": " 1.36M/1.36M [00:00&lt;00:00, 15.1MB/s]"
     }
    },
    "9d0cc52f7c4c4cbdacaa98a0d354e44a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18c9681cdb844845ae2e61bfadf22ef1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48704318a284471ea88f61b111114080": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5fa883f7aa2244ab8b839713c3a3038b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14ca400510c845d9bef73c448f0bd7c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ee9e83493a8a41e5b2d71651d76f2503": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c59280b697fb4f78976b95d3bad62360": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "484385bb0f964adfbbb894fbe6a1a6cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e0a3e168a37a4799b5708e4134bf582e",
       "IPY_MODEL_43762be424e84b93a81effc525c22fd3",
       "IPY_MODEL_c0698dae2e8a45138fbc7ca8a3a83607"
      ],
      "layout": "IPY_MODEL_3529dee8539c41e4893ab21f06686fbe"
     }
    },
    "e0a3e168a37a4799b5708e4134bf582e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bb879d313f543d7a28aea2a9867fc78",
      "placeholder": "​",
      "style": "IPY_MODEL_85a4db0019994a5daedc3eaaa5bd782b",
      "value": "model.safetensors: 100%"
     }
    },
    "43762be424e84b93a81effc525c22fd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41c37905f551467d966c94e2ea10e858",
      "max": 1421700479,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d4c6e32402d74a84b6d6aea59884d59b",
      "value": 1421700479
     }
    },
    "c0698dae2e8a45138fbc7ca8a3a83607": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10310e310dcf49fca5088ad989f07cf6",
      "placeholder": "​",
      "style": "IPY_MODEL_29c25c51eb604113b32e927b7f28429d",
      "value": " 1.42G/1.42G [00:06&lt;00:00, 213MB/s]"
     }
    },
    "3529dee8539c41e4893ab21f06686fbe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bb879d313f543d7a28aea2a9867fc78": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85a4db0019994a5daedc3eaaa5bd782b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "41c37905f551467d966c94e2ea10e858": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4c6e32402d74a84b6d6aea59884d59b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "10310e310dcf49fca5088ad989f07cf6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29c25c51eb604113b32e927b7f28429d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ee3d1c1d052451ea261197cb3766d31": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_353d5d42749f4325afe266d69c7a9160",
       "IPY_MODEL_943f0f4b48af4b76b46f21f7630ca330",
       "IPY_MODEL_8f8dad2609c14532822d83980ec70d64"
      ],
      "layout": "IPY_MODEL_644dbea1ca3241f4813d84640e700b32"
     }
    },
    "353d5d42749f4325afe266d69c7a9160": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c33d5d56896c4cae96990509e97b7a82",
      "placeholder": "​",
      "style": "IPY_MODEL_457de2cbb68348358193375eef33ab54",
      "value": "Map: 100%"
     }
    },
    "943f0f4b48af4b76b46f21f7630ca330": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f6c82407a32472c836bc7e13389b56d",
      "max": 5000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_919084c557ac4659834b49a8b287c99c",
      "value": 5000
     }
    },
    "8f8dad2609c14532822d83980ec70d64": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c576e151a074222b1ff7d53f022b525",
      "placeholder": "​",
      "style": "IPY_MODEL_9a8016d4af6e4ec8bb39e4509f7193b2",
      "value": " 5000/5000 [00:11&lt;00:00, 408.21 examples/s]"
     }
    },
    "644dbea1ca3241f4813d84640e700b32": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c33d5d56896c4cae96990509e97b7a82": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "457de2cbb68348358193375eef33ab54": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f6c82407a32472c836bc7e13389b56d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "919084c557ac4659834b49a8b287c99c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c576e151a074222b1ff7d53f022b525": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a8016d4af6e4ec8bb39e4509f7193b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "654ee06a022b415889fb58a8b2844f58": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_016a37c16b5b493883266dfa8752942c",
       "IPY_MODEL_cce0e4d65d564f7d86cc7698f0b8e256",
       "IPY_MODEL_23962af1e37c44119c5f3a758163cbec"
      ],
      "layout": "IPY_MODEL_66184be99e50474a803d4eb8773c66dd"
     }
    },
    "016a37c16b5b493883266dfa8752942c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_163db47e910a4fdb8bdc634426274b99",
      "placeholder": "​",
      "style": "IPY_MODEL_03bdb2056c884e7083ec677edc202678",
      "value": "Map: 100%"
     }
    },
    "cce0e4d65d564f7d86cc7698f0b8e256": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8b61d810ec14f55b9863f472517dd30",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e8a765fdb96442cfb9aff168e831828b",
      "value": 500
     }
    },
    "23962af1e37c44119c5f3a758163cbec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3af560e00a394bf4888df621d65be6d5",
      "placeholder": "​",
      "style": "IPY_MODEL_fe2dca9ff372452f84ed26ea75bcfca1",
      "value": " 500/500 [00:01&lt;00:00, 456.96 examples/s]"
     }
    },
    "66184be99e50474a803d4eb8773c66dd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "163db47e910a4fdb8bdc634426274b99": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "03bdb2056c884e7083ec677edc202678": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8b61d810ec14f55b9863f472517dd30": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8a765fdb96442cfb9aff168e831828b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3af560e00a394bf4888df621d65be6d5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe2dca9ff372452f84ed26ea75bcfca1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
